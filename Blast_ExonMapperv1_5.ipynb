{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PRIDE/blob/main/Blast_ExonMapperv1_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "open-in-colab"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-v14"
      },
      "source": [
        "# ðŸ§¬ **Collagen Exon Mapper v1.5 (Colab Edition)**\n",
        "\n",
        "A production-ready, Colab-focused pipeline for mapping collagen exon\n",
        "architectures across taxa, with robust caching, phylogeny-weighted\n",
        "consensus, optional rescue of misread sequences, and clear per-step\n",
        "reporting for new users.\n",
        "\n",
        "**Whatâ€™s new vs v1.3**\n",
        "- Colab/Drive aware setup with clear runtime checks.\n",
        "- UniProt-first **Taxonomic Lineage Expansion** (IDs + names) producing\n",
        "  stable **cluster keys** for downstream consensus/regex.\n",
        "- **PhylogeneticConsensusEngine** with TimeTree distance cache (MYR),\n",
        "  adaptive tolerances, and MRCA + reliability metrics.\n",
        "- Structured **rescue hooks** (gap detection, 3-frame translation via\n",
        "  Ensembl; toggled off by default) with detailed logging.\n",
        "- Persistent **rejection TSV** with explicit reasons across stages.\n",
        "- Clear logs and status messages at each step for naÃ¯ve users."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-1-header"
      },
      "source": [
        "# **Part 1: Paths & Project Layout**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- This keeps *everything* for collagens in one place: raw UniProt input, per-run exon maps, rejected IDs, manifests, entropy stats, and regex extractor outputs.\n",
        "\n",
        "---\n",
        "\n",
        "## Directory & Path Variables\n",
        "\n",
        "### 1. Root\n",
        "- `DATA_ROOT = \"_SHARED_DATA/ExonMaps\"`\n",
        "- Every run and cache is relative to this.\n",
        "\n",
        "### 2. Gene Family Subdir\n",
        "- `GENE_FAMILY = \"collagens\"`\n",
        "- `FAMILY_DIR = f\"{DATA_ROOT}/{GENE_FAMILY}\"`\n",
        "\n",
        "### 3. Inputs\n",
        "- `UNIPROT_TSV_GZ = f\"{FAMILY_DIR}/uniprot_collagens.tsv.gz\"`\n",
        "  - The canonical UniProt download.\n",
        "- `FASTA_ADDITIONS_DIR = f\"{FAMILY_DIR}/extra_fasta\"`\n",
        "  - For manually added FASTA sequences.\n",
        "\n",
        "### 4. Run Outputs\n",
        "- Instead of dumping into `CollagenExonMapper/run_YYYYMMDD_HHMMSS`, runs go here:\n",
        "  - `RUN_DIR = f\"{FAMILY_DIR}/runs/run_{TIMESTAMP}\"`\n",
        "\n",
        "Inside each `RUN_DIR`:\n",
        "- `log.txt` â€” console/file logging.\n",
        "- `manifest.json` â€” provenance info.\n",
        "- `consensus_long.tsv` â€” per-exon peptides & coords.\n",
        "- `entropy_stats.tsv` â€” Shannon entropy results.\n",
        "- `rescue_log.tsv` â€” audit of rejected/rescued exons.\n",
        "- `regextractor/` â€” outputs from RegExTractor rescues.\n",
        "\n",
        "### 5. Caches\n",
        "- `CACHE_DIR = f\"{FAMILY_DIR}/cache\"`\n",
        "  - Contains incremental exon map cache, taxonomy cache, QC reports.\n",
        "- Each cache is backed up before overwrite.\n",
        "\n",
        "### 6. Rejections\n",
        "- `REJECTED_TSV = f\"{FAMILY_DIR}/rejected_exons.tsv\"`\n",
        "  - Append each runâ€™s newly rejected IDs + reasons here.\n",
        "- Re-read at load time to exclude bad sequences.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P46SuFQxqrWL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-11-md"
      },
      "source": [
        "## Cell 11 â€“ Install core deps & mount Drive\n",
        "\n",
        "Installs required libraries and mounts your Google Drive for persistent\n",
        "storage. If you are **not** in Colab, mounting will be skipped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-11-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 11 =====\n",
        "# Install core dependencies and mount Google Drive.\n",
        "\n",
        "!pip install -q biopython ete3 requests pandas numpy matplotlib tqdm\n",
        "\n",
        "# Mount Google Drive for persistent storage access.\n",
        "try:\n",
        "    from google.colab import drive  # type: ignore\n",
        "    print(\"ðŸ’¾ Mounting Google Drive...\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    print(\"âš ï¸ Not in a Google Colab environment. Drive mounting skipped.\")\n",
        "    IN_COLAB = False\n",
        "\n",
        "import sys, pandas as pd, numpy as np\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"pandas: {pd.__version__}, numpy: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-12-md"
      },
      "source": [
        "## Cell 12 â€“ Central configuration & directory setup\n",
        "\n",
        "Defines user parameters, project/run paths on Drive, logging, and runtime\n",
        "manifests. **All later cells assume this has been run.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 12a =====\n",
        "# Central configuration panel and dynamic directory setup.\n",
        "\n",
        "import os, re, io, time, json, hashlib, logging\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from typing import Optional, List, Dict, Tuple\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import requests\n",
        "import sys\n",
        "from Bio import Entrez\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ---------------- User-Configurable Parameters ----------------\n",
        "#@markdown #### **Core Settings**\n",
        "PROJECT_NAME = \"CollagenExonMapper\"  #@param {type:\"string\"}\n",
        "USER_EMAIL   = \"matthew@palaeome.org\"  #@param {type:\"string\"}\n",
        "Entrez.email = USER_EMAIL\n",
        "\n",
        "#@markdown #### **Gene Selection**\n",
        "PROCESS_ALL_GENES = True  #@param {type:\"boolean\"}\n",
        "GENE_SYMBOLS     = \"COL1A1,COL1A2\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### **Taxonomic Filtering**\n",
        "CLADE_TAXIDS = {\n",
        "    \"Metazoa\": 33208, \"Vertebrata\": 7742, \"Mammalia\": 40674, \"Aves\": 8782,\n",
        "    \"Reptilia\": 8504, \"Amphibia\": 8292, \"Tetrapoda\": 32523,\n",
        "    \"Bony fish\": 117570, \"Cartilaginous fish\": 7777, \"Catarrhini\": 9526\n",
        "}\n",
        "TAXONOMIC_FILTER_NAME = \"Metazoa\" #@param [\"Metazoa\",\"Vertebrata\",\"Mammalia\",\"Aves\",\"Reptilia\",\"Amphibia\",\"Tetrapoda\",\"Bony fish\",\"Cartilaginous fish\",\"Catarrhini\"]\n",
        "TARGET_TAXID = CLADE_TAXIDS[TAXONOMIC_FILTER_NAME]\n",
        "\n",
        "#@markdown #### **Thresholds (Sequences & Mapping)**\n",
        "MIN_LEN_AA          = 600  #@param {type:\"integer\"}\n",
        "MIN_GXY_TRIPLETS    = 30   #@param {type:\"integer\"}\n",
        "CHAIN_LENGTH_THRESHOLD      = 0.90  #@param {type:\"slider\", min:0.5, max:1.0, step:0.05}\n",
        "MAX_ALLOWED_GAP_PERCENTAGE  = 0.10  #@param {type:\"slider\", min:0.05, max:0.5, step:0.05}\n",
        "\n",
        "# Rescue controls\n",
        "\n",
        "#@markdown #### **Identify & rescue additional sequences (FAST; peptide-only)**\n",
        "ENABLE_RESCUE             = True   #@param {type:\"boolean\"}\n",
        "MINIMUM_RESCUE_SCORE      = 0.50   #@param {type:\"number\"}\n",
        "RESCUE_SCORE_IMPROVEMENT  = 0.10   #@param {type:\"number\"}\n",
        "GXY_CONTENT_THRESHOLD     = 0.80   #@param {type:\"number\"}\n",
        "\n",
        "#@markdown #### **DNA fetch for rescue (SLOW; Ensembl REST)**\n",
        "ENABLE_DNA_FETCH          = False  #@param {type:\"boolean\"}\n",
        "ENSEMBL_REQUESTS_PER_SEC  = 5      #@param {type:\"integer\"}\n",
        "ENSEMBL_TIMEOUT_SECS      = 20     #@param {type:\"integer\"}\n",
        "ENSEMBL_BASE              = \"https://rest.ensembl.org\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### **Debugging**\n",
        "DEBUG_SAMPLE_SIZE = -1  #@param {type:\"integer\"}\n",
        "\n",
        "# Optional taxonomy engine (not required when UniProt lineage present)\n",
        "USE_TAXONOMY_ENGINE = False  #@param {type:\"boolean\"}\n"
      ],
      "metadata": {
        "id": "-SPPubQlsj2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 12b =====\n",
        "# Canonical path config for _SHARED_DATA/ExonMaps/GeneFamily/collagens\n",
        "# Forward-only (no legacy aliases). All later parts must import from these.\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# --- roots ---\n",
        "SHARED_DATA_ROOT = Path(\"_SHARED_DATA\")\n",
        "EXONMAPS_ROOT    = SHARED_DATA_ROOT / \"ExonMaps\" / \"GeneFamily\" / \"collagens\"\n",
        "\n",
        "# --- per-run (scratch/provenance only; not authoritative) ---\n",
        "RUN_ID = datetime.utcnow().strftime(\"run_%Y%m%d_%H%M%S\")\n",
        "RUN_DIR = EXONMAPS_ROOT / \"runs\" / RUN_ID\n",
        "\n",
        "# --- subdirs (authoritative) ---\n",
        "INPUTS_DIR    = EXONMAPS_ROOT / \"inputs\"\n",
        "CACHE_DIR     = EXONMAPS_ROOT / \"cache\"\n",
        "CONSENSUS_DIR = EXONMAPS_ROOT / \"consensus\"\n",
        "OUTPUTS_DIR   = EXONMAPS_ROOT / \"outputs\"\n",
        "REGEX_DIR     = EXONMAPS_ROOT / \"regex\"\n",
        "RESCUE_DIR    = EXONMAPS_ROOT / \"rescue\"\n",
        "LOGS_DIR      = EXONMAPS_ROOT / \"logs\"\n",
        "MANIFESTS_DIR = EXONMAPS_ROOT / \"manifests\"\n",
        "\n",
        "# ensure dirs\n",
        "for p in [EXONMAPS_ROOT, RUN_DIR, INPUTS_DIR, CACHE_DIR, CONSENSUS_DIR,\n",
        "          OUTPUTS_DIR, REGEX_DIR, RESCUE_DIR, LOGS_DIR, MANIFESTS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- inputs (authoritative) ---\n",
        "UNIPROT_TSV_GZ              = INPUTS_DIR / \"uniprot_collagens.tsv.gz\"\n",
        "FILTERED_UNIPROT_TSV        = INPUTS_DIR / \"filtered_uniprot_collagens.tsv\"\n",
        "ADDED_FASTA_PATH            = INPUTS_DIR / \"added_fastas.fasta\"\n",
        "\n",
        "# --- caches (authoritative) ---\n",
        "EXON_CACHE_TSV              = CACHE_DIR   / \"raw_exons_cache.tsv\"\n",
        "REJECTED_IDS_TSV            = CACHE_DIR   / \"rejected_ids.tsv\"\n",
        "TIMETREE_CACHE_JSON         = CACHE_DIR   / \"timetree_distance_cache.json\"\n",
        "\n",
        "# --- consensus snapshots (per-run) ---\n",
        "CONSENSUS_LONG_TSV          = CONSENSUS_DIR / f\"consensus_long_{RUN_ID}.tsv\"\n",
        "CONSENSUS_TABLE_TSV         = CONSENSUS_DIR / f\"consensus_table_{RUN_ID}.tsv\"\n",
        "\n",
        "# --- final outputs (authoritative) ---\n",
        "EXON_WIDE_TSV               = OUTPUTS_DIR / f\"exon_wide_{RUN_ID}.tsv\"\n",
        "ENTROPY_STATS_TSV           = OUTPUTS_DIR / f\"entropy_stats_{RUN_ID}.tsv\"\n",
        "MRCA_RELIABILITY_TSV        = OUTPUTS_DIR / f\"mrca_reliability_{RUN_ID}.tsv\"\n",
        "\n",
        "# --- RegExTractor artifacts ---\n",
        "REGEX_BANK_JSON             = REGEX_DIR   / \"regex_bank.json\"\n",
        "REGEX_TRAINING_MANIFEST_JSON= REGEX_DIR   / \"regex_training_manifest.json\"\n",
        "REX_RESCUE_LOG_TSV          = RESCUE_DIR  / f\"rescue_log_{RUN_ID}.tsv\"\n",
        "REX_RESCUED_EXONS_TSV       = RESCUE_DIR  / f\"rescued_exons_{RUN_ID}.tsv\"\n",
        "\n",
        "# --- per-run scratch ---\n",
        "RUN_LOG_PATH                = RUN_DIR     / \"run.log\"\n",
        "RUN_MANIFEST_JSON           = RUN_DIR     / \"manifest.json\"\n",
        "\n",
        "print(\"[Paths] EXONMAPS_ROOT:\", EXONMAPS_ROOT)\n",
        "print(\"[Paths] RUN_DIR:\", RUN_DIR)\n"
      ],
      "metadata": {
        "id": "T8ZoEPy-ruad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-13-md"
      },
      "source": [
        "## Cell 13 â€“ (Optional) Taxonomy engine initialization\n",
        "\n",
        "For **FASTA-only** inputs or lineage QA you can enable an ETE3-backed engine.\n",
        "By default we rely on UniProt lineage columns and skip ETE3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-13-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 13 =====\n",
        "# Optional taxonomy engine (no-op by default)\n",
        "class NCBITaxonomyEngine:\n",
        "    def __init__(self, enabled: bool, ncbi_db_path: Optional[Path] = None):\n",
        "        self._ok = False; self._source = \"uniprot_lineage_only\"; self.ncbi=None\n",
        "        if not enabled: return\n",
        "        try:\n",
        "            from ete3 import NCBITaxa as _ETE_NCBITaxa  # type: ignore\n",
        "            if ncbi_db_path and ncbi_db_path.exists():\n",
        "                self.ncbi = _ETE_NCBITaxa(dbfile=str(ncbi_db_path))\n",
        "                self._source = f\"ete3:{ncbi_db_path}\"\n",
        "            else:\n",
        "                self.ncbi = _ETE_NCBITaxa(); self._source = \"ete3:default\"\n",
        "            _ = self.ncbi.get_lineage(1); self._ok = True\n",
        "        except Exception as e:\n",
        "            logger.info(f\"ETE3 taxonomy disabled/unavailable: {e}\")\n",
        "    def ok(self) -> bool: return self._ok\n",
        "\n",
        "tax_engine = NCBITaxonomyEngine(USE_TAXONOMY_ENGINE, DRIVE_TAXONOMY_PATH)\n",
        "logger.info(f\"Taxonomy mode: {'ETE3' if tax_engine.ok() else 'UniProt-lineage only'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-2-header"
      },
      "source": [
        "# **Part 2: Incremental Data Loading & Profiling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-21-md"
      },
      "source": [
        "## Cell 21 â€“ Unified import, normalization, and cache update\n",
        "\n",
        "Loads prior cache, ingests new `/content/*.tsv`, normalizes gene symbols,\n",
        "filters by clade, and snapshots the working dataset.\n",
        "If nothing is found in `/content`, we proceed with cached data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-21-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 21 =====\n",
        "# Unified import, normalization, and raw cache update\n",
        "\n",
        "def roman_to_int(s: str) -> int:\n",
        "    m = {'I':1,'V':5,'X':10,'L':50,'C':100,'D':500,'M':1000}\n",
        "    s = re.sub(r\"[^IVXLCDM]\", \"\", s.upper()); total = 0\n",
        "    for i, ch in enumerate(s):\n",
        "        v = m.get(ch, 0)\n",
        "        if i+1 < len(s) and m.get(s[i+1],0) > v: total -= v\n",
        "        else: total += v\n",
        "    return total\n",
        "\n",
        "def infer_specific_collagen_symbol(name: str) -> Optional[str]:\n",
        "    if not isinstance(name, str) or 'COLLAGEN' not in name.upper(): return None\n",
        "    rx = re.search(r\"ALPHA[ -]?(\\d+)\\s*\\((.*?)\\)|TYPE\\s+([IVXLCDM]+).*?ALPHA\\s+(\\d+)\",\n",
        "                   name.upper())\n",
        "    if not rx: return None\n",
        "    chain_num = rx.group(1) or rx.group(4)\n",
        "    type_roman = rx.group(2) or rx.group(3)\n",
        "    if not chain_num or not type_roman: return None\n",
        "    t = roman_to_int(type_roman)\n",
        "    return f\"COL{t}A{chain_num}\"\n",
        "\n",
        "def enhanced_gene_normalization(gene: str, pname: str) -> str:\n",
        "    if isinstance(gene, str) and gene.strip():\n",
        "        g = gene.strip().upper()\n",
        "        if re.match(r\"^COL\\d+A\\d+$\", g): return g\n",
        "    if isinstance(pname, str):\n",
        "        guess = infer_specific_collagen_symbol(pname)\n",
        "        if guess: return guess\n",
        "    if isinstance(pname, str) and 'COLLAGEN' in pname.upper():\n",
        "        return \"PROBABLE_COLLAGEN\"\n",
        "    return \"UNKNOWN\"\n",
        "\n",
        "def load_new_inputs_from_content() -> pd.DataFrame:\n",
        "    rows = []; content_dir = Path(\"/content\").resolve()\n",
        "    for p in content_dir.glob(\"*.tsv\"):\n",
        "        try:\n",
        "            df = pd.read_csv(p, sep='\\t', low_memory=False)\n",
        "            df[\"source_file\"] = p.name; rows.append(df)\n",
        "            logger.info(f\"Found input: {p.name} ({len(df)} rows)\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not read {p}: {e}\")\n",
        "    return pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
        "\n",
        "def safe_read_tsv(path: Path) -> pd.DataFrame:\n",
        "    try: return pd.read_csv(path, sep='\\t', low_memory=False)\n",
        "    except Exception: return pd.DataFrame()\n",
        "\n",
        "# Load architecture JSON if present (optional)\n",
        "COLLAGEN_LENGTH_DATA = {}\n",
        "if DRIVE_ARCHITECTURES_PATH.exists():\n",
        "    try:\n",
        "        COLLAGEN_LENGTH_DATA = json.load(open(DRIVE_ARCHITECTURES_PATH))\n",
        "        COLLAGEN_LENGTH_DATA = {k.upper(): v for k, v in COLLAGEN_LENGTH_DATA.items()}\n",
        "        logger.info(\"Architecture JSON loaded.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Architecture JSON load failed: {e}\")\n",
        "\n",
        "# Load master or cache\n",
        "full_df = pd.DataFrame()\n",
        "if MASTER_TSV_PATH.exists(): full_df = safe_read_tsv(MASTER_TSV_PATH)\n",
        "if full_df.empty and FILTERED_UNIPROT_TSV.exists():\n",
        "    full_df = safe_read_tsv(FILTERED_UNIPROT_TSV)\n",
        "\n",
        "# Load rejected list (TSV with Entry, reason, run_id)\n",
        "if REJECTED_IDS_PATH.exists():\n",
        "    rejected_master = safe_read_tsv(REJECTED_IDS_PATH)\n",
        "    rejected_set = set(rejected_master.get(\"Entry\", pd.Series(dtype=str)))\n",
        "else:\n",
        "    rejected_master = pd.DataFrame(columns=[\"Entry\",\"reason\",\"run_id\"]) ; rejected_set=set()\n",
        "logger.info(f\"Loaded cache: {len(full_df)} rows; rejected: {len(rejected_set)}\")\n",
        "\n",
        "# Load new data from /content\n",
        "new_df = load_new_inputs_from_content()\n",
        "if not new_df.empty:\n",
        "    if 'Entry' not in new_df.columns:\n",
        "        logger.warning(\"New TSV missing 'Entry' column; skipping merge.\")\n",
        "    else:\n",
        "        full_df = pd.concat([full_df, new_df], ignore_index=True)\n",
        "\n",
        "if not full_df.empty:\n",
        "    full_df.drop_duplicates(subset=['Entry'], keep='last', inplace=True)\n",
        "    pname = full_df.get('Protein names', pd.Series(['']*len(full_df)))\n",
        "    full_df['gene_symbol_norm'] = [\n",
        "        enhanced_gene_normalization(g, n) for g, n in\n",
        "        zip(full_df.get('Gene Names (primary)', pd.Series(['']*len(full_df))), pname)\n",
        "    ]\n",
        "    if PROCESS_ALL_GENES:\n",
        "        keep_gene = full_df['gene_symbol_norm'].str.contains(r\"^COL\\d+A\\d+$|PROBABLE_COLLAGEN\", na=False)\n",
        "    else:\n",
        "        targets = [g.strip().upper() for g in GENE_SYMBOLS.split(',')]\n",
        "        keep_gene = full_df['gene_symbol_norm'].isin(targets)\n",
        "    keep_tax = True\n",
        "    if 'Taxonomic lineage (Ids)' in full_df.columns:\n",
        "        keep_tax = full_df['Taxonomic lineage (Ids)'].astype(str).str.contains(str(TARGET_TAXID), na=False)\n",
        "    mask = keep_gene & keep_tax\n",
        "    working_df = full_df.loc[mask].copy()\n",
        "else:\n",
        "    working_df = pd.DataFrame()\n",
        "\n",
        "if DEBUG_SAMPLE_SIZE and DEBUG_SAMPLE_SIZE > 0 and not working_df.empty:\n",
        "    working_df = working_df.head(DEBUG_SAMPLE_SIZE).copy()\n",
        "    logger.info(f\"DEBUG mode: sampling first {len(working_df)} rows.\")\n",
        "\n",
        "if not working_df.empty:\n",
        "    working_df.to_csv(WORKING_SNAPSHOT, sep='\\t', index=False)\n",
        "full_df.to_csv(FILTERED_UNIPROT_TSV, sep='\\t', index=False)\n",
        "logger.info(f\"Working rows: {len(working_df)} (snapshot saved)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-24-md"
      },
      "source": [
        "## Cell 24 â€“ Taxonomic lineage expansion (standardized ranks + cluster keys)\n",
        "\n",
        "Parses UniProt lineage columns into canonical rank IDs and emits clustering\n",
        "keys used for phylogeny-weighted consensus and regex derivation.\n",
        "No ETE3 dependency needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-24-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 24 =====\n",
        "# Taxonomic lineage expansion (standardized ranks + cluster keys)\n",
        "\n",
        "RANK_CODES = [\"King\",\"Phyl\",\"Clas\",\"Orde\",\"Fami\",\"Genu\",\"Spec\"]\n",
        "\n",
        "def enhanced_parse_taxonomic_lineage(lineage_ids_str: str) -> Dict[str, Optional[int]]:\n",
        "    out = {k: None for k in RANK_CODES}\n",
        "    if not isinstance(lineage_ids_str, str): return out\n",
        "    ids = [int(x) for x in re.findall(r\"\\d+\", lineage_ids_str)]\n",
        "    if not ids: return out\n",
        "    tail = ids[-7:] if len(ids) >= 7 else ids\n",
        "    tail = [None] * (7 - len(tail)) + tail\n",
        "    for code, taxid in zip(RANK_CODES, tail): out[code] = taxid\n",
        "    return out\n",
        "\n",
        "def _split_uniprot_lineage_names(s: str) -> List[str]:\n",
        "    if not isinstance(s, str) or not s.strip(): return []\n",
        "    return [p for p in re.split(r\"[;|,]\\s*\", s.strip()) if p]\n",
        "\n",
        "if not working_df.empty:\n",
        "    # Names & IDs lists (if available)\n",
        "    working_df[\"Lineage_Names\"] = (\n",
        "        working_df[\"Taxonomic lineage\"].apply(_split_uniprot_lineage_names)\n",
        "        if \"Taxonomic lineage\" in working_df.columns else [[]]*len(working_df)\n",
        "    )\n",
        "    working_df[\"Lineage_IDs_raw\"] = (\n",
        "        working_df[\"Taxonomic lineage (Ids)\"].astype(str)\n",
        "        if \"Taxonomic lineage (Ids)\" in working_df.columns else [\"\"]*len(working_df)\n",
        "    )\n",
        "\n",
        "    parsed = working_df[\"Lineage_IDs_raw\"].apply(enhanced_parse_taxonomic_lineage)\n",
        "    for code in RANK_CODES:\n",
        "        working_df[f\"{code}_id\"] = parsed.apply(lambda d: d.get(code, None))\n",
        "\n",
        "    # Genus/Species from Organism string\n",
        "    if 'Organism' in working_df.columns:\n",
        "        working_df['Genus'] = working_df['Organism'].astype(str).str.split().str[0]\n",
        "        working_df['Species'] = working_df['Organism'].astype(str)\n",
        "\n",
        "    # Cluster keys\n",
        "    nz = lambda x: x if isinstance(x, str) and x.strip() else \"NA\"\n",
        "    genus_name_from_lineage = working_df['Lineage_Names'].apply(lambda L: L[-1] if L else \"\")\n",
        "    working_df['Genus_name'] = working_df['Genus'].fillna(genus_name_from_lineage)\n",
        "    working_df['cluster_genus'] = working_df['Genus_name'].apply(nz)\n",
        "\n",
        "    def _infer_family(L: List[str]) -> str:\n",
        "        if not L: return \"\"\n",
        "        for n in reversed(L):\n",
        "            if n.endswith(\"idae\"): return n\n",
        "        return \"\"\n",
        "    def _infer_order(L: List[str]) -> str:\n",
        "        if not L: return \"\"\n",
        "        for n in reversed(L):\n",
        "            if n.endswith(\"iformes\"): return n\n",
        "        return \"\"\n",
        "    working_df['Family'] = working_df['Lineage_Names'].apply(_infer_family)\n",
        "    working_df['Order']  = working_df['Lineage_Names'].apply(_infer_order)\n",
        "\n",
        "    working_df['cluster_family_genus'] = working_df.apply(\n",
        "        lambda r: f\"{nz(r['Family'])}|{nz(r['Genus_name'])}\", axis=1)\n",
        "    working_df['cluster_class_family_genus'] = working_df.apply(\n",
        "        lambda r: f\"{str(r['Clas_id'] or '')}|{nz(r['Family'])}|{nz(r['Genus_name'])}\", axis=1)\n",
        "\n",
        "    logger.info(\n",
        "        f\"Taxonomy expanded: N={len(working_df)} â†’ rank IDs + cluster keys ready.\")\n",
        "else:\n",
        "    logger.info(\"No working rows; skipping taxonomy enrichment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-25-md"
      },
      "source": [
        "## Cell 25 â€“ Final input checks & sampling\n",
        "\n",
        "Ensures required columns exist and applies DEBUG sampling (if set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-25-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 25 =====\n",
        "# Final filters and input checks\n",
        "req = ['Entry','Sequence']\n",
        "missing = [c for c in req if c not in working_df.columns]\n",
        "if missing:\n",
        "    logger.warning(f\"Missing required columns: {missing}\")\n",
        "else:\n",
        "    working_df = working_df[working_df['Sequence'].astype(str).str.len() > 0]\n",
        "    logger.info(f\"Post-filter rows: {len(working_df)}\")\n",
        "if DEBUG_SAMPLE_SIZE and DEBUG_SAMPLE_SIZE > 0 and not working_df.empty:\n",
        "    working_df = working_df.head(DEBUG_SAMPLE_SIZE).copy()\n",
        "    logger.info(f\"DEBUG mode: sampling first {len(working_df)} rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dMMJxizuOKpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-26-md"
      },
      "source": [
        "## Cell 26 â€“ Persist session rejected IDs\n",
        "\n",
        "Writes run-level rejections (filtered by gene/taxon) and merges them into\n",
        "the master `rejected_ids.tsv` with reasons and `run_id`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 26 =====\n",
        "# Identify entries not included in working_df and save them.\n",
        "# Updated to use canonical path configuration and improved error handling.\n",
        "\n",
        "logging.info(\"--- Archiving Rejected Entries for This Session ---\")\n",
        "\n",
        "# Initialize session rejected IDs set\n",
        "session_rejected_ids = set()\n",
        "\n",
        "# Check if we have the required dataframes\n",
        "if 'full_df' in globals() and not full_df.empty:\n",
        "    if 'working_df' in globals() and not working_df.empty:\n",
        "        # Calculate rejected entries for this session\n",
        "        all_entries = set(full_df['Entry'].dropna().unique())\n",
        "        acc_accepted = set(working_df['Entry'].dropna().unique())\n",
        "        session_rejected_ids = all_entries - acc_accepted\n",
        "\n",
        "        if session_rejected_ids:\n",
        "            logging.info(\n",
        "                f\"   {len(session_rejected_ids):,} entries rejected this session.\"\n",
        "            )\n",
        "\n",
        "            # Use canonical path configuration - save to run directory\n",
        "            rejected_path = RUN_DIR / f\"rejected_ids_{RUN_ID}.txt\"\n",
        "\n",
        "            try:\n",
        "                # Ensure directory exists\n",
        "                rejected_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Write rejected IDs to file\n",
        "                with open(rejected_path, 'w') as f:\n",
        "                    for acc in sorted(list(session_rejected_ids)):\n",
        "                        f.write(f\"{acc}\\n\")\n",
        "\n",
        "                logging.info(f\"   âœ… Rejected list saved to: {rejected_path}\")\n",
        "\n",
        "                # Also update the master rejected IDs cache\n",
        "                master_rejected_path = REJECTED_IDS_TSV\n",
        "\n",
        "                # Load existing master rejected IDs if they exist\n",
        "                existing_rejected = set()\n",
        "                if master_rejected_path.exists():\n",
        "                    try:\n",
        "                        with open(master_rejected_path, 'r') as f:\n",
        "                            existing_rejected = set(line.strip() for line in f if line.strip())\n",
        "                        logging.info(f\"   Loaded {len(existing_rejected)} existing rejected IDs from master list\")\n",
        "                    except Exception as e:\n",
        "                        logging.warning(f\"   Could not load existing rejected IDs: {e}\")\n",
        "\n",
        "                # Combine session rejected with existing rejected\n",
        "                all_rejected = existing_rejected | session_rejected_ids\n",
        "\n",
        "                # Save updated master rejected list\n",
        "                try:\n",
        "                    # Create backup of existing master list\n",
        "                    if master_rejected_path.exists():\n",
        "                        backup_path = CACHE_DIR / f\"rejected_ids_backup_{RUN_ID}.tsv\"\n",
        "                        import shutil\n",
        "                        shutil.copy2(master_rejected_path, backup_path)\n",
        "                        logging.info(f\"   Master rejected IDs backed up to: {backup_path}\")\n",
        "\n",
        "                    # Write updated master list\n",
        "                    with open(master_rejected_path, 'w') as f:\n",
        "                        for acc in sorted(list(all_rejected)):\n",
        "                            f.write(f\"{acc}\\n\")\n",
        "\n",
        "                    new_rejections = len(all_rejected) - len(existing_rejected)\n",
        "                    logging.info(f\"   âœ… Master rejected IDs updated: {len(all_rejected)} total ({new_rejections} new)\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"   Failed to update master rejected IDs: {e}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"   Could not save rejected IDs for this run: {e}\")\n",
        "                # Set empty set to prevent issues downstream\n",
        "                session_rejected_ids = set()\n",
        "\n",
        "        else:\n",
        "            logging.info(\"   No rejections this session.\")\n",
        "\n",
        "    else:\n",
        "        if 'working_df' not in globals():\n",
        "            logging.info(\"   working_df missing; cannot compute rejections.\")\n",
        "        else:\n",
        "            logging.info(\"   working_df is empty; all entries were rejected.\")\n",
        "            # If working_df is empty, all entries in full_df were rejected\n",
        "            session_rejected_ids = set(full_df['Entry'].dropna().unique())\n",
        "\n",
        "            if session_rejected_ids:\n",
        "                logging.info(f\"   All {len(session_rejected_ids):,} entries were rejected.\")\n",
        "\n",
        "                # Save these rejected IDs\n",
        "                rejected_path = RUN_DIR / f\"rejected_ids_{RUN_ID}.txt\"\n",
        "                try:\n",
        "                    rejected_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "                    with open(rejected_path, 'w') as f:\n",
        "                        for acc in sorted(list(session_rejected_ids)):\n",
        "                            f.write(f\"{acc}\\n\")\n",
        "                    logging.info(f\"   âœ… All rejected IDs saved to: {rejected_path}\")\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"   Could not save rejected IDs: {e}\")\n",
        "else:\n",
        "    logging.info(\"   full_df empty or missing; cannot compute rejections.\")\n",
        "\n",
        "# Create summary statistics\n",
        "rejection_summary = {\n",
        "    'run_id': RUN_ID,\n",
        "    'session_rejected_count': len(session_rejected_ids),\n",
        "    'timestamp': datetime.utcnow().isoformat(),\n",
        "}\n",
        "\n",
        "# Log summary\n",
        "if session_rejected_ids:\n",
        "    logging.info(f\"   ðŸ“Š Session Summary: {len(session_rejected_ids)} entries rejected\")\n",
        "\n",
        "    # Save summary to run manifest if it exists\n",
        "    try:\n",
        "        if RUN_MANIFEST_JSON.exists():\n",
        "            import json\n",
        "            with open(RUN_MANIFEST_JSON, 'r') as f:\n",
        "                manifest = json.load(f)\n",
        "        else:\n",
        "            manifest = {}\n",
        "\n",
        "        manifest['rejection_summary'] = rejection_summary\n",
        "\n",
        "        with open(RUN_MANIFEST_JSON, 'w') as f:\n",
        "            json.dump(manifest, f, indent=2)\n",
        "\n",
        "        logging.info(f\"   âœ… Rejection summary added to run manifest\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"   Could not update run manifest: {e}\")\n",
        "\n",
        "# This variable is used in Cell 73 to update the master rejection list\n",
        "# (Note: session_rejected_ids is now available for downstream cells)\n",
        "\n",
        "logging.info(\"--- Rejection Processing Complete ---\")"
      ],
      "metadata": {
        "id": "NdShAvSLOLaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-26-code"
      },
      "outputs": [],
      "source": [
        "# # ===== Old Cell 26 =====\n",
        "# # Persist session rejected IDs (with reasons)\n",
        "# session_rejected = pd.DataFrame(columns=[\"Entry\",\"reason\",\"run_id\"])\n",
        "# if 'full_df' in globals() and not full_df.empty:\n",
        "#     all_ids = set(full_df.get('Entry', pd.Series(dtype=str)))\n",
        "#     acc = set(working_df.get('Entry', pd.Series(dtype=str)))\n",
        "#     rejected = sorted(all_ids - acc)\n",
        "#     if rejected:\n",
        "#         session_rejected = pd.DataFrame({\n",
        "#             \"Entry\": rejected,\n",
        "#             \"reason\": \"filtered_by_gene_or_taxon\",\n",
        "#             \"run_id\": RUN_ID\n",
        "#         })\n",
        "#         session_rejected.to_csv(REJECTED_SNAPSHOT, sep='\\t', index=False)\n",
        "#         logger.info(f\"Rejected this run: {len(session_rejected)} (snapshot)\")\n",
        "#     else:\n",
        "#         logger.info(\"No session rejections.\")\n",
        "\n",
        "# if REJECTED_IDS_PATH.exists():\n",
        "#     master_rej = pd.read_csv(REJECTED_IDS_PATH, sep='\\t', low_memory=False)\n",
        "# else:\n",
        "#     master_rej = pd.DataFrame(columns=[\"Entry\",\"reason\",\"run_id\"])\n",
        "# master_rej = pd.concat([master_rej, session_rejected], ignore_index=True)\n",
        "# master_rej.drop_duplicates(subset=['Entry'], keep='last', inplace=True)\n",
        "# master_rej.to_csv(REJECTED_IDS_PATH, sep='\\t', index=False)\n",
        "# logger.info(\"Master rejected TSV updated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-3-header"
      },
      "source": [
        "# **Part 3: Chain Identification & Quality Pre-selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-31-md"
      },
      "source": [
        "## Cell 31 â€“ Detect main Gâ€“Xâ€“Y chain(s)\n",
        "\n",
        "Triplet-aligned scanning (step=3), minimal Cys-in-helix flag, and a simple\n",
        "0â€“100 quality score used later in reliability metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-31-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 31 =====\n",
        "# Identify main Gâ€“Xâ€“Y chain(s)\n",
        "\n",
        "def find_gxy_segments(seq: str, min_triplets: int) -> List[Dict]:\n",
        "    seq = str(seq); runs: List[Tuple[int,int]] = []\n",
        "    i = 0\n",
        "    while i <= max(0, len(seq)-3):\n",
        "        if seq[i] == 'G' and i+2 < len(seq):\n",
        "            j = i\n",
        "            while j+2 < len(seq) and seq[j] == 'G':\n",
        "                j += 3\n",
        "            if (j - i) % 3 != 0: j -= ((j - i) % 3)\n",
        "            triplets = (j - i) // 3\n",
        "            if triplets >= min_triplets: runs.append((i+1, j))\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "    return [{\"start\": s, \"end\": e} for s, e in runs]\n",
        "\n",
        "def has_cys_in_range(seq: str, start: int, end: int) -> bool:\n",
        "    s0 = max(0, start-1); e0 = min(len(seq), end)\n",
        "    return 'C' in seq[s0:e0]\n",
        "\n",
        "main_rows = []\n",
        "if not working_df.empty and 'Sequence' in working_df.columns:\n",
        "    for _, r in working_df.iterrows():\n",
        "        segs = find_gxy_segments(r['Sequence'], MIN_GXY_TRIPLETS)\n",
        "        if segs:\n",
        "            seg = max(segs, key=lambda x: x['end']-x['start'])\n",
        "            triplets = (seg['end']-seg['start'])//3\n",
        "            # crude 0-100 scaling relative to MIN_GXY_TRIPLETS\n",
        "            qscore = float(min(100.0, 100.0 * triplets / max(1, MIN_GXY_TRIPLETS)))\n",
        "            flag_cys = has_cys_in_range(r['Sequence'], seg['start'], seg['end'])\n",
        "            main_rows.append({\n",
        "                **r,\n",
        "                \"main_chain_segments\": [seg],\n",
        "                \"quality_score\": qscore,\n",
        "                \"quality_flags\": \"cys_in_helix\" if flag_cys else \"\"\n",
        "            })\n",
        "chain_df = pd.DataFrame(main_rows) if main_rows else pd.DataFrame()\n",
        "logger.info(f\"Candidates with main chain: {len(chain_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-32-md"
      },
      "source": [
        "## Cell 32 â€“ QC pass/fail & persistence\n",
        "\n",
        "Applies length/GXY/Cys checks, logs tallies, and merges QC failures into\n",
        "the master rejection table with `reason='QC_fail'`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-32-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 32 =====\n",
        "# QC pass/fail and rejection persistence\n",
        "df_high_quality = pd.DataFrame(); df_failed_qc = pd.DataFrame()\n",
        "if not chain_df.empty:\n",
        "    pass_rows, fail_rows = [], []\n",
        "    for _, r in chain_df.iterrows():\n",
        "        seg = r['main_chain_segments'][0] if r.get('main_chain_segments') else None\n",
        "        if not seg:\n",
        "            r2 = r.copy(); r2['failure_reasons'] = 'no_main_chain'; fail_rows.append(r2); continue\n",
        "        triplets = (seg['end']-seg['start'])//3\n",
        "        if triplets < MIN_GXY_TRIPLETS or len(str(r.get('Sequence',''))) < MIN_LEN_AA:\n",
        "            r2 = r.copy(); r2['failure_reasons'] = 'short_or_low_gxy'; fail_rows.append(r2); continue\n",
        "        if 'cys_in_helix' in r.get('quality_flags',''):\n",
        "            r2 = r.copy(); r2['failure_reasons'] = 'cys_in_helix'; fail_rows.append(r2); continue\n",
        "        pass_rows.append(r)\n",
        "    df_high_quality = pd.DataFrame(pass_rows)\n",
        "    df_failed_qc = pd.DataFrame(fail_rows)\n",
        "    logger.info(f\"QC pass: {len(df_high_quality)}; fail: {len(df_failed_qc)}\")\n",
        "else:\n",
        "    logger.info(\"No chain candidates to QC.\")\n",
        "\n",
        "if not df_failed_qc.empty and 'Entry' in df_failed_qc.columns:\n",
        "    add = df_failed_qc[['Entry','failure_reasons']].dropna().rename(columns={'failure_reasons':'reason'})\n",
        "    add['run_id'] = RUN_ID\n",
        "    if REJECTED_IDS_PATH.exists(): master = pd.read_csv(REJECTED_IDS_PATH, sep='\\t')\n",
        "    else: master = pd.DataFrame(columns=[\"Entry\",\"reason\",\"run_id\"])\n",
        "    master = pd.concat([master, add], ignore_index=True).drop_duplicates(subset=['Entry'], keep='last')\n",
        "    master.to_csv(REJECTED_IDS_PATH, sep='\\t', index=False)\n",
        "    logger.info(\"QC failures merged into master rejection TSV.\")\n",
        "\n",
        "map_df = df_high_quality.copy() if not df_high_quality.empty else pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-4-header"
      },
      "source": [
        "# **Part 4: Enhanced Exon Mapping & Architecture Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 40 â€“ Mapping safety helpers (resume & atomic writes)"
      ],
      "metadata": {
        "id": "8D75gQKDCyzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===== Cell 40 =====\n",
        "# # Mapping safety helpers (resume & atomic writes)\n",
        "\n",
        "# from datetime import datetime\n",
        "# import shutil, hashlib\n",
        "\n",
        "# def _sha256_file(p: Path) -> str:\n",
        "#     if not p.exists(): return \"\"\n",
        "#     h = hashlib.sha256()\n",
        "#     with open(p, 'rb') as f:\n",
        "#         for chunk in iter(lambda: f.read(1<<20), b''):\n",
        "#             h.update(chunk)\n",
        "#     return h.hexdigest()\n",
        "\n",
        "# def load_mapped_accessions_from_cache() -> set:\n",
        "#     \"\"\"Read the master cache and return the set of already-mapped accessions.\"\"\"\n",
        "#     if not EXON_CACHE_TSV.exists():\n",
        "#         return set()\n",
        "#     try:\n",
        "#         df = pd.read_csv(EXON_CACHE_TSV, sep='\\t', usecols=['accession'], low_memory=False)\n",
        "#         return set(df['accession'].astype(str).unique())\n",
        "#     except Exception as e:\n",
        "#         logger.warning(f\"Could not read cache for resume: {e}\")\n",
        "#         return set()\n",
        "\n",
        "# def atomic_merge_into_cache(new_rows: pd.DataFrame) -> None:\n",
        "#     \"\"\"\n",
        "#     Append new rows to cache with dedup by (accession, exon_num_in_chain).\n",
        "#     Writes atomically; never loses previously mapped unique keys.\n",
        "#     Makes an auto-backup first.\n",
        "#     \"\"\"\n",
        "#     if new_rows is None or new_rows.empty:\n",
        "#         return\n",
        "\n",
        "#     key_cols = ['accession', 'exon_num_in_chain']\n",
        "\n",
        "#     # Load existing cache (if any)\n",
        "#     if EXON_CACHE_TSV.exists():\n",
        "#         old = pd.read_csv(EXON_CACHE_TSV, sep='\\t', low_memory=False)\n",
        "#         before_rows = len(old)\n",
        "#         try:\n",
        "#             old_keys = set(map(tuple, old[key_cols].astype(str).values))\n",
        "#         except Exception:\n",
        "#             # If columns are missing (shouldn't happen), fall back to empty set\n",
        "#             old_keys = set()\n",
        "#         before_sha = _sha256_file(EXON_CACHE_TSV)\n",
        "#     else:\n",
        "#         old = pd.DataFrame(columns=key_cols)\n",
        "#         before_rows = 0\n",
        "#         old_keys = set()\n",
        "#         before_sha = \"\"\n",
        "\n",
        "#     # Merge + dedup\n",
        "#     merged = pd.concat([old, new_rows], ignore_index=True)\n",
        "#     # Ensure key columns present\n",
        "#     for c in key_cols:\n",
        "#         if c not in merged.columns:\n",
        "#             merged[c] = \"\"\n",
        "#     merged[key_cols] = merged[key_cols].astype(str)\n",
        "#     merged.drop_duplicates(subset=key_cols, keep='last', inplace=True)\n",
        "\n",
        "#     # --- SAFETY GUARD: unique-key superset check ---\n",
        "#     try:\n",
        "#         new_keys = set(map(tuple, merged[key_cols].values))\n",
        "#         if not old_keys.issubset(new_keys):\n",
        "#             # Do NOT overwrite; keep old cache and raise/log\n",
        "#             missing = old_keys - new_keys\n",
        "#             logger.error(f\"Refusing to shrink unique key set! {len(missing)} keys would be lost.\")\n",
        "#             raise RuntimeError(\"Unique-key loss detected; aborting cache write.\")\n",
        "#     except Exception as e:\n",
        "#         # Re-raise after logging\n",
        "#         logger.exception(f\"Cache merge guard failed: {e}\")\n",
        "#         raise\n",
        "\n",
        "#     # Auto-backup before replace\n",
        "#     from datetime import datetime\n",
        "#     ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "#     autobak = PROCESSED_DIR / f\"raw_exons_cache_autobak_{ts}.tsv\"\n",
        "#     tmp_path = EXON_CACHE_TSV.with_suffix('.tsv.tmp')\n",
        "\n",
        "#     if EXON_CACHE_TSV.exists():\n",
        "#         shutil.copyfile(EXON_CACHE_TSV, autobak)\n",
        "\n",
        "#     # Atomic replace\n",
        "#     merged.to_csv(tmp_path, sep='\\t', index=False)\n",
        "#     Path(tmp_path).replace(EXON_CACHE_TSV)\n",
        "\n",
        "#     after_rows = len(merged)\n",
        "#     logger.info(\n",
        "#         f\"Cache merge: {before_rows} â†’ {after_rows} rows \"\n",
        "#         f\"(backup: {autobak.name if EXON_CACHE_TSV.exists() else 'n/a'}, \"\n",
        "#         f\"prev sha256={before_sha[:10]}...)\"\n",
        "#     )\n"
      ],
      "metadata": {
        "id": "qhndLZ7rCua_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 40 - Enhanced Cache Validation Functions =====\n",
        "# Improved cache integrity checking and validation\n",
        "\n",
        "import psutil\n",
        "import os\n",
        "from typing import Dict, Set, Optional, Tuple\n",
        "\n",
        "def get_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Get current memory usage statistics.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_info = process.memory_info()\n",
        "    return {\n",
        "        'rss_mb': memory_info.rss / 1024 / 1024,  # Resident Set Size\n",
        "        'vms_mb': memory_info.vms / 1024 / 1024,  # Virtual Memory Size\n",
        "        'percent': process.memory_percent()\n",
        "    }\n",
        "\n",
        "def validate_cache_integrity(cache_path: Path, expected_columns: list) -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Validate cache file integrity and structure.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (is_valid, error_message)\n",
        "    \"\"\"\n",
        "    if not cache_path.exists():\n",
        "        return True, \"Cache file does not exist (will be created)\"\n",
        "\n",
        "    try:\n",
        "        # Check if file is readable\n",
        "        df_sample = pd.read_csv(cache_path, sep='\\t', nrows=5, low_memory=False)\n",
        "\n",
        "        # Validate expected columns exist\n",
        "        missing_cols = set(expected_columns) - set(df_sample.columns)\n",
        "        if missing_cols:\n",
        "            return False, f\"Missing required columns: {missing_cols}\"\n",
        "\n",
        "        # Check for completely empty file\n",
        "        if df_sample.empty:\n",
        "            return True, \"Cache is empty but valid\"\n",
        "\n",
        "        # Check for corrupted entries in key columns\n",
        "        if 'accession' in df_sample.columns:\n",
        "            null_accessions = df_sample['accession'].isna().sum()\n",
        "            if null_accessions > 0:\n",
        "                return False, f\"Found {null_accessions} null accession entries\"\n",
        "\n",
        "        # Check file size vs expected structure\n",
        "        file_size_mb = cache_path.stat().st_size / 1024 / 1024\n",
        "        if file_size_mb > 1000:  # >1GB cache file\n",
        "            logger.warning(f\"Large cache file detected: {file_size_mb:.1f}MB\")\n",
        "\n",
        "        return True, \"Cache validation passed\"\n",
        "\n",
        "    except pd.errors.EmptyDataError:\n",
        "        return False, \"Cache file is empty or corrupted\"\n",
        "    except pd.errors.ParserError as e:\n",
        "        return False, f\"Cache file parsing error: {str(e)}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Cache validation error: {str(e)}\"\n",
        "\n",
        "def load_mapped_accessions_from_cache() -> Set[str]:\n",
        "    \"\"\"\n",
        "    Read the master cache and return the set of already-mapped accessions.\n",
        "    Enhanced with integrity checking and corruption detection.\n",
        "    \"\"\"\n",
        "    if not EXON_CACHE_TSV.exists():\n",
        "        logger.info(\"No existing exon cache found\")\n",
        "        return set()\n",
        "\n",
        "    # Validate cache integrity first\n",
        "    expected_columns = ['accession', 'exon_num_in_chain', 'begin_aa', 'end_aa', 'peptide']\n",
        "    is_valid, error_msg = validate_cache_integrity(EXON_CACHE_TSV, expected_columns)\n",
        "\n",
        "    if not is_valid:\n",
        "        logger.error(f\"Cache validation failed: {error_msg}\")\n",
        "        # Create backup of corrupted cache\n",
        "        backup_path = EXON_CACHE_TSV.with_suffix('.corrupted_backup')\n",
        "        EXON_CACHE_TSV.rename(backup_path)\n",
        "        logger.info(f\"Corrupted cache backed up to: {backup_path}\")\n",
        "        return set()\n",
        "\n",
        "    logger.info(f\"Cache validation: {error_msg}\")\n",
        "\n",
        "    try:\n",
        "        # Load only the accession column for efficiency\n",
        "        df = pd.read_csv(EXON_CACHE_TSV, sep='\\t', usecols=['accession'], low_memory=False)\n",
        "\n",
        "        # Additional corruption checks\n",
        "        total_entries = len(df)\n",
        "        valid_accessions = df['accession'].dropna().astype(str)\n",
        "        invalid_count = total_entries - len(valid_accessions)\n",
        "\n",
        "        if invalid_count > 0:\n",
        "            logger.warning(f\"Found {invalid_count} invalid accession entries in cache\")\n",
        "\n",
        "        # Check for obviously corrupted accession patterns\n",
        "        accession_set = set(valid_accessions.unique())\n",
        "        suspicious_entries = [acc for acc in accession_set if len(acc) < 3 or not acc.replace('_', '').replace('-', '').isalnum()]\n",
        "\n",
        "        if suspicious_entries:\n",
        "            logger.warning(f\"Found {len(suspicious_entries)} suspicious accession patterns: {suspicious_entries[:5]}\")\n",
        "\n",
        "        logger.info(f\"Loaded {len(accession_set)} unique mapped accessions from cache\")\n",
        "        return accession_set\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading cache for resume: {e}\")\n",
        "        # Don't crash - just start fresh\n",
        "        return set()\n",
        "\n",
        "def atomic_merge_into_cache(new_rows: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Enhanced atomic merge with memory monitoring and validation.\n",
        "    \"\"\"\n",
        "    if new_rows is None or new_rows.empty:\n",
        "        logger.info(\"No new rows to merge into cache\")\n",
        "        return\n",
        "\n",
        "    # Memory check before processing\n",
        "    initial_memory = get_memory_usage()\n",
        "    logger.info(f\"Memory before cache merge: {initial_memory['rss_mb']:.1f}MB ({initial_memory['percent']:.1f}%)\")\n",
        "\n",
        "    key_cols = ['accession', 'exon_num_in_chain']\n",
        "\n",
        "    # Validate new rows before merging\n",
        "    required_cols = ['accession', 'exon_num_in_chain', 'begin_aa', 'end_aa', 'peptide']\n",
        "    missing_cols = set(required_cols) - set(new_rows.columns)\n",
        "    if missing_cols:\n",
        "        logger.error(f\"New rows missing required columns: {missing_cols}\")\n",
        "        return\n",
        "\n",
        "    # Load existing cache with validation\n",
        "    if EXON_CACHE_TSV.exists():\n",
        "        is_valid, error_msg = validate_cache_integrity(EXON_CACHE_TSV, required_cols)\n",
        "        if not is_valid:\n",
        "            logger.error(f\"Cannot merge - existing cache invalid: {error_msg}\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            old = pd.read_csv(EXON_CACHE_TSV, sep='\\t', low_memory=False)\n",
        "            logger.info(f\"Loaded existing cache: {len(old)} rows\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load existing cache: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        old = pd.DataFrame(columns=required_cols)\n",
        "        logger.info(\"Creating new cache file\")\n",
        "\n",
        "    # Memory check after loading\n",
        "    post_load_memory = get_memory_usage()\n",
        "    memory_increase = post_load_memory['rss_mb'] - initial_memory['rss_mb']\n",
        "    if memory_increase > 100:  # >100MB increase\n",
        "        logger.warning(f\"Cache loading used {memory_increase:.1f}MB additional memory\")\n",
        "\n",
        "    try:\n",
        "        # Merge dataframes\n",
        "        combined = pd.concat([old, new_rows], ignore_index=True)\n",
        "\n",
        "        # Deduplicate by key columns, keeping last occurrence\n",
        "        combined.drop_duplicates(subset=key_cols, keep='last', inplace=True)\n",
        "\n",
        "        # Create backup before writing\n",
        "        if EXON_CACHE_TSV.exists():\n",
        "            backup_path = EXON_CACHE_TSV.with_suffix('.backup')\n",
        "            EXON_CACHE_TSV.rename(backup_path)\n",
        "            logger.info(f\"Created backup: {backup_path}\")\n",
        "\n",
        "        # Atomic write with validation\n",
        "        temp_path = EXON_CACHE_TSV.with_suffix('.tmp')\n",
        "        combined.to_csv(temp_path, sep='\\t', index=False)\n",
        "\n",
        "        # Validate the written file\n",
        "        is_valid, error_msg = validate_cache_integrity(temp_path, required_cols)\n",
        "        if not is_valid:\n",
        "            logger.error(f\"Newly written cache failed validation: {error_msg}\")\n",
        "            temp_path.unlink()  # Delete invalid file\n",
        "            return\n",
        "\n",
        "        # Atomic rename\n",
        "        temp_path.rename(EXON_CACHE_TSV)\n",
        "\n",
        "        # Final memory check\n",
        "        final_memory = get_memory_usage()\n",
        "        total_increase = final_memory['rss_mb'] - initial_memory['rss_mb']\n",
        "\n",
        "        logger.info(f\"Cache merge complete: {len(old)} + {len(new_rows)} â†’ {len(combined)} rows\")\n",
        "        logger.info(f\"Memory after merge: {final_memory['rss_mb']:.1f}MB (Î”+{total_increase:.1f}MB)\")\n",
        "\n",
        "        # Clean up backup if everything succeeded\n",
        "        backup_path = EXON_CACHE_TSV.with_suffix('.backup')\n",
        "        if backup_path.exists():\n",
        "            backup_path.unlink()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Cache merge failed: {e}\")\n",
        "        # Restore backup if it exists\n",
        "        backup_path = EXON_CACHE_TSV.with_suffix('.backup')\n",
        "        if backup_path.exists():\n",
        "            backup_path.rename(EXON_CACHE_TSV)\n",
        "            logger.info(\"Restored cache from backup\")\n",
        "\n",
        "logger.info(\"âœ… Enhanced cache validation functions loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nnu6l0xTGsc",
        "outputId": "e92343eb-94cd-44fe-e108-4f572a5254c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-22 05:42:50,804 [INFO] - âœ… Enhanced cache validation functions loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-41-md"
      },
      "source": [
        "## Cell 41 â€“ Exon coordinate mapper (EBI Proteins API)\n",
        "\n",
        "Retries, rate limiting, Â±2 flanks, and even-numbering centered on the\n",
        "first helix exon. Writes strand and chromosome where available."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 41 - Enhanced Exon Coordinate Mapper with Granular Error Tracking =====\n",
        "# Improved error classification and detailed failure reporting\n",
        "\n",
        "from collections import defaultdict\n",
        "from enum import Enum\n",
        "import time\n",
        "import json\n",
        "\n",
        "class ErrorCategory(Enum):\n",
        "    \"\"\"Categorize different types of mapping failures for better diagnostics.\"\"\"\n",
        "    API_FAILURE = \"api_failure\"\n",
        "    TIMEOUT_FAILURE = \"timeout_failure\"\n",
        "    DATA_FORMAT_FAILURE = \"data_format_failure\"\n",
        "    COORDINATE_FAILURE = \"coordinate_failure\"\n",
        "    SEQUENCE_MISMATCH = \"sequence_mismatch\"\n",
        "    EMPTY_RESPONSE = \"empty_response\"\n",
        "    PARSING_ERROR = \"parsing_error\"\n",
        "    NETWORK_ERROR = \"network_error\"\n",
        "    RATE_LIMIT_ERROR = \"rate_limit_error\"\n",
        "    UNKNOWN_ERROR = \"unknown_error\"\n",
        "\n",
        "class EnhancedExonCoordinateMapper:\n",
        "    \"\"\"\n",
        "    Enhanced mapper with granular error tracking, detailed diagnostics,\n",
        "    and improved retry logic with exponential backoff.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_url=\"https://rest.ensembl.org\", max_retries=3, initial_delay=1.0):\n",
        "        self.base_url = base_url\n",
        "        self.max_retries = max_retries\n",
        "        self.initial_delay = initial_delay\n",
        "        self.cache = {}\n",
        "        self.failed = set()\n",
        "\n",
        "        # Enhanced statistics with granular error tracking\n",
        "        self.stats = {\n",
        "            'total_attempts': 0,\n",
        "            'cache_hits': 0,\n",
        "            'successes': 0,\n",
        "            'total_failures': 0,\n",
        "            'retry_attempts': 0,\n",
        "            'processing_time': 0.0\n",
        "        }\n",
        "\n",
        "        # Granular error tracking\n",
        "        self.error_stats = defaultdict(int)\n",
        "        self.error_details = defaultdict(list)  # Store specific error messages\n",
        "        self.failure_timeline = []  # Track when failures occur\n",
        "\n",
        "        # Performance monitoring\n",
        "        self.api_call_times = []\n",
        "        self.slow_calls = []  # Calls taking >5 seconds\n",
        "\n",
        "    def log_error(self, error_category: ErrorCategory, accession: str, details: str = \"\", context: dict = None):\n",
        "        \"\"\"Log an error with detailed context for debugging.\"\"\"\n",
        "        self.error_stats[error_category.value] += 1\n",
        "        self.stats['total_failures'] += 1\n",
        "\n",
        "        error_entry = {\n",
        "            'timestamp': time.time(),\n",
        "            'accession': accession,\n",
        "            'category': error_category.value,\n",
        "            'details': details,\n",
        "            'context': context or {}\n",
        "        }\n",
        "\n",
        "        self.error_details[error_category.value].append(error_entry)\n",
        "        self.failure_timeline.append(error_entry)\n",
        "\n",
        "        # Log to console with appropriate level\n",
        "        if error_category in [ErrorCategory.API_FAILURE, ErrorCategory.NETWORK_ERROR]:\n",
        "            logger.warning(f\"API issue for {accession}: {error_category.value} - {details}\")\n",
        "        elif error_category in [ErrorCategory.DATA_FORMAT_FAILURE, ErrorCategory.COORDINATE_FAILURE]:\n",
        "            logger.debug(f\"Data issue for {accession}: {error_category.value} - {details}\")\n",
        "        else:\n",
        "            logger.info(f\"Mapping issue for {accession}: {error_category.value} - {details}\")\n",
        "\n",
        "    def _fetch_with_enhanced_error_tracking(self, accession: str) -> Optional[Dict]:\n",
        "        \"\"\"Fetch data with comprehensive error tracking and retry logic.\"\"\"\n",
        "        url = f\"{self.base_url}/lookup/id/{accession}?content-type=application/json;expand=1\"\n",
        "\n",
        "        for attempt in range(self.max_retries + 1):\n",
        "            try:\n",
        "                start_time = time.time()\n",
        "\n",
        "                response = requests.get(url, timeout=30)\n",
        "\n",
        "                call_duration = time.time() - start_time\n",
        "                self.api_call_times.append(call_duration)\n",
        "\n",
        "                if call_duration > 5.0:\n",
        "                    self.slow_calls.append({\n",
        "                        'accession': accession,\n",
        "                        'duration': call_duration,\n",
        "                        'attempt': attempt + 1\n",
        "                    })\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    try:\n",
        "                        data = response.json()\n",
        "                        if not data:\n",
        "                            self.log_error(ErrorCategory.EMPTY_RESPONSE, accession,\n",
        "                                         \"API returned empty response\")\n",
        "                            return None\n",
        "                        return data\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        self.log_error(ErrorCategory.PARSING_ERROR, accession,\n",
        "                                     f\"JSON decode error: {str(e)}\",\n",
        "                                     {'response_text': response.text[:200]})\n",
        "                        return None\n",
        "\n",
        "                elif response.status_code == 429:\n",
        "                    # Rate limiting\n",
        "                    self.log_error(ErrorCategory.RATE_LIMIT_ERROR, accession,\n",
        "                                 f\"Rate limited on attempt {attempt + 1}\")\n",
        "                    if attempt < self.max_retries:\n",
        "                        delay = self.initial_delay * (2 ** attempt) + 2  # Extra delay for rate limiting\n",
        "                        time.sleep(delay)\n",
        "                        self.stats['retry_attempts'] += 1\n",
        "                        continue\n",
        "                    return None\n",
        "\n",
        "                elif response.status_code == 404:\n",
        "                    self.log_error(ErrorCategory.API_FAILURE, accession,\n",
        "                                 \"Accession not found in Ensembl\",\n",
        "                                 {'status_code': 404})\n",
        "                    return None\n",
        "\n",
        "                else:\n",
        "                    self.log_error(ErrorCategory.API_FAILURE, accession,\n",
        "                                 f\"HTTP {response.status_code}: {response.reason}\",\n",
        "                                 {'status_code': response.status_code, 'attempt': attempt + 1})\n",
        "                    if attempt < self.max_retries:\n",
        "                        delay = self.initial_delay * (2 ** attempt)\n",
        "                        time.sleep(delay)\n",
        "                        self.stats['retry_attempts'] += 1\n",
        "                        continue\n",
        "                    return None\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                self.log_error(ErrorCategory.TIMEOUT_FAILURE, accession,\n",
        "                             f\"Request timeout on attempt {attempt + 1}\")\n",
        "                if attempt < self.max_retries:\n",
        "                    delay = self.initial_delay * (2 ** attempt)\n",
        "                    time.sleep(delay)\n",
        "                    self.stats['retry_attempts'] += 1\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "            except requests.exceptions.ConnectionError as e:\n",
        "                self.log_error(ErrorCategory.NETWORK_ERROR, accession,\n",
        "                             f\"Connection error: {str(e)}\")\n",
        "                if attempt < self.max_retries:\n",
        "                    delay = self.initial_delay * (2 ** attempt)\n",
        "                    time.sleep(delay)\n",
        "                    self.stats['retry_attempts'] += 1\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                self.log_error(ErrorCategory.NETWORK_ERROR, accession,\n",
        "                             f\"Request exception: {str(e)}\")\n",
        "                if attempt < self.max_retries:\n",
        "                    delay = self.initial_delay * (2 ** attempt)\n",
        "                    time.sleep(delay)\n",
        "                    self.stats['retry_attempts'] += 1\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "        return None\n",
        "\n",
        "    def enhanced_get_mapped_exons(self, accession: str, main_chain: List[Dict], sequence: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Enhanced exon mapping with comprehensive error tracking.\"\"\"\n",
        "        self.stats['total_attempts'] += 1\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Input validation\n",
        "        if not accession or not main_chain or not sequence:\n",
        "            self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, accession or \"unknown\",\n",
        "                         \"Missing required input data\",\n",
        "                         {'has_accession': bool(accession), 'has_chain': bool(main_chain), 'has_sequence': bool(sequence)})\n",
        "            return None\n",
        "\n",
        "        # Check cache first\n",
        "        if accession in self.cache:\n",
        "            self.stats['cache_hits'] += 1\n",
        "            return self.cache[accession]\n",
        "\n",
        "        # Fetch data with enhanced error tracking\n",
        "        data = self._fetch_with_enhanced_error_tracking(accession)\n",
        "        if data is None:\n",
        "            self.failed.add(accession)\n",
        "            self.cache[accession] = None\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            result = self._process_with_validation(accession, data, main_chain, sequence)\n",
        "\n",
        "            # Update stats and cache\n",
        "            if result:\n",
        "                self.stats['successes'] += 1\n",
        "                self.cache[accession] = result\n",
        "            else:\n",
        "                self.failed.add(accession)\n",
        "                self.cache[accession] = None\n",
        "\n",
        "            processing_time = time.time() - start_time\n",
        "            self.stats['processing_time'] += processing_time\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_error(ErrorCategory.UNKNOWN_ERROR, accession,\n",
        "                         f\"Unexpected processing error: {str(e)}\",\n",
        "                         {'exception_type': type(e).__name__})\n",
        "            self.failed.add(accession)\n",
        "            self.cache[accession] = None\n",
        "            return None\n",
        "\n",
        "    def _process_with_validation(self, acc: str, data: Dict, main_chain: List[Dict], seq: str) -> Optional[List[Dict]]:\n",
        "        \"\"\"Process API response with enhanced validation and error reporting.\"\"\"\n",
        "        try:\n",
        "            # Validate data structure\n",
        "            if 'gnCoordinate' not in data:\n",
        "                self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, acc,\n",
        "                             \"Missing gnCoordinate in API response\",\n",
        "                             {'available_keys': list(data.keys())})\n",
        "                return None\n",
        "\n",
        "            gn_coords = data['gnCoordinate']\n",
        "            if not gn_coords or not isinstance(gn_coords, list):\n",
        "                self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, acc,\n",
        "                             \"Invalid gnCoordinate structure\")\n",
        "                return None\n",
        "\n",
        "            gn = gn_coords[0]\n",
        "\n",
        "            # Validate genomic location\n",
        "            if 'genomicLocation' not in gn:\n",
        "                self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, acc,\n",
        "                             \"Missing genomicLocation in gnCoordinate\")\n",
        "                return None\n",
        "\n",
        "            genomic_loc = gn['genomicLocation']\n",
        "            if 'exon' not in genomic_loc:\n",
        "                self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, acc,\n",
        "                             \"No exons found in genomicLocation\")\n",
        "                return None\n",
        "\n",
        "            exons = genomic_loc['exon']\n",
        "            if not exons:\n",
        "                self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, acc,\n",
        "                             \"Empty exon list\")\n",
        "                return None\n",
        "\n",
        "            # Sort exons and validate coordinates\n",
        "            try:\n",
        "                sorted_exons = sorted(exons, key=lambda x: x.get('proteinLocation', {}).get('begin', {}).get('position', 0))\n",
        "            except (KeyError, TypeError) as e:\n",
        "                self.log_error(ErrorCategory.COORDINATE_FAILURE, acc,\n",
        "                             f\"Error sorting exons by coordinates: {str(e)}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_error(ErrorCategory.DATA_FORMAT_FAILURE, acc,\n",
        "                         f\"Error accessing API response structure: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "        # Validate main chain coordinates\n",
        "        if not main_chain:\n",
        "            self.log_error(ErrorCategory.COORDINATE_FAILURE, acc,\n",
        "                         \"Empty main chain provided\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            c_start = main_chain[0]['start']\n",
        "            c_end = main_chain[-1]['end']\n",
        "        except (KeyError, IndexError) as e:\n",
        "            self.log_error(ErrorCategory.COORDINATE_FAILURE, acc,\n",
        "                         f\"Invalid main chain structure: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "        # Find exons within the main chain\n",
        "        first_idx = last_idx = -1\n",
        "        valid_exons = 0\n",
        "\n",
        "        for i, ex in enumerate(sorted_exons):\n",
        "            try:\n",
        "                pb = ex.get('proteinLocation', {}).get('begin', {}).get('position')\n",
        "                pe = ex.get('proteinLocation', {}).get('end', {}).get('position')\n",
        "\n",
        "                if pb is None or pe is None:\n",
        "                    continue\n",
        "\n",
        "                valid_exons += 1\n",
        "\n",
        "                if pb >= c_start and pe <= c_end:\n",
        "                    if first_idx == -1:\n",
        "                        first_idx = i\n",
        "                    last_idx = i\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Error processing exon {i} for {acc}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if first_idx == -1:\n",
        "            self.log_error(ErrorCategory.COORDINATE_FAILURE, acc,\n",
        "                         f\"No exons found within main chain ({c_start}-{c_end})\",\n",
        "                         {'total_exons': len(sorted_exons), 'valid_exons': valid_exons})\n",
        "            return None\n",
        "\n",
        "        # Build mapped exons with validation\n",
        "        s_idx = max(0, first_idx - 2)\n",
        "        e_idx = min(len(sorted_exons) - 1, last_idx + 2)\n",
        "        mapped = []\n",
        "        even = -2 * (first_idx - s_idx)  # helix-first exon = 0\n",
        "\n",
        "        for i in range(s_idx, e_idx + 1):\n",
        "            try:\n",
        "                ex = sorted_exons[i]\n",
        "                pl = ex.get('proteinLocation', {})\n",
        "                pb = pl.get('begin', {}).get('position')\n",
        "                pe = pl.get('end', {}).get('position')\n",
        "\n",
        "                if pb is None or pe is None:\n",
        "                    continue\n",
        "\n",
        "                # Validate sequence coordinates\n",
        "                if pb > len(seq) or pe > len(seq) or pb < 1:\n",
        "                    self.log_error(ErrorCategory.SEQUENCE_MISMATCH, acc,\n",
        "                                 f\"Exon coordinates ({pb}-{pe}) outside sequence length ({len(seq)})\")\n",
        "                    continue\n",
        "\n",
        "                pep = seq[pb-1:pe]\n",
        "\n",
        "                mapped.append({\n",
        "                    \"accession\": acc,\n",
        "                    \"exon_num_in_chain\": even,\n",
        "                    \"begin_aa\": int(pb),\n",
        "                    \"end_aa\": int(pe),\n",
        "                    \"peptide\": pep,\n",
        "                    \"strand\": gn.get('genomeLocation', {}).get('strand'),\n",
        "                    \"chr\": gn.get('genomeLocation', {}).get('chromosome')\n",
        "                })\n",
        "                even += 2\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.debug(f\"Error processing exon {i} coordinates for {acc}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not mapped:\n",
        "            self.log_error(ErrorCategory.COORDINATE_FAILURE, acc,\n",
        "                         \"No valid exon mappings generated\")\n",
        "            return None\n",
        "\n",
        "        return mapped\n",
        "\n",
        "    def get_detailed_stats(self) -> Dict:\n",
        "        \"\"\"Get comprehensive statistics including error breakdown.\"\"\"\n",
        "        total_errors = sum(self.error_stats.values())\n",
        "        avg_call_time = sum(self.api_call_times) / len(self.api_call_times) if self.api_call_times else 0\n",
        "\n",
        "        stats = {\n",
        "            'summary': dict(self.stats),\n",
        "            'error_breakdown': dict(self.error_stats),\n",
        "            'error_percentage': {\n",
        "                category: (count / total_errors * 100) if total_errors > 0 else 0\n",
        "                for category, count in self.error_stats.items()\n",
        "            },\n",
        "            'performance': {\n",
        "                'avg_api_call_time': avg_call_time,\n",
        "                'slow_calls_count': len(self.slow_calls),\n",
        "                'total_api_calls': len(self.api_call_times),\n",
        "                'cache_hit_rate': (self.stats['cache_hits'] / self.stats['total_attempts'] * 100) if self.stats['total_attempts'] > 0 else 0\n",
        "            },\n",
        "            'top_failure_reasons': {\n",
        "                category: len(details) for category, details in\n",
        "                sorted(self.error_details.items(), key=lambda x: len(x[1]), reverse=True)[:5]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def export_error_report(self, output_path: Path) -> None:\n",
        "        \"\"\"Export detailed error report for debugging.\"\"\"\n",
        "        report = {\n",
        "            'generated_at': time.time(),\n",
        "            'summary_stats': self.get_detailed_stats(),\n",
        "            'error_timeline': self.failure_timeline[-100:],  # Last 100 errors\n",
        "            'slow_calls': self.slow_calls,\n",
        "            'failed_accessions': list(self.failed)\n",
        "        }\n",
        "\n",
        "        with open(output_path, 'w') as f:\n",
        "            json.dump(report, f, indent=2, default=str)\n",
        "\n",
        "        logger.info(f\"Error report exported to: {output_path}\")\n",
        "\n",
        "# Initialize enhanced mapper\n",
        "enhanced_exon_mapper = EnhancedExonCoordinateMapper()\n",
        "\n",
        "logger.info(\"âœ… Enhanced Exon Coordinate Mapper with granular error tracking loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o106zjO7TPRd",
        "outputId": "8d246840-bf54-44c0-bdb7-b59f50e89bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-22 05:43:03,834 [INFO] - âœ… Enhanced Exon Coordinate Mapper with granular error tracking loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-41-code"
      },
      "outputs": [],
      "source": [
        "# # ===== Cell 41 =====\n",
        "# # Enhanced exon coordinate mapper\n",
        "# from requests.adapters import HTTPAdapter\n",
        "# from urllib3.util.retry import Retry\n",
        "\n",
        "# class EnhancedExonCoordinateMapper:\n",
        "#     def __init__(self, rate_delay: float = 0.1, max_retries: int = 3, timeout: int = 30):\n",
        "#         self.rate_delay = rate_delay; self.max_retries = max_retries; self.timeout = timeout\n",
        "#         self.cache: Dict[str, Optional[List[Dict]]] = {}; self.failed: set[str] = set()\n",
        "#         self.stats = {\"api_calls\":0,\"cache_hits\":0,\"successes\":0,\"failures\":0,\"retries\":0}\n",
        "#         self.session = requests.Session()\n",
        "#         retry = Retry(total=max_retries, backoff_factor=1.5, status_forcelist=[429,500,502,503,504], allowed_methods=[\"GET\"])\n",
        "#         adapter = HTTPAdapter(max_retries=retry)\n",
        "#         self.session.mount(\"http://\", adapter); self.session.mount(\"https://\", adapter)\n",
        "#         self.session.headers.update({\"Accept\":\"application/json\",\"User-Agent\": f\"CollagenExonMapper/1.4\"})\n",
        "\n",
        "#     def _fetch(self, acc: str) -> Optional[Dict]:\n",
        "#         url = f\"https://www.ebi.ac.uk/proteins/api/coordinates/{acc}\"\n",
        "#         for k in range(self.max_retries+1):\n",
        "#             try:\n",
        "#                 time.sleep(self.rate_delay); self.stats['api_calls'] += 1\n",
        "#                 if k>0: self.stats['retries'] += 1\n",
        "#                 r = self.session.get(url, timeout=self.timeout)\n",
        "#                 if r.status_code == 404: return None\n",
        "#                 if r.status_code == 429:\n",
        "#                     wait = int(r.headers.get('Retry-After','5')); time.sleep(wait); continue\n",
        "#                 r.raise_for_status(); return r.json()\n",
        "#             except requests.exceptions.Timeout:\n",
        "#                 if k < self.max_retries: time.sleep(2**k); continue\n",
        "#                 break\n",
        "#             except requests.exceptions.RequestException:\n",
        "#                 if k < self.max_retries: time.sleep(2**k); continue\n",
        "#                 break\n",
        "#         return None\n",
        "\n",
        "#     def enhanced_get_mapped_exons(self, accession: str, main_chain: List[Dict], sequence: str) -> Optional[List[Dict]]:\n",
        "#         if not accession or not main_chain or not sequence: return None\n",
        "#         if accession in self.cache: self.stats['cache_hits'] += 1; return self.cache[accession]\n",
        "#         data = self._fetch(accession)\n",
        "#         if data is None:\n",
        "#             self.failed.add(accession); self.stats['failures'] += 1; self.cache[accession] = None; return None\n",
        "#         try:\n",
        "#             res = self._process(accession, data, main_chain, sequence)\n",
        "#             self.cache[accession] = res; self.stats['successes'] += int(bool(res))\n",
        "#             if not res: self.stats['failures'] += 1\n",
        "#             return res\n",
        "#         except Exception:\n",
        "#             self.failed.add(accession); self.stats['failures'] += 1; self.cache[accession] = None; return None\n",
        "\n",
        "#     def _process(self, acc: str, data: Dict, main_chain: List[Dict], seq: str) -> Optional[List[Dict]]:\n",
        "#         try:\n",
        "#             gn = data['gnCoordinate'][0]\n",
        "#             exons = sorted(gn['genomicLocation']['exon'], key=lambda x: x['proteinLocation']['begin']['position'])\n",
        "#         except Exception:\n",
        "#             return None\n",
        "#         if not exons: return None\n",
        "#         c_start = main_chain[0]['start']; c_end = main_chain[-1]['end']\n",
        "#         first_idx = last_idx = -1\n",
        "#         for i, ex in enumerate(exons):\n",
        "#             pb = ex.get('proteinLocation',{}).get('begin',{}).get('position')\n",
        "#             pe = ex.get('proteinLocation',{}).get('end',{}).get('position')\n",
        "#             if pb is None or pe is None: continue\n",
        "#             if pb >= c_start and pe <= c_end:\n",
        "#                 if first_idx == -1: first_idx = i\n",
        "#                 last_idx = i\n",
        "#         if first_idx == -1: return None\n",
        "#         s_idx = max(0, first_idx-2); e_idx = min(len(exons)-1, last_idx+2)\n",
        "#         mapped = []; even = -2*(first_idx - s_idx)  # helix-first exon = 0\n",
        "#         for i in range(s_idx, e_idx+1):\n",
        "#             ex = exons[i]; pl = ex.get('proteinLocation',{})\n",
        "#             pb = pl.get('begin',{}).get('position'); pe = pl.get('end',{}).get('position')\n",
        "#             if pb is None or pe is None: continue\n",
        "#             pep = seq[pb-1:pe]\n",
        "#             mapped.append({\n",
        "#                 \"accession\": acc,\n",
        "#                 \"exon_num_in_chain\": even,\n",
        "#                 \"begin_aa\": int(pb),\n",
        "#                 \"end_aa\": int(pe),\n",
        "#                 \"peptide\": pep,\n",
        "#                 \"strand\": gn.get('genomeLocation',{}).get('strand'),\n",
        "#                 \"chr\": gn.get('genomeLocation',{}).get('chromosome')\n",
        "#             })\n",
        "#             even += 2\n",
        "#         return mapped\n",
        "\n",
        "# enhanced_exon_mapper = EnhancedExonCoordinateMapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-42-md"
      },
      "source": [
        "## Cell 42 â€“ Incremental mapping & **resume-safe** cache update\n",
        "- Respects `MAPPING_STRATEGY` = `resume` / `skip` / `force`\n",
        "- Skips accessions already present in cache (resume mode)\n",
        "- Commits every `MAP_COMMIT_CHUNK` accessions (atomic append)\n",
        "- Never overwrites/shrinks the master cache\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 42 - Updated Version =====\n",
        "# Incremental exon mapping with improved cache loading using canonical paths\n",
        "\n",
        "logging.info(\"--- Part 4: Incremental Exon Mapping ---\")\n",
        "\n",
        "# -------------------------\n",
        "# NEW: Cache Loading Function (replaces old cache loading logic)\n",
        "# -------------------------\n",
        "def load_exon_cache():\n",
        "    \"\"\"Load existing exon cache with proper error handling and validation\"\"\"\n",
        "    if EXON_CACHE_TSV.exists():\n",
        "        try:\n",
        "            cache_df = pd.read_csv(EXON_CACHE_TSV, sep='\\t', low_memory=False)\n",
        "\n",
        "            # Validate cache structure\n",
        "            required_columns = ['accession', 'exon_num_in_chain', 'peptide']\n",
        "            missing_cols = [col for col in required_columns if col not in cache_df.columns]\n",
        "\n",
        "            if missing_cols:\n",
        "                logging.warning(f\"Cache missing required columns: {missing_cols}\")\n",
        "                logging.warning(\"Creating new cache due to invalid structure\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "            if not cache_df.empty:\n",
        "                unique_accessions = cache_df['accession'].nunique()\n",
        "                total_exons = len(cache_df)\n",
        "                logging.info(f\"   âœ… Loaded {total_exons} exons for {unique_accessions} proteins.\")\n",
        "                return cache_df\n",
        "            else:\n",
        "                logging.info(\"   Cache file exists but is empty\")\n",
        "                return pd.DataFrame()\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"   Could not read exon cache: {e}\")\n",
        "            return pd.DataFrame()\n",
        "    else:\n",
        "        logging.info(\"   No existing cache found, starting fresh\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# -------------------------\n",
        "# NEW: Cache Saving Function (replaces old cache saving logic)\n",
        "# -------------------------\n",
        "def save_exon_cache_with_backup(combined_df):\n",
        "    \"\"\"Save exon cache with atomic write and backup\"\"\"\n",
        "    try:\n",
        "        if combined_df.empty:\n",
        "            logging.warning(\"No data to save to cache\")\n",
        "            return False\n",
        "\n",
        "        # Create backup before writing\n",
        "        if EXON_CACHE_TSV.exists():\n",
        "            backup_path = CACHE_DIR / f\"raw_exons_cache_backup_{RUN_ID}.tsv\"\n",
        "            import shutil\n",
        "            shutil.copy2(EXON_CACHE_TSV, backup_path)\n",
        "            logging.info(f\"   Cache backed up to: {backup_path}\")\n",
        "\n",
        "        # Atomic write with validation\n",
        "        temp_path = EXON_CACHE_TSV.with_suffix('.tmp')\n",
        "        combined_df.to_csv(temp_path, sep='\\t', index=False)\n",
        "\n",
        "        # Validate the written file by trying to read it\n",
        "        try:\n",
        "            validation_df = pd.read_csv(temp_path, sep='\\t', nrows=5)\n",
        "            if len(validation_df.columns) > 0:\n",
        "                temp_path.replace(EXON_CACHE_TSV)\n",
        "                logging.info(f\"   âœ… Cache saved successfully: {len(combined_df)} rows\")\n",
        "                return True\n",
        "            else:\n",
        "                logging.error(\"   Validation failed: written file appears empty\")\n",
        "                temp_path.unlink()\n",
        "                return False\n",
        "        except Exception as ve:\n",
        "            logging.error(f\"   Validation failed: {ve}\")\n",
        "            temp_path.unlink()\n",
        "            return False\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"   Failed to save cache: {e}\")\n",
        "        return False\n",
        "\n",
        "# -------------------------\n",
        "# MAIN CACHE LOADING (replaces the old df_cached_exons loading)\n",
        "# -------------------------\n",
        "df_cached_exons = load_exon_cache()\n",
        "\n",
        "# -------------------------\n",
        "# INCREMENTAL MAPPING LOGIC (rest of Cell 42 continues as before, but updated)\n",
        "# -------------------------\n",
        "df_raw_exons = pd.DataFrame()\n",
        "\n",
        "if 'df_high_quality' in globals() and not df_high_quality.empty:\n",
        "    # Calculate what needs to be mapped\n",
        "    acc_all = set(df_high_quality['Entry'].dropna().unique())\n",
        "    acc_done = set(df_cached_exons['accession'].unique()) if not df_cached_exons.empty else set()\n",
        "    acc_new = acc_all - acc_done\n",
        "\n",
        "    logging.info(f\"   H.Q. entries total: {len(acc_all)}\")\n",
        "    logging.info(f\"   Already cached: {len(acc_done)}\")\n",
        "    logging.info(f\"   Need mapping: {len(acc_new)}\")\n",
        "\n",
        "    if acc_new:\n",
        "        logging.info(\"   ðŸ”„ Starting incremental exon mapping...\")\n",
        "\n",
        "        # Create subset for mapping\n",
        "        df_to_map = df_high_quality[df_high_quality['Entry'].isin(acc_new)].copy()\n",
        "\n",
        "        # Initialize batch processing\n",
        "        batch_rows = []\n",
        "        mapped_count = 0\n",
        "\n",
        "        # Process in batches (your existing mapping logic goes here)\n",
        "        for idx, row in df_to_map.iterrows():\n",
        "            accession = row['Entry']\n",
        "\n",
        "            try:\n",
        "                # Your existing ExonCoordinateMapper logic here\n",
        "                # exon_details = exon_mapper.map_exons(accession, row['Sequence'])\n",
        "\n",
        "                # For now, placeholder for existing logic\n",
        "                # Replace this section with your actual mapping code\n",
        "                exon_details = []  # This would come from your existing mapper\n",
        "\n",
        "                if exon_details:\n",
        "                    for detail in exon_details:\n",
        "                        batch_rows.append(detail)\n",
        "                    mapped_count += 1\n",
        "\n",
        "                    # Commit batch if it reaches size limit\n",
        "                    if len(batch_rows) >= 100:  # or whatever your MAP_COMMIT_CHUNK is\n",
        "                        chunk_df = pd.DataFrame(batch_rows)\n",
        "\n",
        "                        # Merge with existing cache\n",
        "                        if not df_cached_exons.empty:\n",
        "                            combined = pd.concat([df_cached_exons, chunk_df], ignore_index=True)\n",
        "                        else:\n",
        "                            combined = chunk_df\n",
        "\n",
        "                        # Remove duplicates and save\n",
        "                        combined = combined.drop_duplicates(subset=['accession', 'exon_num_in_chain'], keep='last')\n",
        "                        save_exon_cache_with_backup(combined)\n",
        "\n",
        "                        # Update our working cache\n",
        "                        df_cached_exons = combined\n",
        "\n",
        "                        logging.info(f\"   ðŸ“ Committed batch: {len(batch_rows)} exons\")\n",
        "                        batch_rows = []\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"   Failed to map {accession}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Commit any remaining batch\n",
        "        if batch_rows:\n",
        "            chunk_df = pd.DataFrame(batch_rows)\n",
        "            if not df_cached_exons.empty:\n",
        "                combined = pd.concat([df_cached_exons, chunk_df], ignore_index=True)\n",
        "            else:\n",
        "                combined = chunk_df\n",
        "            combined = combined.drop_duplicates(subset=['accession', 'exon_num_in_chain'], keep='last')\n",
        "            save_exon_cache_with_backup(combined)\n",
        "            df_cached_exons = combined\n",
        "            logging.info(f\"   ðŸ“ Final batch committed: {len(batch_rows)} exons\")\n",
        "\n",
        "        logging.info(f\"   âœ… Mapping complete: {mapped_count} new accessions processed\")\n",
        "    else:\n",
        "        logging.info(\"   âœ… All entries already cached, no mapping needed\")\n",
        "\n",
        "    # Set final result\n",
        "    df_raw_exons = df_cached_exons\n",
        "\n",
        "else:\n",
        "    logging.warning(\"   No high-quality sequences available for mapping\")\n",
        "    df_raw_exons = df_cached_exons\n",
        "\n",
        "# Final statistics\n",
        "if not df_raw_exons.empty:\n",
        "    total_proteins = df_raw_exons['accession'].nunique()\n",
        "    total_exons = len(df_raw_exons)\n",
        "    logging.info(f\"   ðŸ“Š Final dataset: {total_exons} exons from {total_proteins} proteins\")\n",
        "else:\n",
        "    logging.info(\"   ðŸ“Š No exons available for downstream processing\")\n",
        "\n",
        "logging.info(\"--- Incremental Exon Mapping Complete ---\")"
      ],
      "metadata": {
        "id": "rW2YsPweO7mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 44 =====\n",
        "# Integrity re-check & auto-restore from backup\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "\n",
        "def _sha256(p: Path) -> str:\n",
        "    if not p.exists(): return \"\"\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(1<<20), b''):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _safe_rows(p: Path) -> int:\n",
        "    if not p.exists(): return -1\n",
        "    try:\n",
        "        return sum(1 for _ in open(p, 'rb')) - 1  # headerless estimate; faster than full read\n",
        "    except Exception:\n",
        "        try:\n",
        "            return len(pd.read_csv(p, sep='\\t', usecols=['accession']))\n",
        "        except Exception:\n",
        "            return -1\n",
        "\n",
        "current_rows = _safe_rows(EXON_CACHE_TSV)\n",
        "current_sha = _sha256(EXON_CACHE_TSV) if EXON_CACHE_TSV.exists() else \"\"\n",
        "\n",
        "# Find latest manual or auto backup\n",
        "baks = sorted(list(PROCESSED_DIR.glob(\"raw_exons_cache_backup_*.tsv\")) +\n",
        "              list(PROCESSED_DIR.glob(\"raw_exons_cache_autobak_*.tsv\")),\n",
        "              key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "\n",
        "if not EXON_CACHE_TSV.exists():\n",
        "    assert baks, \"No cache and no backups found â€“ cannot restore.\"\n",
        "    logger.warning(f\"Cache missing. Restoring from {baks[0].name}\")\n",
        "    Path(baks[0]).replace(EXON_CACHE_TSV)\n",
        "    logger.info(f\"âœ… Restored cache to {EXON_CACHE_TSV.name}\")\n",
        "else:\n",
        "    logger.info(f\"Current cache: rowsâ‰ˆ{current_rows}, sha256={current_sha[:16]}...\")\n",
        "    if baks:\n",
        "        bak_rows = _safe_rows(baks[0])\n",
        "        logger.info(f\"Latest backup: {baks[0].name} rowsâ‰ˆ{bak_rows}\")\n",
        "        if (current_rows >= 0) and (bak_rows > current_rows):\n",
        "            # Auto-restore if backup is larger (safer)\n",
        "            logger.warning(\"Backup is larger than current cache â†’ restoring backup.\")\n",
        "            tmp = EXON_CACHE_TSV.with_suffix(\".restore.tmp\")\n",
        "            # copy via pandas to normalize newline/encoding issues if any\n",
        "            pd.read_csv(baks[0], sep='\\t', low_memory=False).to_csv(tmp, sep='\\t', index=False)\n",
        "            Path(tmp).replace(EXON_CACHE_TSV)\n",
        "            logger.info(\"âœ… Auto-restore complete.\")\n",
        "    else:\n",
        "        logger.info(\"No backups found; keeping current cache unchanged.\")\n"
      ],
      "metadata": {
        "id": "jlaCcRixuaKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-5-header"
      },
      "source": [
        "# **Part 5: Consensus & Data Restructuring**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-50-md"
      },
      "source": [
        "## Cell 50 â€“ PhylogeneticConsensusEngine (FAST MRCA-based)\n",
        "\n",
        "This replaces pairwise genusâ€“genus distances with a **single-pass MRCA scheme**:\n",
        "\n",
        "- Pre-index **TimeTree** once for all genera weâ€™ll need (fast lookups).\n",
        "- Per group, compute weights as: `w(g) = 1 / (1 + dist(leaf_g, MRCA(group)))`, then normalize.\n",
        "- Falls back to **uniform weights** if the tree or taxa are missing.\n",
        "- Keeps a small JSON cache file for backwards compatibility (not required in FAST mode).\n",
        "\n",
        "Inputs assumed from earlier cells:\n",
        "- `DRIVE_TIMETREE_PATH` (optional; soft-fallback if missing)\n",
        "- `CACHE_DIR` (for `timetree_distance_cache.json`)\n",
        "- `logger`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 50 =====\n",
        "# PhylogeneticConsensusEngine (FAST MRCA-based) + preindex support\n",
        "\n",
        "import json\n",
        "\n",
        "try:\n",
        "    from ete3 import Tree  # optional, only used if TimeTree present\n",
        "    _HAS_ETE_TREE = True\n",
        "except Exception:\n",
        "    _HAS_ETE_TREE = False\n",
        "    Tree = None\n",
        "\n",
        "PHYLO_CACHE_JSON = CACHE_DIR / \"timetree_distance_cache.json\"\n",
        "\n",
        "class PhylogeneticConsensusEngine:\n",
        "    \"\"\"\n",
        "    MRCA-based, single-pass weighting to avoid O(k^2) pairwise calls.\n",
        "    - Build a name->node index once (only for needed genera).\n",
        "    - Weights for a group = 1/(1 + dist(leaf, MRCA(group))).\n",
        "    - Optional dist cache kept for compatibility; not required in FAST mode.\n",
        "    \"\"\"\n",
        "    def __init__(self, nwk_path: Path, cache_json: Path):\n",
        "        self.nwk_path = nwk_path\n",
        "        self.cache_json = cache_json\n",
        "        self.tree = None\n",
        "        self._node_by_name = {}   # normalized_name -> TreeNode\n",
        "        self._norm_cache = {}\n",
        "        self.dist_cache = {}\n",
        "        if cache_json.exists():\n",
        "            try:\n",
        "                raw = json.load(open(cache_json))\n",
        "                self.dist_cache = {tuple(k.split(\"|\")): v for k, v in raw.items()}\n",
        "            except Exception:\n",
        "                self.dist_cache = {}\n",
        "        if _HAS_ETE_TREE and nwk_path.exists():\n",
        "            try:\n",
        "                self.tree = Tree(str(nwk_path), format=1)\n",
        "            except Exception:\n",
        "                self.tree = None\n",
        "\n",
        "    def _norm(self, name: str) -> str:\n",
        "        if name in self._norm_cache:\n",
        "            return self._norm_cache[name]\n",
        "        n = (name or \"\").strip().replace(\" \", \"_\")\n",
        "        self._norm_cache[name] = n\n",
        "        return n\n",
        "\n",
        "    def preindex_genera(self, genera) -> None:\n",
        "        \"\"\"Build name->node dict once for all genera weâ€™ll query.\"\"\"\n",
        "        if not self.tree:\n",
        "            return\n",
        "        needed = {self._norm(g) for g in genera if isinstance(g, str) and g.strip()}\n",
        "        # quick leaf-name lookup table\n",
        "        name2node = {}\n",
        "        for node in self.tree.iter_leaves():\n",
        "            if node.name:\n",
        "                name2node[node.name] = node\n",
        "        hit, miss = 0, 0\n",
        "        for n in needed:\n",
        "            node = name2node.get(n)\n",
        "            if node is not None:\n",
        "                self._node_by_name[n] = node\n",
        "                hit += 1\n",
        "            else:\n",
        "                miss += 1\n",
        "        logger.info(f\"TimeTree preindex: {hit} genera resolved, {miss} not found.\")\n",
        "\n",
        "    def _node(self, genus: str):\n",
        "        if not self.tree or not genus:\n",
        "            return None\n",
        "        return self._node_by_name.get(self._norm(genus))\n",
        "\n",
        "    def mrca_node(self, genera):\n",
        "        \"\"\"MRCA of all resolvable genera in the group.\"\"\"\n",
        "        if not self.tree:\n",
        "            return None\n",
        "        nodes = [self._node(g) for g in genera]\n",
        "        nodes = [n for n in nodes if n is not None]\n",
        "        if len(nodes) == 0:\n",
        "            return None\n",
        "        if len(nodes) == 1:\n",
        "            return nodes[0]\n",
        "        try:\n",
        "            return self.tree.get_common_ancestor(nodes)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def weights_from_mrca(self, genera):\n",
        "        \"\"\"\n",
        "        Fast weights: for each genus g, weight = 1/(1 + dist(g, MRCA(genera))).\n",
        "        Normalized to sum to 1. Uniform if tree/labels missing.\n",
        "        \"\"\"\n",
        "        uniq = sorted({g for g in genera if isinstance(g, str) and g.strip()})\n",
        "        if not self.tree or not uniq:\n",
        "            return {g: 1.0/len(uniq) for g in uniq} if uniq else {}\n",
        "        mrca = self.mrca_node(uniq)\n",
        "        if mrca is None:\n",
        "            return {g: 1.0/len(uniq) for g in uniq}\n",
        "        weights = {}\n",
        "        for g in uniq:\n",
        "            node = self._node(g)\n",
        "            if node is None:\n",
        "                w = 1.0  # treat unknowns as close to MRCA to avoid zeroing them\n",
        "            else:\n",
        "                try:\n",
        "                    d = float(self.tree.get_distance(node, mrca))\n",
        "                except Exception:\n",
        "                    d = 1.0\n",
        "                w = 1.0 / (1.0 + d)\n",
        "            weights[g] = w\n",
        "        s = sum(weights.values())\n",
        "        return {g: (w/s if s > 0 else 0.0) for g, w in weights.items()}\n",
        "\n",
        "    def persist_cache(self):\n",
        "        \"\"\"Persist pairwise cache (legacy compatibility).\"\"\"\n",
        "        try:\n",
        "            payload = {\"|\".join(k): v for k, v in self.dist_cache.items()}\n",
        "            json.dump(payload, open(self.cache_json, \"w\"))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "phylo_engine = PhylogeneticConsensusEngine(DRIVE_TIMETREE_PATH, PHYLO_CACHE_JSON)\n",
        "logger.info(\"PhylogeneticConsensusEngine ready \"\n",
        "            f\"(tree={'OK' if phylo_engine.tree else 'fallback'})\")\n"
      ],
      "metadata": {
        "id": "tCmJqdGKDuxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 51 â€“ FAST weighted consensus (MRCA mode, chunked writes, detailed logging)\n",
        "- Uses `weights_from_mrca()` (O(k) per group) â†’ big speed-up.\n",
        "- Logs progress every `PHYLO_LOG_INTERVAL` groups with ETA.\n",
        "- Writes **incremental snapshots** every `CHUNK_SIZE` groups to avoid losing work.\n",
        "- Falls back to **uniform** weights if TimeTree isnâ€™t available.\n"
      ],
      "metadata": {
        "id": "m3kfV0nMdDng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 51 =====\n",
        "# FAST weighted consensus (MRCA-mode), preindex, chunked writes, detailed logging\n",
        "\n",
        "# --- Safety preamble (so Cell 51 can run standalone) ---\n",
        "import time\n",
        "\n",
        "if 'PHYLO_FAST_MODE' not in globals():\n",
        "    PHYLO_FAST_MODE = True\n",
        "if 'PHYLO_LOG_INTERVAL' not in globals():\n",
        "    PHYLO_LOG_INTERVAL = 500\n",
        "if 'CHUNK_SIZE' not in globals():\n",
        "    CHUNK_SIZE = 1000\n",
        "if 'MAX_GROUPS' not in globals():\n",
        "    MAX_GROUPS = -1  # process all groups by default\n",
        "\n",
        "# If phylo_engine wasn't built (Cell 50 not run), fall back to uniform weights\n",
        "if 'phylo_engine' not in globals():\n",
        "    class _DummyPhylo:\n",
        "        tree = None\n",
        "        def preindex_genera(self, genera):  # no-op\n",
        "            pass\n",
        "        def weights_from_mrca(self, genera):\n",
        "            uniq = sorted({g for g in genera if isinstance(g, str) and g.strip()})\n",
        "            return {u: 1.0/len(uniq) for u in uniq} if uniq else {}\n",
        "        def persist_cache(self):\n",
        "            pass\n",
        "    phylo_engine = _DummyPhylo()\n",
        "    logger.info(\"âš ï¸ Phylo engine not initialised; using uniform weights (no TimeTree).\")\n",
        "\n",
        "def weighted_median(values: np.ndarray, weights: np.ndarray) -> float:\n",
        "    if len(values) == 0:\n",
        "        return float(\"nan\")\n",
        "    idx = np.argsort(values)\n",
        "    v = values[idx]; w = weights[idx]\n",
        "    c = np.cumsum(w) / np.sum(w)\n",
        "    j = min(np.searchsorted(c, 0.5), len(v)-1)\n",
        "    return float(v[j])\n",
        "\n",
        "def adaptive_tolerance(depth_proxy: float) -> int:\n",
        "    \"\"\"\n",
        "    We use an approximate 'depth proxy' (see below) to assign tolerance:\n",
        "      0.0 â†’ 2 AA  â€¦  â‰¥3.0 â†’ 5 AA (linear mapping)\n",
        "    If unavailable, return 3 AA.\n",
        "    \"\"\"\n",
        "    if depth_proxy is None or (isinstance(depth_proxy, float) and np.isnan(depth_proxy)):\n",
        "        return 3\n",
        "    x = float(np.clip(depth_proxy, 0.0, 3.0))\n",
        "    return int(round(2.0 + x * (3.0/3.0)))  # 2..5\n",
        "\n",
        "consensus_rows = []\n",
        "refined_rows = []\n",
        "\n",
        "start = time.perf_counter()\n",
        "groups_done = 0\n",
        "\n",
        "if EXON_CACHE_TSV.exists():\n",
        "    base = pd.read_csv(EXON_CACHE_TSV, sep='\\t', low_memory=False)\n",
        "    need = ['accession','gene_symbol','organism','exon_num_in_chain','begin_aa','end_aa','peptide']\n",
        "    base = base[[c for c in need if c in base.columns]].copy()\n",
        "\n",
        "    # Join cluster_genus from working_df if available\n",
        "    if 'cluster_genus' in working_df.columns:\n",
        "        base = base.merge(\n",
        "            working_df[['Entry','cluster_genus']].rename(columns={'Entry':'accession'}),\n",
        "            on='accession', how='left'\n",
        "        )\n",
        "        base['cluster_genus'] = base['cluster_genus'].fillna(\n",
        "            base['organism'].astype(str).str.split().str[0]\n",
        "        )\n",
        "    else:\n",
        "        base['cluster_genus'] = base['organism'].astype(str).str.split().str[0]\n",
        "\n",
        "    # Preindex all genera for this run (fast MRCA lookups)\n",
        "    try:\n",
        "        all_genera = base['cluster_genus'].astype(str).tolist()\n",
        "        phylo_engine.preindex_genera(all_genera)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    grouped = base.groupby(['gene_symbol','exon_num_in_chain'], sort=False)\n",
        "    total_groups = len(grouped)\n",
        "    logger.info(f\"Consensus: processing {total_groups} exon groups \"\n",
        "                f\"(FAST mode={'ON' if PHYLO_FAST_MODE and getattr(phylo_engine, 'tree', None) else 'OFF'})\")\n",
        "\n",
        "    chunk_idx = 0\n",
        "    for i, ((g, e), df) in enumerate(grouped):\n",
        "        if MAX_GROUPS > 0 and i >= MAX_GROUPS:\n",
        "            break\n",
        "\n",
        "        genera = df['cluster_genus'].astype(str).tolist()\n",
        "        if getattr(phylo_engine, 'tree', None) and PHYLO_FAST_MODE:\n",
        "            weights_by_genus = phylo_engine.weights_from_mrca(genera)\n",
        "            # depth proxy: approx distance via inverse of weights (w ~ 1/(1+d) â‡’ d ~ 1/w - 1)\n",
        "            ds = [(1.0/max(1e-9, w) - 1.0) for w in weights_by_genus.values()]\n",
        "            mean_depth = float(np.mean(ds)) if ds else float('nan')\n",
        "        else:\n",
        "            uniq = sorted(set(genera))\n",
        "            weights_by_genus = {u: 1.0/len(uniq) for u in uniq} if uniq else {}\n",
        "            mean_depth = float('nan')\n",
        "\n",
        "        tol = adaptive_tolerance(mean_depth)\n",
        "        w = df['cluster_genus'].map(weights_by_genus).fillna(0.0).to_numpy()\n",
        "        if w.sum() == 0:\n",
        "            w = np.ones(len(df)) / max(1, len(df))\n",
        "\n",
        "        b = df['begin_aa'].astype(float).to_numpy()\n",
        "        epos = df['end_aa'].astype(float).to_numpy()\n",
        "\n",
        "        cons_beg = weighted_median(b, w)\n",
        "        cons_end = weighted_median(epos, w)\n",
        "        cb, ce = int(round(cons_beg)), int(round(cons_end))\n",
        "\n",
        "        consensus_rows.append({\n",
        "            'gene_symbol': g,\n",
        "            'exon_num_in_chain': e,\n",
        "            'cons_begin': cb,\n",
        "            'cons_end': ce,\n",
        "            'tolerance_aa': int(tol),\n",
        "            'depth_proxy': round(mean_depth, 3) if not np.isnan(mean_depth) else np.nan\n",
        "        })\n",
        "\n",
        "        for _, r in df.iterrows():\n",
        "            db = int(r['begin_aa']) - cb\n",
        "            de = int(r['end_aa'])   - ce\n",
        "            refined_rows.append({\n",
        "                'accession': r['accession'],\n",
        "                'gene_symbol': g,\n",
        "                'exon_num_in_chain': e,\n",
        "                'begin_aa': int(r['begin_aa']),\n",
        "                'end_aa': int(r['end_aa']),\n",
        "                'peptide': r.get('peptide',''),\n",
        "                'cluster_genus': r.get('cluster_genus',''),\n",
        "                'cons_begin': cb,\n",
        "                'cons_end': ce,\n",
        "                'delta_begin': db,\n",
        "                'delta_end': de,\n",
        "                'boundary_ok': (abs(db) <= tol) and (abs(de) <= tol)\n",
        "            })\n",
        "\n",
        "        groups_done += 1\n",
        "        if groups_done % PHYLO_LOG_INTERVAL == 0:\n",
        "            elapsed = time.perf_counter() - start\n",
        "            rate = groups_done / max(1e-9, elapsed)\n",
        "            eta = (total_groups - groups_done) / max(1e-9, rate)\n",
        "            logger.info(f\"Consensus progress: {groups_done}/{total_groups} groups \"\n",
        "                        f\"({rate:.2f} grp/s, ETA ~{eta/60:.1f} min)\")\n",
        "\n",
        "        if groups_done % CHUNK_SIZE == 0:\n",
        "            # write checkpoints to guard against interrupts\n",
        "            tmp_long = RUN_DIR / f\"consensus_long_{RUN_ID}.chunk{chunk_idx}.tsv\"\n",
        "            tmp_tbl  = RUN_DIR / f\"consensus_tbl_{RUN_ID}.chunk{chunk_idx}.tsv\"\n",
        "            pd.DataFrame(refined_rows).to_csv(tmp_long, sep='\\t', index=False)\n",
        "            pd.DataFrame(consensus_rows).to_csv(tmp_tbl,  sep='\\t', index=False)\n",
        "            logger.info(f\"Checkpoint written: chunk {chunk_idx} \"\n",
        "                        f\"(rows so far: {len(refined_rows)})\")\n",
        "            chunk_idx += 1\n",
        "\n",
        "# Finalize outputs atomically\n",
        "consensus_tbl  = pd.DataFrame(consensus_rows)\n",
        "consensus_long = pd.DataFrame(refined_rows)\n",
        "if not consensus_long.empty:\n",
        "    tmp1 = CONSENSUS_LONG_SNAPSHOT.with_suffix(\".tmp.tsv\")\n",
        "    tmp2 = CONSENSUS_LONG_SNAPSHOT.parent / f\"consensus_tbl_{RUN_ID}.tmp.tsv\"\n",
        "    consensus_long.to_csv(tmp1, sep='\\t', index=False)\n",
        "    consensus_tbl.to_csv(tmp2,  sep='\\t', index=False)\n",
        "    Path(tmp1).replace(CONSENSUS_LONG_SNAPSHOT)\n",
        "    Path(tmp2).replace(CONSENSUS_LONG_SNAPSHOT.parent / f\"consensus_tbl_{RUN_ID}.tsv\")\n",
        "    # persist legacy cache (no-op in FAST mode)\n",
        "    try:\n",
        "        phylo_engine.persist_cache()\n",
        "    except Exception:\n",
        "        pass\n",
        "    elapsed = time.perf_counter() - start\n",
        "    logger.info(f\"Consensus refined: groups={groups_done}, rows={len(consensus_long)}, \"\n",
        "                f\"time={elapsed/60:.1f} min (FAST mode {'ON' if getattr(phylo_engine,'tree',None) else 'OFF'})\")\n",
        "else:\n",
        "    logger.info(\"No data for consensus refinement.\")\n"
      ],
      "metadata": {
        "id": "PScJgQNPD-UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-52-md"
      },
      "source": [
        "## Cell 52 â€“ MRCA Level & Reliability Scoring (robust lineage merge)\n",
        "\n",
        "**Purpose.**  \n",
        "Determine the deepest taxonomic level where exon boundaries are consistent (an MRCA-like\n",
        "coherence check), and compute a 0â€“100 reliability score per exon group using\n",
        "phylogenetic diversity, boundary consistency, and sequence quality.\n",
        "\n",
        "**Inputs.**  \n",
        "- `consensus_long`, `consensus_tbl` from **Cell 51** (contains `cons_begin`, `cons_end`, `tolerance_aa`, `depth_proxy`, and `boundary_ok`).  \n",
        "- Lineage features from `working_df` (**Cell 24**): `Clas_id`, `Order`, `Family`, `cluster_genus`.  \n",
        "- Optional quality from `EXON_CACHE_TSV`: per `(accession, exon_num_in_chain)` `quality_score`.  \n",
        "\n",
        "**Method.**  \n",
        "1. Merge `consensus_long` + `consensus_tbl`, then join lineage fields from `working_df`\n",
        "   and `quality_score` from the exon cache. If `cluster_genus` is missing, it is created\n",
        "   as `'NA'`.  \n",
        "2. For each (`gene_symbol`, `exon_num_in_chain`) group:  \n",
        "   - **MRCA level**: test boundary spread (SD of `end_aa - begin_aa`) within tolerance\n",
        "     across successive ranks present in the data (`Class`â†’`Order`â†’`Family`â†’`Genus`).\n",
        "     The deepest rank where all bins are within `tolerance_aa` is reported; if none,\n",
        "     return `\"None\"`.  \n",
        "   - **Reliability score (0â€“100)**:\n",
        "     \\[\n",
        "     100 \\times \\big(0.45 \\cdot \\text{phylo\\_div} + 0.35 \\cdot \\text{ok\\_frac} + 0.20 \\cdot \\text{seq\\_q}\\big) \\times \\text{tol\\_factor}\n",
        "     \\]\n",
        "     where:\n",
        "     - `phylo_div = min(1, 0.2 * #genera + 0.1 * #orders)`  \n",
        "     - `ok_frac = mean(boundary_ok)`  \n",
        "     - `seq_q = mean(quality_score/100)` if available else `0.5`  \n",
        "     - `tol_factor` slightly downweights wide tolerances: `1 - clip((tol-2)/3, 0, 1)*0.2`  \n",
        "   - `depth_proxy` from **Cell 51** is carried for context but not directly in the score\n",
        "     (tolerance already reflects phylogenetic spread).\n",
        "\n",
        "**Outputs.**  \n",
        "- `mrca_df` with:  \n",
        "  `['gene_symbol','exon_num_in_chain','MRCA_level','reliability_score','tolerance_aa','depth_proxy']`  \n",
        "- Logged summary: number of rows and genes processed.  \n",
        "- **Note**: Not written to disk by default. If needed, persist with:  \n",
        "  `mrca_df.to_csv(OUTPUTS_PATH / f\"mrca_reliability_{RUN_ID}.tsv\", sep=\"\\t\", index=False)`\n",
        "\n",
        "**Edge cases & safeguards.**  \n",
        "- If a lineage column is absent, that level is skipped automatically.  \n",
        "- If `consensus_long`/`consensus_tbl` are missing, the cell exits gracefully.  \n",
        "- Grouping keys with `NaN` are coerced to string bins (e.g., `'NA'`) to avoid errors.\n",
        "\n",
        "**Downstream.**  \n",
        "`mrca_df` can be merged in **Cell 54** (wide architecture) for annotation, reporting, and\n",
        "filtering based on reliability thresholds.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 52 =====\n",
        "# MRCA level + reliability scoring (robust lineage merge)\n",
        "\n",
        "def mrca_level_for_group(df: pd.DataFrame, tol: int) -> str:\n",
        "    # Only use levels that are present\n",
        "    levels = []\n",
        "    if 'Clas_id' in df.columns: levels.append(('Clas_id','Class'))\n",
        "    if 'Order'   in df.columns: levels.append(('Order','Order'))\n",
        "    if 'Family'  in df.columns: levels.append(('Family','Family'))\n",
        "    if 'cluster_genus' in df.columns: levels.append(('cluster_genus','Genus'))\n",
        "    for col, label in levels:\n",
        "        good = True\n",
        "        # groupby needs non-null keys\n",
        "        for _, sub in df.groupby(df[col].astype(str).fillna('NA')):\n",
        "            dv = (sub['end_aa'].astype(float) - sub['begin_aa'].astype(float)).var()\n",
        "            spread = float(np.sqrt(dv)) if pd.notna(dv) and dv > 0 else 0.0\n",
        "            if spread > tol:\n",
        "                good = False\n",
        "                break\n",
        "        if good:\n",
        "            return label\n",
        "    return \"None\"\n",
        "\n",
        "def reliability_score_for_group(df: pd.DataFrame, tol: int) -> float:\n",
        "    n_genera = df['cluster_genus'].nunique() if 'cluster_genus' in df.columns else 0\n",
        "    n_orders = df['Order'].astype(str).nunique() if 'Order' in df.columns else 0\n",
        "    phylo_div = min(1.0, 0.2 * n_genera + 0.1 * n_orders)\n",
        "    ok_frac = float((df['boundary_ok'] == True).mean()) if 'boundary_ok' in df.columns else 0.0\n",
        "    if 'quality_score' in df.columns:\n",
        "        q = df['quality_score'].fillna(0).astype(float)\n",
        "        seq_q = float(np.clip(q / 100.0, 0, 1).mean())\n",
        "    else:\n",
        "        seq_q = 0.5\n",
        "    # Slightly relax score if tolerance is wide\n",
        "    tol_factor = 1.0 - np.clip((tol - 2) / 3.0, 0, 1) * 0.2\n",
        "    score = 100.0 * (0.45 * phylo_div + 0.35 * ok_frac + 0.20 * seq_q) * tol_factor\n",
        "    return float(np.clip(score, 0, 100))\n",
        "\n",
        "mrca_rows = []\n",
        "\n",
        "if 'consensus_long' in globals() and not consensus_long.empty and 'consensus_tbl' in globals() and not consensus_tbl.empty:\n",
        "    # ---- Build lineage LUT from working_df ----\n",
        "    lineage_cols = []\n",
        "    if 'Clas_id' in working_df.columns: lineage_cols.append('Clas_id')\n",
        "    if 'Order'   in working_df.columns: lineage_cols.append('Order')\n",
        "    if 'Family'  in working_df.columns: lineage_cols.append('Family')\n",
        "    if 'cluster_genus' in working_df.columns: lineage_cols.append('cluster_genus')\n",
        "    lineage_lut = (working_df[['Entry'] + lineage_cols].drop_duplicates()\n",
        "                   if lineage_cols else pd.DataFrame(columns=['Entry']))\n",
        "\n",
        "    # ---- Quality score LUT from exon cache (per accession/exon) ----\n",
        "    q_lut = pd.DataFrame()\n",
        "    if EXON_CACHE_TSV.exists():\n",
        "        try:\n",
        "            q_lut = pd.read_csv(\n",
        "                EXON_CACHE_TSV, sep='\\t', low_memory=False,\n",
        "                usecols=lambda c: c in ('accession','exon_num_in_chain','quality_score')\n",
        "            ).drop_duplicates(['accession','exon_num_in_chain'])\n",
        "        except Exception:\n",
        "            q_lut = pd.DataFrame()\n",
        "\n",
        "    # ---- Merge consensus_long + consensus_tbl ----\n",
        "    merged = consensus_long.merge(consensus_tbl, on=['gene_symbol','exon_num_in_chain'], how='left')\n",
        "\n",
        "    # ---- Join lineage & quality info ----\n",
        "    if not lineage_lut.empty:\n",
        "        merged = merged.merge(\n",
        "            lineage_lut.rename(columns={'Entry':'accession'}),\n",
        "            on='accession', how='left'\n",
        "        )\n",
        "    if not q_lut.empty and 'quality_score' not in merged.columns:\n",
        "        merged = merged.merge(q_lut, on=['accession','exon_num_in_chain'], how='left')\n",
        "\n",
        "    # Ensure cluster_genus exists for scoring\n",
        "    if 'cluster_genus' not in merged.columns:\n",
        "        merged['cluster_genus'] = 'NA'\n",
        "\n",
        "    # Compute MRCA level + reliability per exon group\n",
        "    for (g, e), sub in merged.groupby(['gene_symbol','exon_num_in_chain']):\n",
        "        tol = int(sub['tolerance_aa'].iloc[0]) if 'tolerance_aa' in sub.columns else 3\n",
        "        mrca = mrca_level_for_group(sub, tol)\n",
        "        rel  = reliability_score_for_group(sub, tol)\n",
        "        depth = sub['depth_proxy'].iloc[0] if 'depth_proxy' in sub.columns else np.nan\n",
        "        mrca_rows.append({\n",
        "            'gene_symbol': g,\n",
        "            'exon_num_in_chain': e,\n",
        "            'MRCA_level': mrca,\n",
        "            'reliability_score': round(rel, 1),\n",
        "            'tolerance_aa': tol,\n",
        "            'depth_proxy': depth\n",
        "        })\n",
        "\n",
        "    mrca_df = pd.DataFrame(mrca_rows)\n",
        "    logger.info(f\"MRCA/reliability computed rows: {len(mrca_df)} \"\n",
        "                f\"(genes={mrca_df['gene_symbol'].nunique() if not mrca_df.empty else 0})\")\n",
        "else:\n",
        "    mrca_df = pd.DataFrame()\n",
        "    logger.info(\"No consensus tables available; run Cell 51 first.\")\n"
      ],
      "metadata": {
        "id": "DEfiYpN7F664"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-54-md"
      },
      "source": [
        "## Cell 54 â€“ Final wide architecture (peptides + coords + status)\n",
        "\n",
        "Builds a wide table per accession including exon peptides, AA coordinates,\n",
        "boundary flags, and consensus coordinates."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 54 - Enhanced Wide Architecture with Memory Management =====\n",
        "# Memory-optimized wide DataFrame creation with monitoring and chunked processing\n",
        "\n",
        "import gc\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "\n",
        "def monitor_memory_usage(operation_name: str, threshold_mb: float = 500.0) -> Dict[str, float]:\n",
        "    \"\"\"Monitor memory usage and warn if above threshold.\"\"\"\n",
        "    memory_info = get_memory_usage()\n",
        "\n",
        "    if memory_info['rss_mb'] > threshold_mb:\n",
        "        logger.warning(f\"{operation_name}: High memory usage detected - {memory_info['rss_mb']:.1f}MB ({memory_info['percent']:.1f}%)\")\n",
        "    else:\n",
        "        logger.info(f\"{operation_name}: Memory usage - {memory_info['rss_mb']:.1f}MB ({memory_info['percent']:.1f}%)\")\n",
        "\n",
        "    return memory_info\n",
        "\n",
        "def estimate_dataframe_memory(num_rows: int, num_cols: int, avg_string_length: int = 50) -> float:\n",
        "    \"\"\"Estimate memory usage for a DataFrame in MB.\"\"\"\n",
        "    # Rough estimation: each cell ~8 bytes + string overhead\n",
        "    estimated_bytes = num_rows * num_cols * (8 + avg_string_length)\n",
        "    return estimated_bytes / 1024 / 1024\n",
        "\n",
        "def create_wide_architecture_chunked(consensus_long: pd.DataFrame,\n",
        "                                   consensus_tbl: pd.DataFrame,\n",
        "                                   mrca_df: pd.DataFrame,\n",
        "                                   chunk_size: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create wide architecture DataFrame using chunked processing to manage memory.\n",
        "\n",
        "    Args:\n",
        "        consensus_long: Long-format consensus data\n",
        "        consensus_tbl: Consensus table for annotation\n",
        "        mrca_df: MRCA data for merging\n",
        "        chunk_size: Number of accessions to process per chunk\n",
        "\n",
        "    Returns:\n",
        "        Wide-format DataFrame or empty DataFrame if processing fails\n",
        "    \"\"\"\n",
        "\n",
        "    if consensus_long.empty:\n",
        "        logger.warning(\"Empty consensus_long DataFrame - returning empty result\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Initial memory check\n",
        "    initial_memory = monitor_memory_usage(\"Before wide DataFrame creation\")\n",
        "\n",
        "    # Estimate memory requirements\n",
        "    unique_accessions = consensus_long['accession'].nunique()\n",
        "    unique_exons = consensus_long['exon_num_in_chain'].nunique()\n",
        "    estimated_cols = unique_exons * 3 + 10  # 3 cols per exon + metadata\n",
        "\n",
        "    estimated_memory = estimate_dataframe_memory(unique_accessions, estimated_cols)\n",
        "    logger.info(f\"Estimated memory for wide DataFrame: {estimated_memory:.1f}MB\")\n",
        "\n",
        "    if estimated_memory > 1000:  # >1GB\n",
        "        logger.warning(\"Large DataFrame expected - consider reducing data or increasing chunk size\")\n",
        "\n",
        "    # Prepare annotation data efficiently\n",
        "    try:\n",
        "        annot = consensus_tbl.merge(mrca_df, on=['gene_symbol', 'exon_num_in_chain'], how='left')\n",
        "        logger.info(f\"Created annotation table with {len(annot)} rows\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to create annotation table: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Group by accession and gene_symbol for chunked processing\n",
        "    grouped = consensus_long.groupby(['accession', 'gene_symbol'])\n",
        "    total_groups = len(grouped)\n",
        "\n",
        "    logger.info(f\"Processing {total_groups} accession-gene combinations in chunks of {chunk_size}\")\n",
        "\n",
        "    # Process in chunks to manage memory\n",
        "    all_rows = []\n",
        "    processed_count = 0\n",
        "\n",
        "    # Get all group keys for chunking\n",
        "    group_keys = list(grouped.groups.keys())\n",
        "\n",
        "    for chunk_start in range(0, len(group_keys), chunk_size):\n",
        "        chunk_end = min(chunk_start + chunk_size, len(group_keys))\n",
        "        chunk_keys = group_keys[chunk_start:chunk_end]\n",
        "\n",
        "        chunk_rows = []\n",
        "        chunk_memory_start = get_memory_usage()\n",
        "\n",
        "        logger.info(f\"Processing chunk {chunk_start//chunk_size + 1}/{(len(group_keys)-1)//chunk_size + 1} \"\n",
        "                   f\"({len(chunk_keys)} groups)\")\n",
        "\n",
        "        for (acc, g) in chunk_keys:\n",
        "            try:\n",
        "                sub = grouped.get_group((acc, g))\n",
        "                row = {'accession': acc, 'gene_symbol': g}\n",
        "\n",
        "                # Process each exon for this accession-gene combination\n",
        "                for _, r in sub.iterrows():\n",
        "                    try:\n",
        "                        k = int(r['exon_num_in_chain'])\n",
        "\n",
        "                        # Store peptide, coordinates, and status\n",
        "                        row[f'exon_{k}_peptide'] = r.get('peptide', '')\n",
        "                        row[f'exon_{k}_coords'] = f\"{int(r['begin_aa'])}-{int(r['end_aa'])}\"\n",
        "                        row[f'exon_{k}_status'] = 'OK' if r.get('boundary_ok', False) else 'FLAG'\n",
        "\n",
        "                    except (ValueError, KeyError) as e:\n",
        "                        logger.debug(f\"Error processing exon for {acc}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                chunk_rows.append(row)\n",
        "                processed_count += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error processing group ({acc}, {g}): {e}\")\n",
        "                continue\n",
        "\n",
        "        # Convert chunk to DataFrame and add to results\n",
        "        if chunk_rows:\n",
        "            chunk_df = pd.DataFrame(chunk_rows)\n",
        "            all_rows.extend(chunk_rows)\n",
        "\n",
        "            # Memory monitoring for chunk\n",
        "            chunk_memory_end = get_memory_usage()\n",
        "            chunk_memory_used = chunk_memory_end['rss_mb'] - chunk_memory_start['rss_mb']\n",
        "\n",
        "            if chunk_memory_used > 50:  # >50MB per chunk\n",
        "                logger.warning(f\"Chunk used {chunk_memory_used:.1f}MB - consider smaller chunk size\")\n",
        "\n",
        "            # Periodic garbage collection\n",
        "            if (chunk_start // chunk_size + 1) % 5 == 0:  # Every 5 chunks\n",
        "                gc.collect()\n",
        "                logger.debug(\"Performed garbage collection\")\n",
        "\n",
        "        # Progress reporting\n",
        "        progress = (chunk_end / len(group_keys)) * 100\n",
        "        logger.info(f\"Progress: {progress:.1f}% ({processed_count}/{total_groups} groups processed)\")\n",
        "\n",
        "    # Create final DataFrame\n",
        "    if not all_rows:\n",
        "        logger.warning(\"No valid rows created - returning empty DataFrame\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Creating final wide DataFrame from {len(all_rows)} rows\")\n",
        "        wide_df = pd.DataFrame(all_rows)\n",
        "\n",
        "        # Memory check after DataFrame creation\n",
        "        post_creation_memory = monitor_memory_usage(\"After wide DataFrame creation\")\n",
        "        memory_increase = post_creation_memory['rss_mb'] - initial_memory['rss_mb']\n",
        "\n",
        "        logger.info(f\"Wide DataFrame created: {wide_df.shape} (Memory increase: {memory_increase:.1f}MB)\")\n",
        "\n",
        "    except MemoryError:\n",
        "        logger.error(\"MemoryError creating wide DataFrame - try smaller chunk size or filtering data\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error creating wide DataFrame: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Add consensus coordinates efficiently\n",
        "    try:\n",
        "        logger.info(\"Adding consensus coordinates...\")\n",
        "        cons_coords = {}\n",
        "\n",
        "        for _, r in annot.iterrows():\n",
        "            try:\n",
        "                k = int(r['exon_num_in_chain'])\n",
        "                gsym = r['gene_symbol']\n",
        "                cons_coords[(gsym, k)] = f\"{int(r['cons_begin'])}-{int(r['cons_end'])}\"\n",
        "            except (ValueError, KeyError):\n",
        "                continue\n",
        "\n",
        "        # Add consensus coordinate columns efficiently\n",
        "        for (gsym, k), coord in cons_coords.items():\n",
        "            col = f'cons_exon_{k}_coords'\n",
        "            if col not in wide_df.columns:\n",
        "                wide_df[col] = ''\n",
        "\n",
        "            # Use boolean indexing for efficiency\n",
        "            mask = wide_df['gene_symbol'] == gsym\n",
        "            wide_df.loc[mask, col] = coord\n",
        "\n",
        "        logger.info(f\"Added consensus coordinates for {len(cons_coords)} exon-gene combinations\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error adding consensus coordinates: {e}\")\n",
        "        # Continue without consensus coordinates rather than failing completely\n",
        "\n",
        "    # Final memory and performance summary\n",
        "    final_memory = monitor_memory_usage(\"Final wide DataFrame complete\")\n",
        "    total_memory_increase = final_memory['rss_mb'] - initial_memory['rss_mb']\n",
        "\n",
        "    logger.info(f\"Wide DataFrame processing complete:\")\n",
        "    logger.info(f\"  Final shape: {wide_df.shape}\")\n",
        "    logger.info(f\"  Total memory increase: {total_memory_increase:.1f}MB\")\n",
        "    logger.info(f\"  Memory efficiency: {wide_df.memory_usage(deep=True).sum() / 1024 / 1024:.1f}MB actual usage\")\n",
        "\n",
        "    # Memory optimization suggestions\n",
        "    if total_memory_increase > 200:\n",
        "        logger.info(\"ðŸ’¡ Memory optimization suggestions:\")\n",
        "        logger.info(\"  - Consider filtering to fewer species/genes\")\n",
        "        logger.info(\"  - Reduce chunk_size parameter\")\n",
        "        logger.info(\"  - Use dtype optimization for string columns\")\n",
        "\n",
        "    return wide_df\n",
        "\n",
        "# ===== Main Cell 54 Execution =====\n",
        "\n",
        "logger.info(\"ðŸ”„ Creating final wide architecture...\")\n",
        "\n",
        "wide_df = pd.DataFrame()\n",
        "\n",
        "if not consensus_long.empty:\n",
        "    try:\n",
        "        # Your existing wide DataFrame creation logic here\n",
        "        # (Keep all the existing logic for creating the wide_df)\n",
        "\n",
        "        # Build annotation table\n",
        "        annot = consensus_tbl.merge(mrca_df, on=['gene_symbol','exon_num_in_chain'], how='left')\n",
        "\n",
        "        # Build wide DataFrame rows\n",
        "        rows = []\n",
        "        for (acc, g), sub in consensus_long.groupby(['accession','gene_symbol']):\n",
        "            row = {'accession': acc, 'gene_symbol': g}\n",
        "            for _, r in sub.iterrows():\n",
        "                k = int(r['exon_num_in_chain'])\n",
        "                row[f'exon_{k}_peptide'] = r.get('peptide','')\n",
        "                row[f'exon_{k}_coords']  = f\"{int(r['begin_aa'])}-{int(r['end_aa'])}\"\n",
        "                row[f'exon_{k}_status']  = 'OK' if r.get('boundary_ok', False) else 'FLAG'\n",
        "            rows.append(row)\n",
        "\n",
        "        wide_df = pd.DataFrame(rows)\n",
        "\n",
        "        # Add consensus coordinates\n",
        "        cons_coords = {}\n",
        "        for _, r in annot.iterrows():\n",
        "            k = int(r['exon_num_in_chain'])\n",
        "            gsym = r['gene_symbol']\n",
        "            cons_coords[(gsym, k)] = f\"{int(r['cons_begin'])}-{int(r['cons_end'])}\"\n",
        "\n",
        "        for (gsym, k), coord in cons_coords.items():\n",
        "            col = f'cons_exon_{k}_coords'\n",
        "            if col not in wide_df.columns:\n",
        "                wide_df[col] = ''\n",
        "            wide_df.loc[wide_df['gene_symbol']==gsym, col] = coord\n",
        "\n",
        "        # ===== NEW: UPDATED SAVE LOGIC USING CANONICAL PATHS =====\n",
        "        if not wide_df.empty:\n",
        "            # Save to the canonical output path (REPLACES OLD: WIDE_ARCH_SNAPSHOT)\n",
        "            wide_df.to_csv(EXON_WIDE_TSV, sep='\\t', index=False)\n",
        "            logger.info(f\"âœ… Final wide architecture rows: {len(wide_df)} (saved to {EXON_WIDE_TSV})\")\n",
        "\n",
        "            # Also create a copy in the run directory for provenance\n",
        "            run_wide_path = RUN_DIR / f\"wide_architecture_{RUN_ID}.tsv\"\n",
        "            wide_df.to_csv(run_wide_path, sep='\\t', index=False)\n",
        "            logger.info(f\"ðŸ“ Run copy saved to: {run_wide_path}\")\n",
        "\n",
        "            # Optional: Save summary statistics\n",
        "            try:\n",
        "                stats = {\n",
        "                    'total_rows': len(wide_df),\n",
        "                    'unique_accessions': wide_df['accession'].nunique(),\n",
        "                    'unique_genes': wide_df['gene_symbol'].nunique(),\n",
        "                    'total_columns': len(wide_df.columns),\n",
        "                    'exon_columns': len([col for col in wide_df.columns if 'exon_' in col and '_peptide' in col])\n",
        "                }\n",
        "\n",
        "                stats_path = RUN_DIR / f\"wide_architecture_stats_{RUN_ID}.json\"\n",
        "                import json\n",
        "                with open(stats_path, 'w') as f:\n",
        "                    json.dump(stats, f, indent=2)\n",
        "                logger.info(f\"ðŸ“Š Architecture statistics saved: {stats_path}\")\n",
        "\n",
        "            except Exception as stats_error:\n",
        "                logger.warning(f\"Could not save architecture statistics: {stats_error}\")\n",
        "\n",
        "        else:\n",
        "            logger.warning(\"âŒ Wide DataFrame is empty - no output generated\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"âŒ Error creating wide architecture: {e}\")\n",
        "        wide_df = pd.DataFrame()\n",
        "\n",
        "else:\n",
        "    logger.warning(\"âš ï¸ No consensus_long data available; skipping final wide output.\")\n",
        "\n",
        "# Final memory cleanup if using memory management\n",
        "if 'gc' in globals():\n",
        "    gc.collect()\n",
        "\n",
        "logger.info(\"âœ… Cell 54 wide architecture processing complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8ac1zPcTvRu",
        "outputId": "bc070f83-43a9-4bb7-9275-3c3d1e04fd90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-08-22 05:44:55,758 [INFO] - ðŸ”„ Creating final wide architecture with memory management...\n",
            "2025-08-22 05:44:56,097 [WARNING] - Before wide DataFrame creation: High memory usage detected - 1716.6MB (13.2%)\n",
            "2025-08-22 05:44:56,302 [INFO] - Estimated memory for wide DataFrame: 205.6MB\n",
            "2025-08-22 05:44:56,328 [INFO] - Created annotation table with 1150 rows\n",
            "2025-08-22 05:44:58,730 [INFO] - Processing 14029 accession-gene combinations in chunks of 500\n",
            "2025-08-22 05:44:58,741 [INFO] - Processing chunk 1/29 (500 groups)\n",
            "2025-08-22 05:45:00,376 [INFO] - Progress: 3.6% (500/14029 groups processed)\n",
            "2025-08-22 05:45:00,379 [INFO] - Processing chunk 2/29 (500 groups)\n",
            "2025-08-22 05:45:01,408 [INFO] - Progress: 7.1% (1000/14029 groups processed)\n",
            "2025-08-22 05:45:01,410 [INFO] - Processing chunk 3/29 (500 groups)\n",
            "2025-08-22 05:45:02,381 [INFO] - Progress: 10.7% (1500/14029 groups processed)\n",
            "2025-08-22 05:45:02,384 [INFO] - Processing chunk 4/29 (500 groups)\n",
            "2025-08-22 05:45:03,352 [INFO] - Progress: 14.3% (2000/14029 groups processed)\n",
            "2025-08-22 05:45:03,354 [INFO] - Processing chunk 5/29 (500 groups)\n",
            "2025-08-22 05:45:05,270 [INFO] - Progress: 17.8% (2500/14029 groups processed)\n",
            "2025-08-22 05:45:05,275 [INFO] - Processing chunk 6/29 (500 groups)\n",
            "2025-08-22 05:45:06,142 [INFO] - Progress: 21.4% (3000/14029 groups processed)\n",
            "2025-08-22 05:45:06,145 [INFO] - Processing chunk 7/29 (500 groups)\n",
            "2025-08-22 05:45:06,943 [INFO] - Progress: 24.9% (3500/14029 groups processed)\n",
            "2025-08-22 05:45:06,945 [INFO] - Processing chunk 8/29 (500 groups)\n",
            "2025-08-22 05:45:07,752 [INFO] - Progress: 28.5% (4000/14029 groups processed)\n",
            "2025-08-22 05:45:07,754 [INFO] - Processing chunk 9/29 (500 groups)\n",
            "2025-08-22 05:45:08,517 [INFO] - Progress: 32.1% (4500/14029 groups processed)\n",
            "2025-08-22 05:45:08,519 [INFO] - Processing chunk 10/29 (500 groups)\n",
            "2025-08-22 05:45:10,188 [INFO] - Progress: 35.6% (5000/14029 groups processed)\n",
            "2025-08-22 05:45:10,190 [INFO] - Processing chunk 11/29 (500 groups)\n",
            "2025-08-22 05:45:11,401 [INFO] - Progress: 39.2% (5500/14029 groups processed)\n",
            "2025-08-22 05:45:11,403 [INFO] - Processing chunk 12/29 (500 groups)\n",
            "2025-08-22 05:45:12,793 [INFO] - Progress: 42.8% (6000/14029 groups processed)\n",
            "2025-08-22 05:45:12,796 [INFO] - Processing chunk 13/29 (500 groups)\n",
            "2025-08-22 05:45:13,974 [INFO] - Progress: 46.3% (6500/14029 groups processed)\n",
            "2025-08-22 05:45:13,976 [INFO] - Processing chunk 14/29 (500 groups)\n",
            "2025-08-22 05:45:14,801 [INFO] - Progress: 49.9% (7000/14029 groups processed)\n",
            "2025-08-22 05:45:14,803 [INFO] - Processing chunk 15/29 (500 groups)\n",
            "2025-08-22 05:45:15,944 [INFO] - Progress: 53.5% (7500/14029 groups processed)\n",
            "2025-08-22 05:45:15,945 [INFO] - Processing chunk 16/29 (500 groups)\n",
            "2025-08-22 05:45:16,746 [INFO] - Progress: 57.0% (8000/14029 groups processed)\n",
            "2025-08-22 05:45:16,748 [INFO] - Processing chunk 17/29 (500 groups)\n",
            "2025-08-22 05:45:17,565 [INFO] - Progress: 60.6% (8500/14029 groups processed)\n",
            "2025-08-22 05:45:17,567 [INFO] - Processing chunk 18/29 (500 groups)\n",
            "2025-08-22 05:45:18,273 [INFO] - Progress: 64.2% (9000/14029 groups processed)\n",
            "2025-08-22 05:45:18,275 [INFO] - Processing chunk 19/29 (500 groups)\n",
            "2025-08-22 05:45:19,129 [INFO] - Progress: 67.7% (9500/14029 groups processed)\n",
            "2025-08-22 05:45:19,131 [INFO] - Processing chunk 20/29 (500 groups)\n",
            "2025-08-22 05:45:20,733 [INFO] - Progress: 71.3% (10000/14029 groups processed)\n",
            "2025-08-22 05:45:20,746 [INFO] - Processing chunk 21/29 (500 groups)\n",
            "2025-08-22 05:45:22,230 [INFO] - Progress: 74.8% (10500/14029 groups processed)\n",
            "2025-08-22 05:45:22,232 [INFO] - Processing chunk 22/29 (500 groups)\n",
            "2025-08-22 05:45:23,043 [INFO] - Progress: 78.4% (11000/14029 groups processed)\n",
            "2025-08-22 05:45:23,045 [INFO] - Processing chunk 23/29 (500 groups)\n",
            "2025-08-22 05:45:24,048 [INFO] - Progress: 82.0% (11500/14029 groups processed)\n",
            "2025-08-22 05:45:24,052 [INFO] - Processing chunk 24/29 (500 groups)\n",
            "2025-08-22 05:45:26,322 [INFO] - Progress: 85.5% (12000/14029 groups processed)\n",
            "2025-08-22 05:45:26,327 [INFO] - Processing chunk 25/29 (500 groups)\n",
            "2025-08-22 05:45:28,279 [INFO] - Progress: 89.1% (12500/14029 groups processed)\n",
            "2025-08-22 05:45:28,281 [INFO] - Processing chunk 26/29 (500 groups)\n",
            "2025-08-22 05:45:29,395 [INFO] - Progress: 92.7% (13000/14029 groups processed)\n",
            "2025-08-22 05:45:29,398 [INFO] - Processing chunk 27/29 (500 groups)\n",
            "2025-08-22 05:45:30,388 [INFO] - Progress: 96.2% (13500/14029 groups processed)\n",
            "2025-08-22 05:45:30,390 [INFO] - Processing chunk 28/29 (500 groups)\n",
            "2025-08-22 05:45:31,866 [INFO] - Progress: 99.8% (14000/14029 groups processed)\n",
            "2025-08-22 05:45:31,869 [INFO] - Processing chunk 29/29 (29 groups)\n",
            "2025-08-22 05:45:31,929 [INFO] - Progress: 100.0% (14029/14029 groups processed)\n",
            "2025-08-22 05:45:31,930 [INFO] - Creating final wide DataFrame from 14029 rows\n",
            "2025-08-22 05:45:32,361 [WARNING] - After wide DataFrame creation: High memory usage detected - 1750.9MB (13.5%)\n",
            "2025-08-22 05:45:32,362 [INFO] - Wide DataFrame created: (14029, 257) (Memory increase: 34.3MB)\n",
            "2025-08-22 05:45:32,365 [INFO] - Adding consensus coordinates...\n",
            "2025-08-22 05:45:34,601 [INFO] - Added consensus coordinates for 1150 exon-gene combinations\n",
            "2025-08-22 05:45:34,603 [WARNING] - Final wide DataFrame complete: High memory usage detected - 1751.2MB (13.5%)\n",
            "2025-08-22 05:45:34,605 [INFO] - Wide DataFrame processing complete:\n",
            "2025-08-22 05:45:34,608 [INFO] -   Final shape: (14029, 342)\n",
            "2025-08-22 05:45:34,610 [INFO] -   Total memory increase: 34.6MB\n",
            "2025-08-22 05:45:35,921 [INFO] -   Memory efficiency: 194.3MB actual usage\n",
            "2025-08-22 05:45:37,870 [INFO] - âœ… Wide architecture saved: 14029 rows\n",
            "2025-08-22 05:45:37,876 [INFO] -    File: /content/drive/MyDrive/CollagenExonMapper/run_20250822_050245/outputs/exon_wide_run_20250822_050245.tsv\n",
            "2025-08-22 05:45:37,880 [INFO] -    Save operation memory: 0.0MB\n",
            "2025-08-22 05:45:39,512 [INFO] -    Data density: 0        85\n",
            "1        85\n",
            "2        85\n",
            "3        85\n",
            "4        85\n",
            "         ..\n",
            "14024    85\n",
            "14025    85\n",
            "14026    85\n",
            "14027    85\n",
            "14028    85\n",
            "Length: 14029, dtype: int64/85 exon columns have data\n",
            "2025-08-22 05:45:40,634 [WARNING] - Cell 54 complete: High memory usage detected - 1747.5MB (13.5%)\n",
            "2025-08-22 05:45:40,640 [INFO] - âœ… Cell 54 wide architecture processing complete with memory management\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-54-code"
      },
      "outputs": [],
      "source": [
        "# # ===== Cell 54 =====\n",
        "# # Final wide architecture with coordinates & flags\n",
        "# wide_df = pd.DataFrame()\n",
        "# if not consensus_long.empty:\n",
        "#     annot = consensus_tbl.merge(mrca_df, on=['gene_symbol','exon_num_in_chain'], how='left')\n",
        "#     rows = []\n",
        "#     for (acc, g), sub in consensus_long.groupby(['accession','gene_symbol']):\n",
        "#         row = {'accession': acc, 'gene_symbol': g}\n",
        "#         for _, r in sub.iterrows():\n",
        "#             k = int(r['exon_num_in_chain'])\n",
        "#             row[f'exon_{k}_peptide'] = r.get('peptide','')\n",
        "#             row[f'exon_{k}_coords']  = f\"{int(r['begin_aa'])}-{int(r['end_aa'])}\"\n",
        "#             row[f'exon_{k}_status']  = 'OK' if r.get('boundary_ok', False) else 'FLAG'\n",
        "#         rows.append(row)\n",
        "#     wide_df = pd.DataFrame(rows)\n",
        "#     cons_coords = {}\n",
        "#     for _, r in annot.iterrows():\n",
        "#         k = int(r['exon_num_in_chain']); gsym = r['gene_symbol']\n",
        "#         cons_coords[(gsym, k)] = f\"{int(r['cons_begin'])}-{int(r['cons_end'])}\"\n",
        "#     for (gsym, k), coord in cons_coords.items():\n",
        "#         col = f'cons_exon_{k}_coords';\n",
        "#         if col not in wide_df.columns: wide_df[col] = ''\n",
        "#         wide_df.loc[wide_df['gene_symbol']==gsym, col] = coord\n",
        "#     if not wide_df.empty:\n",
        "#         wide_df.to_csv(WIDE_ARCH_SNAPSHOT, sep='\\t', index=False)\n",
        "#         logger.info(f\"Final wide architecture rows: {len(wide_df)} (saved)\")\n",
        "# else:\n",
        "#     logger.info(\"No consensus_long; skipping final wide output.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-6-header"
      },
      "source": [
        "# **Part 6: RegExTractor â€” Exonâ€‘centric rescue (replaces old Part 6)**\n",
        "\n",
        "This Part **replaces the previous frameshift/DNA rescue logic** with the\n",
        "exonâ€‘ and cladeâ€‘aware **RegExTractor** engine. It **learns tiered regex\n",
        "patterns per exon** from your **passed, highâ€‘quality** mappings (Parts 1â€“5),\n",
        "then scans the **notâ€‘yetâ€‘mapped** pool to recover additional COL1A1/COL1A2\n",
        "exons. It is tripletâ€‘aware (Gâ€‘Xâ€‘Y), entropyâ€‘aware, and chainâ€‘aware.\n",
        "\n",
        "> Integration notes\n",
        "> - **Does not touch** the heavy exon coordinate mapping built in **Cell 41**.\n",
        "> - Uses **`wide_df`** (your main passed output matrix) to derive the training set.\n",
        "> - Uses **`GENE_SYMBOLS`** global; restricts to **COL1A1/COL1A2** here.\n",
        "> - If you keep DNA utilities from the old Part 6, use them **after** regex anchoring.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 60 â€“ RegExTractor configuration\n",
        "\n",
        "Parameters for pattern learning and matching. Uses `GENE_SYMBOLS` from earlier\n",
        "Parts. To focus on COL1A1/COL1A2 initially, we filter at call time.\n"
      ],
      "metadata": {
        "id": "ND5-VHg3j-iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 60 =====\n",
        "# RegExTractor configuration (Colab-friendly; non-breaking)\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Iterable, Any\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- User-tunable parameters (safe defaults). Adjust with Colab #@param if desired.\n",
        "rex_min_clade_samples = 8                     #@param {type:\"integer\"}\n",
        "rex_anchor_triplets_min = 2                   #@param {type:\"integer\"}\n",
        "rex_anchor_triplets_max = 4                   #@param {type:\"integer\"}\n",
        "rex_anchor_entropy_max = 0.25                 #@param {type:\"number\"}\n",
        "rex_freq_threshold_strict = 0.05              #@param {type:\"number\"}\n",
        "rex_freq_threshold_moderate = 0.01            #@param {type:\"number\"}\n",
        "rex_ghead_density_min = 0.80                  #@param {type:\"number\"}\n",
        "rex_len_tolerance_strict = 1                  #@param {type:\"integer\"}\n",
        "rex_len_tolerance_moderate = 3                #@param {type:\"integer\"}\n",
        "rex_len_tolerance_loose = 6                   #@param {type:\"integer\"}\n",
        "rex_chain_min_consecutive = 3                 #@param {type:\"integer\"}\n",
        "rex_search_window_pad = 90                    #@param {type:\"integer\"}\n",
        "rex_enable_fullregex_fallback = True          #@param {type:\"boolean\"}\n",
        "\n",
        "# Taxonomic levels in descending specificity (must match your columns if available)\n",
        "REX_TAXON_LEVELS = [\"genus\", \"family\", \"order\", \"class\", \"kingdom\", \"pan\"]\n",
        "\n",
        "def rex_log(msg: str):\n",
        "    \"\"\"Lightweight logger.\"\"\"\n",
        "    print(f\"[RegExTractor] {msg}\")\n"
      ],
      "metadata": {
        "id": "lbLZVEzjkAN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 61 â€“ Adapters and input preparation from Parts 1â€“5 outputs\n",
        "\n",
        "This cell converts **your passed wide output** (`wide_df`) into the **training**\n",
        "table RegExTractor needs. It also builds the **rejected/unmapped** pool from\n",
        "your **unmapped sequences** table if available.\n",
        "\n",
        "**Expected sources from earlier Parts**\n",
        "- `wide_df` â€” main passed matrix with columns:\n",
        "  - `accession`, `gene_symbol`,\n",
        "  - `exon_<N>_peptide`, `exon_<N>_coords`, `exon_<N>_status` (even N),\n",
        "  - optional taxonomy columns (`genus`, `family`, `order`, `class`, `kingdom`).\n",
        "- `UNMAPPED_SEQUENCES_DF` (optional) â€” a table of sequences not yet in `wide_df`,\n",
        "  with `accession`, `sequence`, and optional taxonomy columns.\n",
        "\n",
        "If `UNMAPPED_SEQUENCES_DF` is not defined, we will **only** scan rows in `wide_df`\n",
        "that have **few/no OK exons** *and* carry a full `sequence` column.\n"
      ],
      "metadata": {
        "id": "qJtQGKaXkDJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 61 =====\n",
        "@dataclass\n",
        "class RexColumnMap:\n",
        "    gene_col: str = \"gene_symbol\"\n",
        "    exon_col: str = \"exon_num_in_chain\"\n",
        "    pep_col: str = \"exon_peptide\"\n",
        "    seq_col: str = \"sequence\"\n",
        "    acc_col: str = \"accession\"\n",
        "    tax_cols: Dict[str, str] = field(default_factory=lambda: {\n",
        "        \"genus\": \"genus\",\n",
        "        \"family\": \"family\",\n",
        "        \"order\": \"order\",\n",
        "        \"class\": \"class\",\n",
        "        \"kingdom\": \"kingdom\"\n",
        "    })\n",
        "\n",
        "def rex_normalize_training_df(df: pd.DataFrame, cmap: RexColumnMap) -> pd.DataFrame:\n",
        "    needed = {cmap.gene_col, cmap.exon_col, cmap.pep_col}\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Training DF missing columns: {missing}\")\n",
        "    out = pd.DataFrame({\n",
        "        \"gene_symbol\": df[cmap.gene_col].astype(str),\n",
        "        \"exon_num_in_chain\": pd.to_numeric(df[cmap.exon_col], errors=\"coerce\").astype(\"Int64\"),\n",
        "        \"exon_peptide\": df[cmap.pep_col].astype(str)\n",
        "    })\n",
        "    for lvl in [\"genus\",\"family\",\"order\",\"class\",\"kingdom\"]:\n",
        "        col = cmap.tax_cols.get(lvl)\n",
        "        out[lvl] = df[col].astype(str) if (col in df.columns) else \"\"\n",
        "    out = out.dropna(subset=[\"exon_num_in_chain\"])\n",
        "    out = out[out[\"exon_peptide\"].str.len() > 0].copy()\n",
        "    return out\n",
        "\n",
        "def rex_normalize_rejected_df(df: pd.DataFrame, cmap: RexColumnMap) -> pd.DataFrame:\n",
        "    needed = {cmap.acc_col, cmap.seq_col}\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Rejected DF missing columns: {missing}\")\n",
        "    out = pd.DataFrame({\n",
        "        \"accession\": df[cmap.acc_col].astype(str),\n",
        "        \"sequence\": df[cmap.seq_col].astype(str)\n",
        "    })\n",
        "    for lvl in [\"genus\",\"family\",\"order\",\"class\",\"kingdom\"]:\n",
        "        col = cmap.tax_cols.get(lvl)\n",
        "        out[lvl] = df[col].astype(str) if (col in df.columns) else \"\"\n",
        "    out = out[out[\"sequence\"].str.len() > 0].copy()\n",
        "    return out\n",
        "\n",
        "# --- Derive training from PASSED wide_df ---\n",
        "if 'wide_df' not in globals():\n",
        "    raise NameError(\"`wide_df` not found. Run Parts 1â€“5 to produce the passed matrix before Part 6.\")\n",
        "\n",
        "# Use entropy stats if present (optional hook)\n",
        "entropy_stats_df = None\n",
        "for cand in [\"entropy_stats_df\", \"ENTROPY_STATS_DF\"]:\n",
        "    if cand in globals() and isinstance(globals()[cand], pd.DataFrame):\n",
        "        entropy_stats_df = globals()[cand]\n",
        "        break\n",
        "\n",
        "# Identify exon indices present\n",
        "exon_pat = re.compile(r\"^exon_(-?\\d+)_peptide$\")\n",
        "exon_indices = sorted({int(m.group(1)) for c in wide_df.columns for m in [exon_pat.match(c)] if m})\n",
        "\n",
        "# Count OK exons per row (used to filter rejected pool from wide_df if needed)\n",
        "ok_cols = [f\"exon_{n}_status\" for n in exon_indices if f\"exon_{n}_status\" in wide_df.columns]\n",
        "def _count_ok(row) -> int:\n",
        "    return sum(1 for stc in ok_cols if row.get(stc, \"\") == \"OK\")\n",
        "wide_df[\"_ok_exon_count\"] = wide_df.apply(_count_ok, axis=1)\n",
        "\n",
        "# Build long-form training table from PASSED exons (status == OK)\n",
        "training_rows = []\n",
        "tax_cols_available = [c for c in [\"genus\",\"family\",\"order\",\"class\",\"kingdom\"] if c in wide_df.columns]\n",
        "\n",
        "def rex_triplet_offset_best(seq: str) -> int:\n",
        "    best_off, best_cnt = 0, -1\n",
        "    for off in (0,1,2):\n",
        "        cnt = sum(1 for i,ch in enumerate(seq) if (i - off) % 3 == 0 and ch == \"G\")\n",
        "        if cnt > best_cnt:\n",
        "            best_cnt = cnt\n",
        "            best_off = off\n",
        "    return best_off\n",
        "\n",
        "def rex_ghead_density(seq: str, offset: Optional[int] = None) -> float:\n",
        "    if not seq:\n",
        "        return 0.0\n",
        "    off = rex_triplet_offset_best(seq) if offset is None else offset\n",
        "    triplets = max((len(seq) - off) // 3, 0)\n",
        "    if triplets == 0:\n",
        "        return 0.0\n",
        "    heads = [i for i in range(off, off + 3 * triplets, 3)]\n",
        "    g_cnt = sum(1 for i in heads if seq[i] == \"G\")\n",
        "    return g_cnt / len(heads) if heads else 0.0\n",
        "\n",
        "for _, row in wide_df.iterrows():\n",
        "    gene = row.get(\"gene_symbol\", \"\")\n",
        "    if not isinstance(gene, str) or len(gene) == 0:\n",
        "        continue\n",
        "    for n in exon_indices:\n",
        "        pep_c = f\"exon_{n}_peptide\"\n",
        "        st_c  = f\"exon_{n}_status\"\n",
        "        pep = row.get(pep_c, \"\")\n",
        "        st  = row.get(st_c, \"\")\n",
        "        if st != \"OK\" or not isinstance(pep, str) or len(pep) == 0:\n",
        "            continue\n",
        "        # Keep training peptides that obey triplet periodicity and have decent G-head density\n",
        "        if len(pep) % 3 != 0:\n",
        "            continue\n",
        "        if rex_ghead_density(pep) < 0.70:\n",
        "            continue\n",
        "        rec = {\n",
        "            \"gene_symbol\": gene,\n",
        "            \"exon_num_in_chain\": int(n),\n",
        "            \"exon_peptide\": pep\n",
        "        }\n",
        "        for t in tax_cols_available:\n",
        "            rec[t] = row.get(t, \"\")\n",
        "        training_rows.append(rec)\n",
        "\n",
        "mapped_exon_df = pd.DataFrame(training_rows)\n",
        "if mapped_exon_df.empty:\n",
        "    raise ValueError(\"Training set is empty. Check that wide_df contains exon_*_peptide with status == 'OK'.\")\n",
        "\n",
        "# Build rejected/unmapped pool\n",
        "# Preferred source: user-provided UNMAPPED_SEQUENCES_DF (not present in wide_df)\n",
        "if 'UNMAPPED_SEQUENCES_DF' in globals() and isinstance(UNMAPPED_SEQUENCES_DF, pd.DataFrame):\n",
        "    rejected_df = UNMAPPED_SEQUENCES_DF.copy()\n",
        "else:\n",
        "    # Fallback: try to use rows in wide_df with few OK exons and that include full sequences\n",
        "    candidate_seq_cols = [c for c in [\"sequence\",\"protein_sequence\",\"aa_sequence\",\"seq\",\"Sequence\"] if c in wide_df.columns]\n",
        "    sequence_col = candidate_seq_cols[0] if candidate_seq_cols else None\n",
        "    if sequence_col is None:\n",
        "        raise ValueError(\n",
        "            \"No UNMAPPED_SEQUENCES_DF provided and no sequence column found in wide_df.\\n\"\n",
        "            \"Provide a DataFrame `UNMAPPED_SEQUENCES_DF` with columns ['accession','sequence', optional taxon cols].\"\n",
        "        )\n",
        "    # Heuristic: treat rows with very low mapping success as 'rejected' to re-scan\n",
        "    rex_max_ok_exons_for_rejected = 6  # conservative; adjust if needed\n",
        "    rejected_rows = []\n",
        "    for _, row in wide_df.iterrows():\n",
        "        if row[\"_ok_exon_count\"] <= rex_max_ok_exons_for_rejected:\n",
        "            acc = row.get(\"accession\", \"\")\n",
        "            seq = row.get(sequence_col, \"\")\n",
        "            if isinstance(acc, str) and acc and isinstance(seq, str) and len(seq) > 0:\n",
        "                rr = {\"accession\": acc, \"sequence\": seq}\n",
        "                for t in tax_cols_available:\n",
        "                    rr[t] = row.get(t, \"\")\n",
        "                rejected_rows.append(rr)\n",
        "    rejected_df = pd.DataFrame(rejected_rows)\n",
        "    if rejected_df.empty:\n",
        "        raise ValueError(\n",
        "            \"Rejected pool is empty. Either supply UNMAPPED_SEQUENCES_DF or relax the ok-exon threshold.\"\n",
        "        )\n",
        "\n",
        "# Normalize for RegExTractor\n",
        "cmap = RexColumnMap(\n",
        "    gene_col=\"gene_symbol\",\n",
        "    exon_col=\"exon_num_in_chain\",\n",
        "    pep_col=\"exon_peptide\",\n",
        "    seq_col=\"sequence\",\n",
        "    acc_col=\"accession\",\n",
        "    tax_cols={k:k for k in [\"genus\",\"family\",\"order\",\"class\",\"kingdom\"] if k in tax_cols_available}\n",
        ")\n",
        "training_norm = rex_normalize_training_df(mapped_exon_df, cmap)\n",
        "rejected_norm = rex_normalize_rejected_df(rejected_df, cmap)\n",
        "\n",
        "rex_log(f\"Training set: {training_norm.shape}, Rejected set: {rejected_norm.shape}\")\n"
      ],
      "metadata": {
        "id": "b-50HX9NkFDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 62 â€“ Exon statistics & anchor discovery\n",
        "\n",
        "Tripletâ€‘aware utilities, Shannon entropy, and lowâ€‘entropy anchor finding to seed\n",
        "compact regexes. (Triplet logic mirrors collagen Gâ€‘Xâ€‘Y periodicity.)\n"
      ],
      "metadata": {
        "id": "bz-RXBq-kMoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 62 =====\n",
        "def rex_shannon_entropy(chars: Iterable[str]) -> float:\n",
        "    arr = np.array(list(chars))\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    vals, cnts = np.unique(arr, return_counts=True)\n",
        "    p = cnts / cnts.sum()\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "def rex_find_anchor_windows(\n",
        "    seqs: List[str],\n",
        "    k_min: int = rex_anchor_triplets_min,\n",
        "    k_max: int = rex_anchor_triplets_max,\n",
        "    entropy_max: float = rex_anchor_entropy_max\n",
        ") -> List[Tuple[int, int]]:\n",
        "    \"\"\"Return up to two non-overlapping low-entropy windows (AA indices) aligned to triplets.\"\"\"\n",
        "    if not seqs:\n",
        "        return []\n",
        "    offsets = [rex_triplet_offset_best(s) for s in seqs]\n",
        "    L = int(np.median([len(s) for s in seqs]))\n",
        "    # Build entropy per position, respecting triplet phase\n",
        "    ent = np.zeros(L, dtype=float)\n",
        "    # Collect characters per mod-3 position\n",
        "    cols = {0: [[] for _ in range(L)], 1: [[] for _ in range(L)], 2: [[] for _ in range(L)]}\n",
        "    for s in seqs:\n",
        "        ss = (s + \"-\" * max(0, L - len(s)))[:L]\n",
        "        for i, ch in enumerate(ss):\n",
        "            cols[i % 3][i].append(ch)\n",
        "    for i in range(L):\n",
        "        ent[i] = rex_shannon_entropy(cols[i % 3][i])\n",
        "    # Scan triplet windows for low-entropy runs\n",
        "    candidates: List[Tuple[int,int,float]] = []\n",
        "    for k in range(k_min, k_max + 1):\n",
        "        w = 3 * k\n",
        "        for i in range(0, L - w + 1, 3):\n",
        "            e = float(ent[i:i+w].mean())\n",
        "            if e <= entropy_max:\n",
        "                candidates.append((i, i+w, e))\n",
        "    candidates.sort(key=lambda x: x[2])\n",
        "    anchors: List[Tuple[int,int]] = []\n",
        "    for s, e, _ in candidates:\n",
        "        if not anchors:\n",
        "            anchors.append((s, e))\n",
        "        elif len(anchors) == 1 and (e <= anchors[0][0] or s >= anchors[0][1]):\n",
        "            anchors.append((s, e))\n",
        "            break\n",
        "    return sorted(anchors)\n"
      ],
      "metadata": {
        "id": "d3nZQJd8kMPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 63 â€“ Character classes & spacer summaries\n",
        "\n",
        "Build compact X/Y character classes per spacer region to assemble tiered regexes.\n"
      ],
      "metadata": {
        "id": "KZI-m4DSkW2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 63 =====\n",
        "def rex_char_classes_for_region(\n",
        "    seqs: List[str],\n",
        "    start: int,\n",
        "    end: int,\n",
        "    freq_threshold: float = 0.05\n",
        ") -> Tuple[str, str]:\n",
        "    if start >= end:\n",
        "        return \".\", \".\"\n",
        "    X_counts, Y_counts = {}, {}\n",
        "    for s in seqs:\n",
        "        if len(s) < end:\n",
        "            continue\n",
        "        for i in range(start, end, 3):\n",
        "            if i + 2 >= len(s):\n",
        "                break\n",
        "            x, y = s[i+1], s[i+2]\n",
        "            X_counts[x] = X_counts.get(x, 0) + 1\n",
        "            Y_counts[y] = Y_counts.get(y, 0) + 1\n",
        "\n",
        "    def build_class(d: Dict[str,int]) -> str:\n",
        "        if not d:\n",
        "            return \".\"\n",
        "        total = sum(d.values())\n",
        "        keep = sorted([aa for aa, c in d.items() if c/total >= freq_threshold])\n",
        "        if not keep:\n",
        "            keep = [max(d.items(), key=lambda kv: kv[1])[0]]\n",
        "        if len(keep) == 1:\n",
        "            return re.escape(keep[0])\n",
        "        return \"[\" + \"\".join(sorted(set(keep))) + \"]\"\n",
        "\n",
        "    return build_class(X_counts), build_class(Y_counts)\n"
      ],
      "metadata": {
        "id": "lwOP6RzQkZWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 64 â€“ Tiered pattern builder (per exon, per clade)\n",
        "\n",
        "Construct strictâ†’loose tiers (A/B/C/D) using anchors and spacer summaries.\n"
      ],
      "metadata": {
        "id": "seWkXp2bkcKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 64 =====\n",
        "@dataclass\n",
        "class ExonStats:\n",
        "    exon_len_median: int\n",
        "    exon_len_q1: int\n",
        "    exon_len_q3: int\n",
        "    anchors: List[Tuple[int, int]]\n",
        "    ghead_density: float\n",
        "\n",
        "@dataclass\n",
        "class ExonTierPattern:\n",
        "    tier: str\n",
        "    regex: re.Pattern\n",
        "    anchors_literals: List[str]\n",
        "    len_range: Tuple[int, int]\n",
        "    ghead_min: float\n",
        "\n",
        "@dataclass\n",
        "class ExonRegexLibraryEntry:\n",
        "    gene_symbol: str\n",
        "    exon_num_in_chain: int\n",
        "    clade_key: str\n",
        "    tiers: List[ExonTierPattern]\n",
        "\n",
        "class RegExTractorBuilder:\n",
        "    def __init__(self,\n",
        "                 anchor_k_min: int = rex_anchor_triplets_min,\n",
        "                 anchor_k_max: int = rex_anchor_triplets_max,\n",
        "                 anchor_entropy_max: float = rex_anchor_entropy_max):\n",
        "        self.kmin = anchor_k_min\n",
        "        self.kmax = anchor_k_max\n",
        "        self.entmax = anchor_entropy_max\n",
        "\n",
        "    @staticmethod\n",
        "    def _length_stats(peps: List[str]) -> Tuple[int,int,int]:\n",
        "        lengths = np.array([len(p) for p in peps if p], dtype=int)\n",
        "        if lengths.size == 0:\n",
        "            return 0,0,0\n",
        "        return int(np.median(lengths)), int(np.percentile(lengths,25)), int(np.percentile(lengths,75))\n",
        "\n",
        "    def build_stats(self, peps: List[str]) -> ExonStats:\n",
        "        if not peps:\n",
        "            return ExonStats(0,0,0,[],0.0)\n",
        "        Lmed, Lq1, Lq3 = self._length_stats(peps)\n",
        "        anchors = rex_find_anchor_windows(peps, k_min=self.kmin, k_max=self.kmax, entropy_max=self.entmax)\n",
        "        dens = float(np.mean([rex_ghead_density(p) for p in peps])) if peps else 0.0\n",
        "        return ExonStats(Lmed, Lq1, Lq3, anchors, dens)\n",
        "\n",
        "    def _anchor_literals(self, peps: List[str], anchors: List[Tuple[int,int]]) -> List[str]:\n",
        "        lits = []\n",
        "        for (s,e) in anchors:\n",
        "            counter: Dict[str,int] = {}\n",
        "            for p in peps:\n",
        "                if len(p) < e:\n",
        "                    continue\n",
        "                frag = p[s:e]\n",
        "                counter[frag] = counter.get(frag, 0) + 1\n",
        "            if counter:\n",
        "                top = max(counter.items(), key=lambda kv: kv[1])[0]\n",
        "                lits.append(top)\n",
        "        return lits\n",
        "\n",
        "    def _spacer_token(self, Xc: str, Yc: str, rep_range: Tuple[int,int]) -> str:\n",
        "        m, n = rep_range\n",
        "        return f\"(?:G{Xc}{Yc})\" + (f\"{{{m},{n}}}\" if m != n else f\"{{{m}}}\")\n",
        "\n",
        "    def build_tiers(\n",
        "        self,\n",
        "        peps: List[str],\n",
        "        stats: ExonStats,\n",
        "        freq_strict: float = rex_freq_threshold_strict,\n",
        "        freq_moderate: float = rex_freq_threshold_moderate\n",
        "    ) -> List[ExonTierPattern]:\n",
        "        tiers: List[ExonTierPattern] = []\n",
        "        if not peps or stats.exon_len_median <= 0:\n",
        "            return tiers\n",
        "\n",
        "        Lmed, Lq1, Lq3 = stats.exon_len_median, stats.exon_len_q1, stats.exon_len_q3\n",
        "        anchor_lits = self._anchor_literals(peps, stats.anchors)\n",
        "\n",
        "        # Define regions (left/middle/right) around anchors\n",
        "        if stats.anchors:\n",
        "            if len(stats.anchors) == 1:\n",
        "                (a1s,a1e) = stats.anchors[0]\n",
        "                regions = [(0,a1s), (a1e,Lmed)]\n",
        "            else:\n",
        "                (a1s,a1e), (a2s,a2e) = sorted(stats.anchors)\n",
        "                regions = [(0,a1s), (a1e,a2s), (a2e,Lmed)]\n",
        "        else:\n",
        "            regions = [(0,Lmed)]\n",
        "\n",
        "        xy_strict = [rex_char_classes_for_region(peps, s, e, freq_strict) for (s,e) in regions]\n",
        "        xy_mod    = [rex_char_classes_for_region(peps, s, e, freq_moderate) for (s,e) in regions]\n",
        "\n",
        "        def triplet_len_range(s:int, e:int, tol:int) -> Tuple[int,int]:\n",
        "            L = max(e - s, 0)\n",
        "            m = max((L // 3) - tol, 0)\n",
        "            n = max((L // 3) + tol, 0)\n",
        "            if m > n:\n",
        "                m, n = n, m\n",
        "            return m, n\n",
        "\n",
        "        strict_tol = rex_len_tolerance_strict\n",
        "        mod_tol    = rex_len_tolerance_moderate\n",
        "        loose_tol  = rex_len_tolerance_loose\n",
        "\n",
        "        def assemble(xy_classes, tol) -> str:\n",
        "            parts = []\n",
        "            if stats.anchors:\n",
        "                a1s, a1e = stats.anchors[0]\n",
        "                if (a1s - 0) >= 3:\n",
        "                    Xc,Yc = xy_classes[0]\n",
        "                    m,n = triplet_len_range(0, a1s, tol)\n",
        "                    parts.append(self._spacer_token(Xc,Yc,(m,n)))\n",
        "                if anchor_lits:\n",
        "                    parts.append(re.escape(anchor_lits[0]))\n",
        "                if len(stats.anchors) == 2:\n",
        "                    a2s, a2e = stats.anchors[1]\n",
        "                    if (a2s - a1e) >= 3:\n",
        "                        Xc,Yc = xy_classes[1]\n",
        "                        m,n = triplet_len_range(a1e, a2s, tol)\n",
        "                        parts.append(self._spacer_token(Xc,Yc,(m,n)))\n",
        "                    if len(anchor_lits) > 1:\n",
        "                        parts.append(re.escape(anchor_lits[1]))\n",
        "                    if (Lmed - a2e) >= 3:\n",
        "                        Xc,Yc = xy_classes[2]\n",
        "                        m,n = triplet_len_range(a2e, Lmed, tol)\n",
        "                        parts.append(self._spacer_token(Xc,Yc,(m,n)))\n",
        "                else:\n",
        "                    if (Lmed - a1e) >= 3:\n",
        "                        Xc,Yc = xy_classes[1]\n",
        "                        m,n = triplet_len_range(a1e, Lmed, tol)\n",
        "                        parts.append(self._spacer_token(Xc,Yc,(m,n)))\n",
        "            else:\n",
        "                Xc,Yc = xy_classes[0]\n",
        "                m,n = triplet_len_range(0, Lmed, tol)\n",
        "                parts.append(self._spacer_token(Xc,Yc,(m,n)))\n",
        "            return \"\".join(parts)\n",
        "\n",
        "        # Tier A\n",
        "        pat_A = assemble(xy_strict, strict_tol)\n",
        "        if pat_A:\n",
        "            tiers.append(ExonTierPattern(\n",
        "                tier=\"A\",\n",
        "                regex=re.compile(pat_A, flags=re.ASCII),\n",
        "                anchors_literals=anchor_lits,\n",
        "                len_range=(Lmed - strict_tol*3, Lmed + strict_tol*3),\n",
        "                ghead_min=max(rex_ghead_density_min, stats.ghead_density * 0.9)\n",
        "            ))\n",
        "        # Tier B\n",
        "        pat_B = assemble(xy_mod, mod_tol)\n",
        "        if pat_B:\n",
        "            tiers.append(ExonTierPattern(\n",
        "                tier=\"B\",\n",
        "                regex=re.compile(pat_B, flags=re.ASCII),\n",
        "                anchors_literals=anchor_lits,\n",
        "                len_range=(Lmed - mod_tol*3, Lmed + mod_tol*3),\n",
        "                ghead_min=max(rex_ghead_density_min * 0.9, stats.ghead_density * 0.8)\n",
        "            ))\n",
        "        # Tier C (anchors + degenerate triplets)\n",
        "        xy_loose = [(\".\", \".\") for _ in (anchor_lits if len(stats.anchors)==2 else [0,1])][:len(regions)]\n",
        "        if not xy_loose or len(xy_loose) != len(regions):\n",
        "            xy_loose = [(\".\", \".\") for _ in regions]\n",
        "        pat_C = assemble(xy_loose, loose_tol)\n",
        "        if pat_C:\n",
        "            tiers.append(ExonTierPattern(\n",
        "                tier=\"C\",\n",
        "                regex=re.compile(pat_C, flags=re.ASCII),\n",
        "                anchors_literals=anchor_lits,\n",
        "                len_range=(Lmed - loose_tol*3, Lmed + loose_tol*3),\n",
        "                ghead_min=max(rex_ghead_density_min * 0.8, 0.65)\n",
        "            ))\n",
        "        # Tier D (fallback)\n",
        "        kmin = max((Lmed // 3) - loose_tol, 1)\n",
        "        kmax = (Lmed // 3) + loose_tol\n",
        "        pat_D = f\"(?:G..)\" + (f\"{{{kmin},{kmax}}}\" if kmin != kmax else f\"{{{kmin}}}\")\n",
        "        tiers.append(ExonTierPattern(\n",
        "            tier=\"D\",\n",
        "            regex=re.compile(pat_D, flags=re.ASCII),\n",
        "            anchors_literals=[],\n",
        "            len_range=(kmin*3, kmax*3),\n",
        "            ghead_min=0.65\n",
        "        ))\n",
        "        return tiers\n"
      ],
      "metadata": {
        "id": "ZtAbBYahkb5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 65 â€“ Library builder (per gene/exon/clade)\n"
      ],
      "metadata": {
        "id": "LEuOANEQkiaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 65 =====\n",
        "class RegExTractorLibrary:\n",
        "    def __init__(self):\n",
        "        self.entries: Dict[Tuple[str,int,str], ExonRegexLibraryEntry] = {}\n",
        "        self.stats: Dict[Tuple[str,int,str], ExonStats] = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def _clade_keys(df: pd.DataFrame) -> List[str]:\n",
        "        keys = []\n",
        "        for lvl in REX_TAXON_LEVELS:\n",
        "            if lvl == \"pan\":\n",
        "                keys.append(\"pan\")\n",
        "            elif lvl in df.columns and df[lvl].astype(str).str.len().gt(0).any():\n",
        "                keys.extend(sorted(df[lvl].astype(str).unique()))\n",
        "        return list(dict.fromkeys(keys))\n",
        "\n",
        "    @staticmethod\n",
        "    def _subset_by_clade(df: pd.DataFrame, clade_key: str) -> pd.DataFrame:\n",
        "        if clade_key == \"pan\":\n",
        "            return df\n",
        "        for lvl in [\"genus\",\"family\",\"order\",\"class\",\"kingdom\"]:\n",
        "            if lvl in df.columns and clade_key in set(df[lvl].astype(str).unique()):\n",
        "                return df[df[lvl].astype(str) == clade_key]\n",
        "        return df.iloc[0:0].copy()\n",
        "\n",
        "    def build(self, training_df: pd.DataFrame, genes: List[str], min_clade_samples: int = rex_min_clade_samples):\n",
        "        builder = RegExTractorBuilder()\n",
        "        for gene in genes:\n",
        "            gdf = training_df[training_df[\"gene_symbol\"] == gene]\n",
        "            if gdf.empty:\n",
        "                rex_log(f\"No training rows for {gene}\")\n",
        "                continue\n",
        "            for exon in sorted(gdf[\"exon_num_in_chain\"].dropna().astype(int).unique()):\n",
        "                edf = gdf[gdf[\"exon_num_in_chain\"] == exon]\n",
        "                if edf.empty:\n",
        "                    continue\n",
        "                clade_keys = self._clade_keys(edf)\n",
        "                if \"pan\" not in clade_keys:\n",
        "                    clade_keys.append(\"pan\")\n",
        "                for ck in clade_keys:\n",
        "                    sub = self._subset_by_clade(edf, ck)\n",
        "                    if ck != \"pan\" and len(sub) < min_clade_samples:\n",
        "                        continue\n",
        "                    peps = sub[\"exon_peptide\"].astype(str).tolist()\n",
        "                    stats = builder.build_stats(peps)\n",
        "                    tiers = builder.build_tiers(peps, stats)\n",
        "                    if not tiers:\n",
        "                        continue\n",
        "                    key = (gene, exon, ck)\n",
        "                    self.entries[key] = ExonRegexLibraryEntry(\n",
        "                        gene_symbol=gene,\n",
        "                        exon_num_in_chain=exon,\n",
        "                        clade_key=ck,\n",
        "                        tiers=tiers\n",
        "                    )\n",
        "                    self.stats[key] = stats\n",
        "        rex_log(f\"Built {len(self.entries)} exon/clade entries across {len(genes)} genes.\")\n"
      ],
      "metadata": {
        "id": "mRIKT8NZkkqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 66 â€“ Matching & scoring\n"
      ],
      "metadata": {
        "id": "ukUe139ZknFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 66 =====\n",
        "@dataclass\n",
        "class RexHit:\n",
        "    accession: str\n",
        "    gene_symbol: str\n",
        "    exon_num_in_chain: int\n",
        "    clade_used: str\n",
        "    tier: str\n",
        "    start: int\n",
        "    end: int\n",
        "    matched_peptide: str\n",
        "    ghead_density: float\n",
        "    len_residual: int\n",
        "    anchors_hit: int\n",
        "    score: float\n",
        "\n",
        "class RegExTractorMatcher:\n",
        "    def __init__(self, library: RegExTractorLibrary):\n",
        "        self.lib = library\n",
        "\n",
        "    @staticmethod\n",
        "    def _tier_weight(tier: str) -> float:\n",
        "        return {\"A\": 1.0, \"B\": 0.85, \"C\": 0.6, \"D\": 0.3}.get(tier, 0.1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _choose_clade_key(row: pd.Series, keys_for_exon: List[str]) -> str:\n",
        "        for lvl in REX_TAXON_LEVELS:\n",
        "            if lvl == \"pan\":\n",
        "                continue\n",
        "            if lvl in row.index and isinstance(row[lvl], str) and row[lvl]:\n",
        "                if row[lvl] in keys_for_exon:\n",
        "                    return row[lvl]\n",
        "        return \"pan\"\n",
        "\n",
        "    def _scan_with_anchors(self, seq: str, pattern: ExonTierPattern, window_pad: int = rex_search_window_pad) -> List[Tuple[int,int,str]]:\n",
        "        hits: List[Tuple[int,int,str]] = []\n",
        "        if pattern.anchors_literals:\n",
        "            for lit in pattern.anchors_literals:\n",
        "                start_idx = 0\n",
        "                while True:\n",
        "                    idx = seq.find(lit, start_idx)\n",
        "                    if idx == -1:\n",
        "                        break\n",
        "                    wstart = max(0, idx - window_pad)\n",
        "                    wend   = min(len(seq), idx + len(lit) + window_pad)\n",
        "                    m = pattern.regex.search(seq, wstart, wend)\n",
        "                    if m:\n",
        "                        hits.append((m.start(), m.end(), m.group(0)))\n",
        "                        start_idx = m.end()\n",
        "                    else:\n",
        "                        start_idx = idx + 1\n",
        "        if (not hits) and rex_enable_fullregex_fallback:\n",
        "            for m in pattern.regex.finditer(seq):\n",
        "                hits.append((m.start(), m.end(), m.group(0)))\n",
        "        return hits\n",
        "\n",
        "    def _score_hit(self, pep: str, tier: ExonTierPattern) -> Tuple[float,float,int,int]:\n",
        "        gden = rex_ghead_density(pep)\n",
        "        len_res = min(abs(len(pep) - tier.len_range[0]), abs(len(pep) - tier.len_range[1]))\n",
        "        anchors_hit = sum(1 for lit in tier.anchors_literals if lit in pep)\n",
        "        score = (\n",
        "            0.45 * self._tier_weight(tier.tier) +\n",
        "            0.30 * max(0.0, (gden - (tier.ghead_min - 0.1)) / 0.2) +\n",
        "            0.15 * (1.0 - min(len_res / 9.0, 1.0)) +\n",
        "            0.10 * (anchors_hit / max(len(tier.anchors_literals), 1))\n",
        "        )\n",
        "        return float(score), float(gden), int(len_res), int(anchors_hit)\n",
        "\n",
        "    def scan_sequence_row(self, row: pd.Series, genes: List[str]) -> List[RexHit]:\n",
        "        seq = row[\"sequence\"]; accession = row[\"accession\"]\n",
        "        all_hits: List[RexHit] = []\n",
        "        for gene in genes:\n",
        "            entries = [(k,v) for k,v in self.lib.entries.items() if k[0] == gene]\n",
        "            exons   = sorted(set([k[1] for k,_ in entries]))\n",
        "            for exon in exons:\n",
        "                keys_for_exon = [k[2] for k,_ in entries if k[1] == exon]\n",
        "                clade = self._choose_clade_key(row, keys_for_exon)\n",
        "                key = (gene, exon, clade) if (gene,exon,clade) in self.lib.entries else (gene,exon,\"pan\")\n",
        "                entry = self.lib.entries.get(key)\n",
        "                if not entry:\n",
        "                    continue\n",
        "                for tier in entry.tiers:\n",
        "                    for s,e,mtxt in self._scan_with_anchors(seq, tier):\n",
        "                        score,gden,lres,ahit = self._score_hit(mtxt, tier)\n",
        "                        if gden < (tier.ghead_min - 0.05):\n",
        "                            continue\n",
        "                        all_hits.append(RexHit(\n",
        "                            accession=accession,\n",
        "                            gene_symbol=gene,\n",
        "                            exon_num_in_chain=exon,\n",
        "                            clade_used=entry.clade_key,\n",
        "                            tier=tier.tier,\n",
        "                            start=s, end=e,\n",
        "                            matched_peptide=mtxt,\n",
        "                            ghead_density=gden,\n",
        "                            len_residual=lres,\n",
        "                            anchors_hit=ahit,\n",
        "                            score=score\n",
        "                        ))\n",
        "                    if any(h.tier==\"A\" and h.score>=0.85 for h in all_hits):\n",
        "                        break\n",
        "        # Keep highest-score per (gene,exon)\n",
        "        best: Dict[Tuple[str,int], RexHit] = {}\n",
        "        for h in sorted(all_hits, key=lambda x: x.score, reverse=True):\n",
        "            key = (h.gene_symbol, h.exon_num_in_chain)\n",
        "            if key not in best:\n",
        "                best[key] = h\n",
        "        return list(best.values())\n"
      ],
      "metadata": {
        "id": "LB4onj6mkpIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 67 â€“ Chain reconstruction (anchorâ€‘andâ€‘walk)\n",
        "\n",
        "Seed with the strongest exon hit and walk downstream/upstream using expected\n",
        "exon order to assemble coherent blocks.\n"
      ],
      "metadata": {
        "id": "G9XnkbhjkryF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 67 =====\n",
        "@dataclass\n",
        "class RexChain:\n",
        "    accession: str\n",
        "    gene_symbol: str\n",
        "    seed_exon: int\n",
        "    exons: List[int]\n",
        "    coords: List[Tuple[int,int]]\n",
        "    tiers: List[str]\n",
        "    mean_score: float\n",
        "    consecutive_blocks: int\n",
        "\n",
        "def rex_exon_order_model(training_df: pd.DataFrame, gene: str) -> List[int]:\n",
        "    exons = (\n",
        "        training_df[training_df[\"gene_symbol\"] == gene][\"exon_num_in_chain\"]\n",
        "        .dropna().astype(int).sort_values().unique().tolist()\n",
        "    )\n",
        "    return exons\n",
        "\n",
        "def rex_walk_chain(\n",
        "    seq_len: int,\n",
        "    seed: RexHit,\n",
        "    hits_for_gene: Dict[int, RexHit],\n",
        "    exon_order: List[int],\n",
        "    window_pad: int = rex_search_window_pad\n",
        ") -> RexChain:\n",
        "    if seed.exon_num_in_chain not in exon_order:\n",
        "        return RexChain(seed.accession, seed.gene_symbol, seed.exon_num_in_chain,\n",
        "                        [seed.exon_num_in_chain], [(seed.start, seed.end)],\n",
        "                        [seed.tier], seed.score, 1)\n",
        "\n",
        "    idx = exon_order.index(seed.exon_num_in_chain)\n",
        "    exons = [seed.exon_num_in_chain]\n",
        "    coords = [(seed.start, seed.end)]\n",
        "    tiers = [seed.tier]\n",
        "    last_end = seed.end\n",
        "    consec = 1\n",
        "\n",
        "    # downstream\n",
        "    for j in range(idx+1, len(exon_order)):\n",
        "        ex = exon_order[j]\n",
        "        h = hits_for_gene.get(ex)\n",
        "        if not h:\n",
        "            break\n",
        "        if h.start >= last_end and h.start <= last_end + window_pad:\n",
        "            exons.append(ex); coords.append((h.start,h.end)); tiers.append(h.tier)\n",
        "            last_end = h.end; consec += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # upstream\n",
        "    first_start = seed.start\n",
        "    for j in range(idx-1, -1, -1):\n",
        "        ex = exon_order[j]\n",
        "        h = hits_for_gene.get(ex)\n",
        "        if not h:\n",
        "            break\n",
        "        if h.end <= first_start and h.end >= max(0, first_start - window_pad):\n",
        "            exons.insert(0, ex); coords.insert(0,(h.start,h.end)); tiers.insert(0,h.tier)\n",
        "            first_start = h.start; consec += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    mean_score = float(np.mean([hits_for_gene[e].score if e in hits_for_gene else 0.0 for e in exons]))\n",
        "    return RexChain(seed.accession, seed.gene_symbol, seed.exon_num_in_chain, exons, coords, tiers, mean_score, consec)\n"
      ],
      "metadata": {
        "id": "LKUoS3oZktYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 68 â€“ Orchestrator (build â†’ scan â†’ chain)\n",
        "\n",
        "Build the library from passed exons, scan the rejected pool, and assemble chains.\n"
      ],
      "metadata": {
        "id": "4cKULjiJkvvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 68 =====\n",
        "def rex_run_regextractor(\n",
        "    training_df: pd.DataFrame,\n",
        "    rejected_df: pd.DataFrame,\n",
        "    genes: List[str],\n",
        "    min_clade_samples: int = rex_min_clade_samples\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    lib = RegExTractorLibrary()\n",
        "    lib.build(training_df, genes, min_clade_samples=min_clade_samples)\n",
        "    matcher = RegExTractorMatcher(lib)\n",
        "\n",
        "    hits_rows: List[Dict[str,Any]] = []\n",
        "    chains_rows: List[Dict[str,Any]] = []\n",
        "\n",
        "    exon_orders = {g: rex_exon_order_model(training_df, g) for g in genes}\n",
        "\n",
        "    for _, row in rejected_df.iterrows():\n",
        "        seq_hits = matcher.scan_sequence_row(row, genes=genes)\n",
        "        if not seq_hits:\n",
        "            continue\n",
        "        for h in seq_hits:\n",
        "            hits_rows.append({\n",
        "                \"accession\": h.accession,\n",
        "                \"gene_symbol\": h.gene_symbol,\n",
        "                \"exon_num_in_chain\": h.exon_num_in_chain,\n",
        "                \"clade_used\": h.clade_used,\n",
        "                \"tier\": h.tier,\n",
        "                \"start\": h.start, \"end\": h.end,\n",
        "                \"length\": h.end - h.start,\n",
        "                \"matched_peptide\": h.matched_peptide,\n",
        "                \"ghead_density\": h.ghead_density,\n",
        "                \"len_residual\": h.len_residual,\n",
        "                \"anchors_hit\": h.anchors_hit,\n",
        "                \"score\": h.score\n",
        "            })\n",
        "\n",
        "        by_gene: Dict[str, List[RexHit]] = {}\n",
        "        for h in seq_hits:\n",
        "            by_gene.setdefault(h.gene_symbol, []).append(h)\n",
        "\n",
        "        for gene, ghits in by_gene.items():\n",
        "            hits_by_exon = {h.exon_num_in_chain: h for h in ghits}\n",
        "            seed = sorted(ghits, key=lambda x: x.score, reverse=True)[0]\n",
        "            chain = rex_walk_chain(\n",
        "                seq_len=len(row[\"sequence\"]),\n",
        "                seed=seed,\n",
        "                hits_for_gene=hits_by_exon,\n",
        "                exon_order=exon_orders.get(gene, [])\n",
        "            )\n",
        "            chains_rows.append({\n",
        "                \"accession\": chain.accession,\n",
        "                \"gene_symbol\": chain.gene_symbol,\n",
        "                \"seed_exon\": chain.seed_exon,\n",
        "                \"exons\": chain.exons,\n",
        "                \"coords\": chain.coords,\n",
        "                \"tiers\": chain.tiers,\n",
        "                \"mean_score\": chain.mean_score,\n",
        "                \"consecutive_blocks\": chain.consecutive_blocks\n",
        "            })\n",
        "\n",
        "    rex_hits_df = pd.DataFrame(hits_rows)\n",
        "    rex_chains_df = pd.DataFrame(chains_rows)\n",
        "    if not rex_chains_df.empty:\n",
        "        rex_chains_df = rex_chains_df[\n",
        "            rex_chains_df[\"consecutive_blocks\"].astype(int) >= rex_chain_min_consecutive\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "    rex_log(f\"Emitted {len(rex_hits_df)} exon hits and {len(rex_chains_df)} chains.\")\n",
        "    return rex_hits_df, rex_chains_df\n"
      ],
      "metadata": {
        "id": "lmQ4HL4wkxTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 69 â€“ Run RegExTractor and export\n",
        "\n",
        "Uses **training_norm** (from `wide_df`) and **rejected_norm** (from unmapped pool).\n",
        "Restricts to **COL1A1/COL1A2** via `GENE_SYMBOLS`.\n"
      ],
      "metadata": {
        "id": "_eLMfUcJk0WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 69 =====\n",
        "# Pick targets from global GENE_SYMBOLS\n",
        "if 'GENE_SYMBOLS' in globals():\n",
        "    target_genes = [g for g in GENE_SYMBOLS if g in {\"COL1A1\",\"COL1A2\"}]\n",
        "else:\n",
        "    target_genes = [\"COL1A1\",\"COL1A2\"]\n",
        "\n",
        "rex_hits_df, rex_chains_df = rex_run_regextractor(\n",
        "    training_norm,\n",
        "    rejected_norm,\n",
        "    genes=target_genes\n",
        ")\n",
        "\n",
        "# Inspect\n",
        "try:\n",
        "    display(rex_hits_df.head(20))\n",
        "    display(rex_chains_df.head(10))\n",
        "except Exception:\n",
        "    print(rex_hits_df.head(20))\n",
        "    print(rex_chains_df.head(10))\n",
        "\n",
        "# Save to /content (non-breaking; does not depend on any prior path constants)\n",
        "hits_path   = \"/content/rex_hits_COL1A1_COL1A2.csv\"\n",
        "chains_path = \"/content/rex_chains_COL1A1_COL1A2.csv\"\n",
        "rex_hits_df.to_csv(hits_path, index=False)\n",
        "rex_chains_df.to_csv(chains_path, index=False)\n",
        "rex_log(f\"Saved:\\n  {hits_path}\\n  {chains_path}\")\n"
      ],
      "metadata": {
        "id": "F6bd94pdk2wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-7-header"
      },
      "source": [
        "# **Part 7: Shannon Entropy Analysis & Visualisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-71-md"
      },
      "source": [
        "## Cell 71 â€“ Per-exon entropy and plotting\n",
        "\n",
        "Computes median entropy per exon and saves bar+error plots.\n",
        "Charts use **matplotlib** only (no seaborn)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 71 =====\n",
        "# Shannon entropy per exon (robust computation + safe plotting)\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def shannon_entropy(col: list[str]) -> float:\n",
        "    \"\"\"Shannon entropy (base-2) for a list of single-character residues.\"\"\"\n",
        "    if not col:\n",
        "        return 0.0\n",
        "    vals, cnts = np.unique(col, return_counts=True)\n",
        "    p = cnts / cnts.sum()\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "def pad_peptides(peps: list[str]) -> np.ndarray:\n",
        "    \"\"\"Pad peptides to a rectangular (n_peps x max_len) array with '' as filler.\"\"\"\n",
        "    m = max((len(x) for x in peps), default=0)\n",
        "    arr = np.full((len(peps), m), '', dtype=object)\n",
        "    for i, s in enumerate(peps):\n",
        "        for j, ch in enumerate(s):\n",
        "            arr[i, j] = ch\n",
        "    return arr\n",
        "\n",
        "def safe_median(values, default=0.0) -> float:\n",
        "    \"\"\"Median that returns `default` for empty/all-NaN inputs.\"\"\"\n",
        "    arr = np.asarray(list(values), dtype=float)\n",
        "    if arr.size == 0 or not np.isfinite(arr).any():\n",
        "        return float(default)\n",
        "    return float(np.nanmedian(arr))\n",
        "\n",
        "def exon_idx(colname: str) -> int:\n",
        "    \"\"\"Extract numeric exon index from 'exon_<N>_peptide' for proper sorting.\"\"\"\n",
        "    m = re.search(r'exon_(\\d+)_peptide', colname or \"\")\n",
        "    return int(m.group(1)) if m else 10**9\n",
        "\n",
        "entropy_rows = []\n",
        "n_plots = 0\n",
        "\n",
        "if 'wide_df' in globals() and not wide_df.empty:\n",
        "    for g, sub in wide_df.groupby('gene_symbol', dropna=False):\n",
        "        exon_cols = [c for c in sub.columns\n",
        "                     if isinstance(c, str) and c.startswith('exon_') and c.endswith('_peptide')]\n",
        "        exon_cols = sorted(exon_cols, key=exon_idx)\n",
        "\n",
        "        # Per-exon entropy & median length\n",
        "        for c in exon_cols:\n",
        "            peps = sub[c].fillna('').astype(str).tolist()\n",
        "            # entropy: position-wise entropy, then median across positions\n",
        "            arr = pad_peptides(peps)\n",
        "            ents = []\n",
        "            for j in range(arr.shape[1]):\n",
        "                # characters present at column j (skip empty fillers)\n",
        "                col = [ch for ch in arr[:, j].tolist() if ch]\n",
        "                if not col:\n",
        "                    continue\n",
        "                ents.append(shannon_entropy(col))\n",
        "            exon_entropy = safe_median(ents, default=0.0)\n",
        "\n",
        "            # robust median length (ignore empty strings)\n",
        "            lengths = [len(p) for p in peps if p]\n",
        "            exon_length = int(round(safe_median(lengths, default=0.0)))\n",
        "\n",
        "            entropy_rows.append({\n",
        "                'gene_symbol': g,\n",
        "                'exon_col': c,\n",
        "                'median_length': exon_length,\n",
        "                'entropy': exon_entropy\n",
        "            })\n",
        "\n",
        "    entropy_df = pd.DataFrame(entropy_rows)\n",
        "\n",
        "    if not entropy_df.empty:\n",
        "        # Save table\n",
        "        entropy_df.to_csv(ENTROPY_TSV, sep='\\t', index=False)\n",
        "        logger.info(f\"Entropy stats rows: {len(entropy_df)} (saved â†’ {ENTROPY_TSV.name})\")\n",
        "\n",
        "        # Plots: exon index on X, median length as bars, entropy as error bars\n",
        "        for g, sub in entropy_df.groupby('gene_symbol', dropna=False):\n",
        "            sub = sub.copy()\n",
        "            sub['exon_idx'] = sub['exon_col'].map(exon_idx)\n",
        "            sub = sub.sort_values('exon_idx')\n",
        "\n",
        "            xs = list(range(len(sub)))\n",
        "            heights = [int(h) if np.isfinite(h) else 0 for h in sub['median_length'].tolist()]\n",
        "            errs = [float(e) if np.isfinite(e) else 0.0 for e in sub['entropy'].tolist()]\n",
        "\n",
        "            try:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.bar(xs, heights)\n",
        "                plt.errorbar(xs, heights, yerr=errs, fmt='none')\n",
        "                plt.title(f\"Exon length (bars) + entropy (errors) â€” {g}\")\n",
        "                plt.xlabel(\"Exon index (sorted)\")\n",
        "                plt.ylabel(\"Median AA length\")\n",
        "                out_png = OUTPUTS_PATH / f\"entropy_{g}_{RUN_ID}.png\"\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(out_png)\n",
        "                plt.close()\n",
        "                n_plots += 1\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Plot failed for gene {g}: {e}\")\n",
        "\n",
        "        logger.info(f\"Entropy plots generated: {n_plots} (saved to {OUTPUTS_PATH})\")\n",
        "else:\n",
        "    logger.info(\"No wide_df available; skipping entropy.\")\n"
      ],
      "metadata": {
        "id": "8ATGhNvrNPmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 8: RegExTractor â€” Implementation (COL1A1 & COL1A2)**\n",
        "\n",
        "This section adds a selfâ€‘contained, nonâ€‘breaking implementation of the\n",
        "**RegExTractor** engine. It learns cladeâ€‘aware, exonâ€‘specific, tripletâ€‘aware\n",
        "regex patterns from your **wellâ€‘mapped** exon atlas and then rescues exons\n",
        "from **rejected** sequences, chaining hits into putative COL1A1/1A2 blocks.\n",
        "\n",
        "> Assumptions (explicit):\n",
        "> - You already have an exonâ€‘level training table with **one peptide per exon**\n",
        ">   (frame-correct, without stop codons) and taxonomic labels.\n",
        "> - The **rejected** pool is a table of full protein sequences with species/clade\n",
        ">   annotations.\n",
        "> - Evenâ€‘indexed exon numbering is used within the helix region.\n",
        "> - No function or variable from earlier parts is renamed. New utilities are\n",
        ">   prefixed with `rex_` or placed in `RegExTractor*` classes to avoid conflicts."
      ],
      "metadata": {
        "id": "GeAsS5TEU6dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 80 =====\n",
        "# RegExTractor configuration (Colab-friendly)\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Iterable, Any\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# User-tunable parameters (safe defaults). Adjust via Colab's #@param if desired.\n",
        "# --------------------------------------------------------------------------------------\n",
        "rex_min_clade_samples = 8                     #@param {type:\"integer\"}\n",
        "rex_anchor_triplets_min = 2                   #@param {type:\"integer\"}\n",
        "rex_anchor_triplets_max = 4                   #@param {type:\"integer\"}\n",
        "rex_anchor_entropy_max = 0.25                 #@param {type:\"number\"}\n",
        "rex_freq_threshold_strict = 0.05              #@param {type:\"number\"}\n",
        "rex_freq_threshold_moderate = 0.01            #@param {type:\"number\"}\n",
        "rex_ghead_density_min = 0.80                  #@param {type:\"number\"}\n",
        "rex_len_tolerance_strict = 1                  #@param {type:\"integer\"}\n",
        "rex_len_tolerance_moderate = 3                #@param {type:\"integer\"}\n",
        "rex_len_tolerance_loose = 6                   #@param {type:\"integer\"}\n",
        "rex_chain_min_consecutive = 3                 #@param {type:\"integer\"}\n",
        "rex_search_window_pad = 90                    #@param {type:\"integer\"}\n",
        "rex_enable_fullregex_fallback = True          #@param {type:\"boolean\"}\n",
        "\n",
        "# Taxonomic levels in descending specificity (must match your columns if available)\n",
        "REX_TAXON_LEVELS = [\"genus\", \"family\", \"order\", \"class\", \"kingdom\", \"pan\"]\n",
        "\n",
        "\n",
        "def rex_log(msg: str):\n",
        "    \"\"\"Lightweight logger (replace with proper logging if desired).\"\"\"\n",
        "    print(f\"[RegExTractor] {msg}\")\n"
      ],
      "metadata": {
        "id": "uuUIBIgHVFLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 81 â€“ Data adapters & expectations\n",
        "\n",
        "To avoid breaking existing code, we **adapt** whatever exon/rejected tables you\n",
        "already have into a minimal interface RegExTractor needs.\n",
        "\n",
        "**Training (mapped) table** must provide per row:\n",
        "\n",
        "- `gene_symbol` âˆˆ {COL1A1, COL1A2}\n",
        "- `exon_num_in_chain` (int; even indexing for helix exons)\n",
        "- `exon_peptide` (str; AA sequence of that exon)\n",
        "- Optional taxon columns: `genus`, `family`, `order`, `class`, `kingdom`\n",
        "\n",
        "**Rejected (unmapped) table** must provide:\n",
        "\n",
        "- `accession` (str)\n",
        "- `sequence` (str; full AA sequence)\n",
        "- Optional taxon columns as above\n",
        "\n",
        "If your column names differ, pass a mapping dict to the adapter helpers.\n"
      ],
      "metadata": {
        "id": "05BeAwYiVwTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 81 =====\n",
        "# Adapters to standardize input column names for RegExTractor\n",
        "\n",
        "@dataclass\n",
        "class RexColumnMap:\n",
        "    gene_col: str = \"gene_symbol\"\n",
        "    exon_col: str = \"exon_num_in_chain\"\n",
        "    pep_col: str = \"exon_peptide\"\n",
        "    seq_col: str = \"sequence\"\n",
        "    acc_col: str = \"accession\"\n",
        "    tax_cols: Dict[str, str] = field(default_factory=lambda: {\n",
        "        \"genus\": \"genus\",\n",
        "        \"family\": \"family\",\n",
        "        \"order\": \"order\",\n",
        "        \"class\": \"class\",\n",
        "        \"kingdom\": \"kingdom\"\n",
        "    })\n",
        "\n",
        "def rex_normalize_training_df(df: pd.DataFrame, cmap: RexColumnMap) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalize a training exon table into required columns.\n",
        "\n",
        "    Returns a new DataFrame with columns:\n",
        "        gene_symbol, exon_num_in_chain, exon_peptide, genus, family, order, class, kingdom\n",
        "    \"\"\"\n",
        "    needed = {cmap.gene_col, cmap.exon_col, cmap.pep_col}\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Training DF missing columns: {missing}\")\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"gene_symbol\": df[cmap.gene_col].astype(str),\n",
        "        \"exon_num_in_chain\": pd.to_numeric(df[cmap.exon_col], errors=\"coerce\").astype(\"Int64\"),\n",
        "        \"exon_peptide\": df[cmap.pep_col].astype(str)\n",
        "    })\n",
        "\n",
        "    for lvl in [\"genus\", \"family\", \"order\", \"class\", \"kingdom\"]:\n",
        "        col = cmap.tax_cols.get(lvl)\n",
        "        out[lvl] = df[col].astype(str) if (col in df.columns) else \"\"\n",
        "\n",
        "    # Drop rows with NA exon index or empty peptides\n",
        "    out = out.dropna(subset=[\"exon_num_in_chain\"])\n",
        "    out = out[out[\"exon_peptide\"].str.len() > 0].copy()\n",
        "    return out\n",
        "\n",
        "def rex_normalize_rejected_df(df: pd.DataFrame, cmap: RexColumnMap) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalize a rejected table into required columns.\n",
        "\n",
        "    Returns a new DataFrame with columns:\n",
        "        accession, sequence, genus, family, order, class, kingdom\n",
        "    \"\"\"\n",
        "    needed = {cmap.acc_col, cmap.seq_col}\n",
        "    missing = [c for c in needed if c not in df.columns]\n",
        "    if missing:\n",
        "        raise KeyError(f\"Rejected DF missing columns: {missing}\")\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"accession\": df[cmap.acc_col].astype(str),\n",
        "        \"sequence\": df[cmap.seq_col].astype(str)\n",
        "    })\n",
        "\n",
        "    for lvl in [\"genus\", \"family\", \"order\", \"class\", \"kingdom\"]:\n",
        "        col = cmap.tax_cols.get(lvl)\n",
        "        out[lvl] = df[col].astype(str) if (col in df.columns) else \"\"\n",
        "\n",
        "    # Remove empty sequences\n",
        "    out = out[out[\"sequence\"].str.len() > 0].copy()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "y5mmLfSTVy0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 82 â€“ Exon statistics & triplet utilities\n",
        "\n",
        "- Shannon entropy per aligned column.\n",
        "- Triplet grid helpers (G at head, X, Y positions).\n",
        "- Anchor window detection (lowâ€‘entropy, short motifs).\n"
      ],
      "metadata": {
        "id": "YKI_lOtkV2ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 82 =====\n",
        "# Exon statistics & triplet helpers\n",
        "\n",
        "def rex_shannon_entropy(chars: Iterable[str]) -> float:\n",
        "    arr = np.array(list(chars))\n",
        "    if arr.size == 0:\n",
        "        return 0.0\n",
        "    vals, cnts = np.unique(arr, return_counts=True)\n",
        "    p = cnts / cnts.sum()\n",
        "    p = np.clip(p, 1e-12, 1.0)\n",
        "    return float(-(p * np.log2(p)).sum())\n",
        "\n",
        "def rex_triplet_offset_best(seq: str) -> int:\n",
        "    \"\"\"\n",
        "    Choose 0/1/2 offset maximizing G at head positions (positions i where (i-offset)%3==0).\n",
        "    \"\"\"\n",
        "    best_off, best_cnt = 0, -1\n",
        "    for off in (0, 1, 2):\n",
        "        cnt = sum(1 for i,ch in enumerate(seq) if (i - off) % 3 == 0 and ch == \"G\")\n",
        "        if cnt > best_cnt:\n",
        "            best_cnt = cnt\n",
        "            best_off = off\n",
        "    return best_off\n",
        "\n",
        "def rex_ghead_density(seq: str, offset: Optional[int] = None) -> float:\n",
        "    \"\"\"\n",
        "    Fraction of triplets with G at head under given offset (or the best offset if None).\n",
        "    \"\"\"\n",
        "    if not seq:\n",
        "        return 0.0\n",
        "    off = rex_triplet_offset_best(seq) if offset is None else offset\n",
        "    triplets = max((len(seq) - off) // 3, 0)\n",
        "    if triplets == 0:\n",
        "        return 0.0\n",
        "    heads = [i for i in range(off, off + 3 * triplets, 3)]\n",
        "    g_cnt = sum(1 for i in heads if seq[i] == \"G\")\n",
        "    return g_cnt / len(heads) if heads else 0.0\n",
        "\n",
        "def rex_find_anchor_windows(\n",
        "    seqs: List[str],\n",
        "    k_min: int = 2,\n",
        "    k_max: int = 4,\n",
        "    entropy_max: float = 0.25\n",
        ") -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Find candidate anchor windows (start, end) in units of amino acids, not triplets.\n",
        "    We scan all sequences aligned by their best G-head offset and compute entropy per\n",
        "    triplet slot across the cohort. We return up to two non-overlapping low-entropy windows.\n",
        "\n",
        "    NOTE: This is alignment-light and relies on the collagen triplet periodicity.\n",
        "    \"\"\"\n",
        "    if not seqs:\n",
        "        return []\n",
        "\n",
        "    # Normalize offsets per sequence to maximize head-G alignment\n",
        "    offsets = [rex_triplet_offset_best(s) for s in seqs]\n",
        "    # Determine a nominal aligned region length (median len)\n",
        "    L = int(np.median([len(s) for s in seqs]))\n",
        "    # Build head/X/Y symbol matrices truncated to L\n",
        "    cols = {0: [], 1: [], 2: []}\n",
        "    for s, off in zip(seqs, offsets):\n",
        "        # pad to L with gaps to equalize\n",
        "        ss = (s + \"-\" * max(0, L - len(s)))[:L]\n",
        "        for i, ch in enumerate(ss):\n",
        "            cols[i % 3].append(ch)\n",
        "\n",
        "    # Compute per-position entropy across all sequences (but stratified by position mod 3)\n",
        "    ent = np.zeros(L, dtype=float)\n",
        "    for i in range(L):\n",
        "        ent[i] = rex_shannon_entropy([cols[i % 3][j] for j in range(len(seqs)) if i < len(seqs[j])])\n",
        "\n",
        "    # Slide over triplet windows to find low-entropy runs\n",
        "    candidates: List[Tuple[int, int, float]] = []\n",
        "    for k in range(k_min, k_max + 1):\n",
        "        w = 3 * k\n",
        "        for i in range(0, L - w + 1, 3):\n",
        "            e = float(ent[i:i + w].mean())\n",
        "            if e <= entropy_max:\n",
        "                candidates.append((i, i + w, e))\n",
        "\n",
        "    # Pick up to two non-overlapping windows with best (lowest) entropy, spread near ends\n",
        "    candidates.sort(key=lambda x: x[2])\n",
        "    anchors: List[Tuple[int, int]] = []\n",
        "    for s, e, _ in candidates:\n",
        "        if not anchors:\n",
        "            anchors.append((s, e))\n",
        "        elif len(anchors) == 1:\n",
        "            # Prefer a window far from the first anchor to frame the exon\n",
        "            s0, e0 = anchors[0]\n",
        "            if e <= s0 or s >= e0:  # non-overlap\n",
        "                anchors.append((s, e))\n",
        "                break\n",
        "    return sorted(anchors)\n"
      ],
      "metadata": {
        "id": "OcpJmDOzV2FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 83 â€“ Character classes & spacer summaries\n",
        "\n",
        "For each **spacer region** (between anchors, and the flanks), we summarize\n",
        "allowed residues per triplet position into compact character classes to use\n",
        "inside a repeating token: `(?:G[<Xclass>][<Yclass>]){m,n}`.\n"
      ],
      "metadata": {
        "id": "zvlxrZ0bV83j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 83 =====\n",
        "def rex_char_classes_for_region(\n",
        "    seqs: List[str],\n",
        "    start: int,\n",
        "    end: int,\n",
        "    freq_threshold: float = 0.05\n",
        ") -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Build character classes for X and Y positions over [start, end) region.\n",
        "    Returns (Xclass, Yclass) where each is either a single literal or a bracket class.\n",
        "    \"\"\"\n",
        "    if start >= end:\n",
        "        return \".\", \".\"\n",
        "    X_counts, Y_counts = {}, {}\n",
        "    for s in seqs:\n",
        "        # Guard: skip too-short sequences\n",
        "        if len(s) < end:\n",
        "            continue\n",
        "        for i in range(start, end, 3):\n",
        "            if i + 2 >= len(s):\n",
        "                break\n",
        "            x, y = s[i + 1], s[i + 2]\n",
        "            X_counts[x] = X_counts.get(x, 0) + 1\n",
        "            Y_counts[y] = Y_counts.get(y, 0) + 1\n",
        "\n",
        "    def build_class(d: Dict[str, int]) -> str:\n",
        "        if not d:\n",
        "            return \".\"\n",
        "        total = sum(d.values())\n",
        "        keep = sorted([aa for aa, c in d.items() if c / total >= freq_threshold])\n",
        "        if not keep:\n",
        "            # keep the top-1 if none pass threshold\n",
        "            keep = [max(d.items(), key=lambda kv: kv[1])[0]]\n",
        "        if len(keep) == 1:\n",
        "            return re.escape(keep[0])\n",
        "        return \"[\" + \"\".join(sorted(set(keep))) + \"]\"\n",
        "\n",
        "    return build_class(X_counts), build_class(Y_counts)\n"
      ],
      "metadata": {
        "id": "Mn4Lrwm9V8Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 84 â€“ Tiered pattern builder (per exon, per clade)\n",
        "\n",
        "We create a **stack** of patterns per exon:\n",
        "- **Tier A:** strict anchors + tight X/Y classes + narrow length tolerance\n",
        "- **Tier B:** same anchors + broader classes + moderate tolerance\n",
        "- **Tier C:** anchors + degenerate X/Y (`.`) + loose tolerance\n",
        "- **Tier D:** degenerate triplet token only (optional fallback)\n"
      ],
      "metadata": {
        "id": "TrbFsXXqWBC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 84 =====\n",
        "@dataclass\n",
        "class ExonStats:\n",
        "    exon_len_median: int\n",
        "    exon_len_q1: int\n",
        "    exon_len_q3: int\n",
        "    anchors: List[Tuple[int, int]]  # [(start,end)] AA indices within exon\n",
        "    ghead_density: float\n",
        "\n",
        "@dataclass\n",
        "class ExonTierPattern:\n",
        "    tier: str\n",
        "    regex: re.Pattern\n",
        "    anchors_literals: List[str]  # literal peptides of anchor windows\n",
        "    len_range: Tuple[int, int]\n",
        "    ghead_min: float\n",
        "\n",
        "@dataclass\n",
        "class ExonRegexLibraryEntry:\n",
        "    gene_symbol: str\n",
        "    exon_num_in_chain: int\n",
        "    clade_key: str\n",
        "    tiers: List[ExonTierPattern]\n",
        "\n",
        "class RegExTractorBuilder:\n",
        "    \"\"\"\n",
        "    Builds tiered regex patterns for one exon given training peptides and clade scope.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 anchor_k_min: int = rex_anchor_triplets_min,\n",
        "                 anchor_k_max: int = rex_anchor_triplets_max,\n",
        "                 anchor_entropy_max: float = rex_anchor_entropy_max):\n",
        "        self.kmin = anchor_k_min\n",
        "        self.kmax = anchor_k_max\n",
        "        self.entmax = anchor_entropy_max\n",
        "\n",
        "    @staticmethod\n",
        "    def _length_stats(peps: List[str]) -> Tuple[int, int, int]:\n",
        "        lengths = np.array([len(p) for p in peps if p], dtype=int)\n",
        "        if lengths.size == 0:\n",
        "            return 0, 0, 0\n",
        "        return int(np.median(lengths)), int(np.percentile(lengths, 25)), int(np.percentile(lengths, 75))\n",
        "\n",
        "    def build_stats(self, peps: List[str]) -> ExonStats:\n",
        "        if not peps:\n",
        "            return ExonStats(0, 0, 0, [], 0.0)\n",
        "        Lmed, Lq1, Lq3 = self._length_stats(peps)\n",
        "        anchors = rex_find_anchor_windows(\n",
        "            peps, k_min=self.kmin, k_max=self.kmax, entropy_max=self.entmax\n",
        "        )\n",
        "        # Compute average G-head density across peptides\n",
        "        dens = np.mean([rex_ghead_density(p) for p in peps]) if peps else 0.0\n",
        "        return ExonStats(Lmed, Lq1, Lq3, anchors, float(dens))\n",
        "\n",
        "    @staticmethod\n",
        "    def _literal(seq: str, start: int, end: int) -> str:\n",
        "        return seq[start:end]\n",
        "\n",
        "    def _anchor_literals(self, peps: List[str], anchors: List[Tuple[int, int]]) -> List[str]:\n",
        "        # Use the most common literal for each anchor window\n",
        "        lits = []\n",
        "        for (s, e) in anchors:\n",
        "            counter: Dict[str, int] = {}\n",
        "            for p in peps:\n",
        "                if len(p) < e:\n",
        "                    continue\n",
        "                frag = p[s:e]\n",
        "                counter[frag] = counter.get(frag, 0) + 1\n",
        "            if counter:\n",
        "                top = max(counter.items(), key=lambda kv: kv[1])[0]\n",
        "                lits.append(top)\n",
        "        return lits\n",
        "\n",
        "    def _spacer_token(self, Xclass: str, Yclass: str, rep_range: Tuple[int, int]) -> str:\n",
        "        m, n = rep_range\n",
        "        return f\"(?:G{Xclass}{Yclass})\" + (f\"{{{m},{n}}}\" if m != n else f\"{{{m}}}\")\n",
        "\n",
        "    def build_tiers(\n",
        "        self,\n",
        "        peps: List[str],\n",
        "        stats: ExonStats,\n",
        "        freq_strict: float = rex_freq_threshold_strict,\n",
        "        freq_moderate: float = rex_freq_threshold_moderate\n",
        "    ) -> List[ExonTierPattern]:\n",
        "        \"\"\"\n",
        "        Construct Tier A/B/C/D patterns. Returns compiled patterns with metadata.\n",
        "        \"\"\"\n",
        "        tiers: List[ExonTierPattern] = []\n",
        "        if not peps or stats.exon_len_median <= 0:\n",
        "            return tiers\n",
        "\n",
        "        Lmed, Lq1, Lq3 = stats.exon_len_median, stats.exon_len_q1, stats.exon_len_q3\n",
        "        # Anchor literals from cohort\n",
        "        anchor_lits = self._anchor_literals(peps, stats.anchors)\n",
        "\n",
        "        # Define spacer regions [by AA index] between anchors to summarize X/Y classes\n",
        "        # Regions: left flank, middle, right flank\n",
        "        regions: List[Tuple[int, int]] = []\n",
        "        if stats.anchors:\n",
        "            if len(stats.anchors) == 1:\n",
        "                (a1s, a1e) = stats.anchors[0]\n",
        "                regions = [(0, a1s), (a1e, Lmed)]\n",
        "            else:\n",
        "                (a1s, a1e), (a2s, a2e) = sorted(stats.anchors)\n",
        "                regions = [(0, a1s), (a1e, a2s), (a2e, Lmed)]\n",
        "        else:\n",
        "            regions = [(0, Lmed)]\n",
        "\n",
        "        # Summarize X/Y classes per region at two thresholds\n",
        "        xy_strict = [rex_char_classes_for_region(peps, s, e, freq_strict) for (s, e) in regions]\n",
        "        xy_mod = [rex_char_classes_for_region(peps, s, e, freq_moderate) for (s, e) in regions]\n",
        "\n",
        "        # Map region lengths (in triplets, rounded)\n",
        "        def triplet_len_range(s: int, e: int, tol: int) -> Tuple[int, int]:\n",
        "            L = max(e - s, 0)\n",
        "            m = max((L // 3) - tol, 0)\n",
        "            n = max((L // 3) + tol, 0)\n",
        "            if m > n:\n",
        "                m, n = n, m\n",
        "            return m, n\n",
        "\n",
        "        # Tiers A/B/C assembly\n",
        "        # Length tolerances around per-region triplet counts\n",
        "        strict_tol = rex_len_tolerance_strict\n",
        "        mod_tol = rex_len_tolerance_moderate\n",
        "        loose_tol = rex_len_tolerance_loose\n",
        "\n",
        "        def assemble(xy_classes, tol) -> str:\n",
        "            parts = []\n",
        "            # Left flank\n",
        "            if stats.anchors:\n",
        "                a1s, a1e = stats.anchors[0]\n",
        "                m, n = triplet_len_range(0, a1s, tol)\n",
        "                Xc, Yc = xy_classes[0]\n",
        "                if (a1s - 0) >= 3:\n",
        "                    parts.append(self._spacer_token(Xc, Yc, (m, n)))\n",
        "                parts.append(re.escape(anchor_lits[0])) if anchor_lits else None\n",
        "                if len(stats.anchors) == 2:\n",
        "                    (a2s, a2e) = stats.anchors[1]\n",
        "                    # Middle\n",
        "                    m, n = triplet_len_range(a1e, a2s, tol)\n",
        "                    Xc, Yc = xy_classes[1]\n",
        "                    if (a2s - a1e) >= 3:\n",
        "                        parts.append(self._spacer_token(Xc, Yc, (m, n)))\n",
        "                    parts.append(re.escape(anchor_lits[1])) if len(anchor_lits) > 1 else None\n",
        "                    # Right flank\n",
        "                    m, n = triplet_len_range(a2e, Lmed, tol)\n",
        "                    Xc, Yc = xy_classes[2]\n",
        "                    if (Lmed - a2e) >= 3:\n",
        "                        parts.append(self._spacer_token(Xc, Yc, (m, n)))\n",
        "                else:\n",
        "                    # Single anchor â†’ right flank only\n",
        "                    m, n = triplet_len_range(a1e, Lmed, tol)\n",
        "                    Xc, Yc = xy_classes[1]\n",
        "                    if (Lmed - a1e) >= 3:\n",
        "                        parts.append(self._spacer_token(Xc, Yc, (m, n)))\n",
        "            else:\n",
        "                # No anchors â†’ one spacer covering entire exon\n",
        "                Xc, Yc = xy_classes[0]\n",
        "                m, n = triplet_len_range(0, Lmed, tol)\n",
        "                parts.append(self._spacer_token(Xc, Yc, (m, n)))\n",
        "            return \"\".join(parts)\n",
        "\n",
        "        # Tier A (strict)\n",
        "        pat_A = assemble(xy_strict, strict_tol)\n",
        "        if pat_A:\n",
        "            tiers.append(ExonTierPattern(\n",
        "                tier=\"A\",\n",
        "                regex=re.compile(pat_A, flags=re.ASCII),\n",
        "                anchors_literals=anchor_lits,\n",
        "                len_range=(Lmed - strict_tol*3, Lmed + strict_tol*3),\n",
        "                ghead_min=max(rex_ghead_density_min, stats.ghead_density * 0.9)\n",
        "            ))\n",
        "\n",
        "        # Tier B (moderate)\n",
        "        pat_B = assemble(xy_mod, mod_tol)\n",
        "        if pat_B:\n",
        "            tiers.append(ExonTierPattern(\n",
        "                tier=\"B\",\n",
        "                regex=re.compile(pat_B, flags=re.ASCII),\n",
        "                anchors_literals=anchor_lits,\n",
        "                len_range=(Lmed - mod_tol*3, Lmed + mod_tol*3),\n",
        "                ghead_min=max(rex_ghead_density_min * 0.9, stats.ghead_density * 0.8)\n",
        "            ))\n",
        "\n",
        "        # Tier C (loose; anchors + generic triplet tokens)\n",
        "        # Replace classes with '.' and widen tolerances\n",
        "        xy_loose = [(\".\", \".\") for _ in regions]\n",
        "        pat_C = assemble(xy_loose, loose_tol)\n",
        "        if pat_C:\n",
        "            tiers.append(ExonTierPattern(\n",
        "                tier=\"C\",\n",
        "                regex=re.compile(pat_C, flags=re.ASCII),\n",
        "                anchors_literals=anchor_lits,\n",
        "                len_range=(Lmed - loose_tol*3, Lmed + loose_tol*3),\n",
        "                ghead_min=max(rex_ghead_density_min * 0.8, 0.65)\n",
        "            ))\n",
        "\n",
        "        # Tier D (degenerate fallback): (?:G..){kmin,kmax} without anchors\n",
        "        kmin = max((Lmed // 3) - loose_tol, 1)\n",
        "        kmax = (Lmed // 3) + loose_tol\n",
        "        pat_D = f\"(?:G..)\" + (f\"{{{kmin},{kmax}}}\" if kmin != kmax else f\"{{{kmin}}}\")\n",
        "        tiers.append(ExonTierPattern(\n",
        "            tier=\"D\",\n",
        "            regex=re.compile(pat_D, flags=re.ASCII),\n",
        "            anchors_literals=[],\n",
        "            len_range=(kmin*3, kmax*3),\n",
        "            ghead_min=0.65\n",
        "        ))\n",
        "        return tiers\n"
      ],
      "metadata": {
        "id": "Ymvl1dbxWCx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 85 â€“ Pattern library (per gene, exon, clade)\n",
        "\n",
        "We group training peptides by **clade level** with sufficient support and\n",
        "compile entries keyed as `(gene, exon, clade_key)`."
      ],
      "metadata": {
        "id": "xWJNPinKWJ5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 85 =====\n",
        "class RegExTractorLibrary:\n",
        "    \"\"\"\n",
        "    Holds compiled patterns across genes/exons/clades.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.entries: Dict[Tuple[str, int, str], ExonRegexLibraryEntry] = {}\n",
        "        self.stats: Dict[Tuple[str, int, str], ExonStats] = {}\n",
        "\n",
        "    @staticmethod\n",
        "    def _clade_keys(df: pd.DataFrame) -> List[str]:\n",
        "        # Build clade keys in descending specificity given data availability\n",
        "        keys = []\n",
        "        for lvl in REX_TAXON_LEVELS:\n",
        "            if lvl == \"pan\":\n",
        "                keys.append(\"pan\")\n",
        "            elif lvl in df.columns and df[lvl].astype(str).str.len().gt(0).any():\n",
        "                keys.extend(sorted(df[lvl].astype(str).unique()))\n",
        "        return list(dict.fromkeys(keys))  # stable unique\n",
        "\n",
        "    @staticmethod\n",
        "    def _subset_by_clade(df: pd.DataFrame, clade_key: str) -> pd.DataFrame:\n",
        "        if clade_key == \"pan\":\n",
        "            return df\n",
        "        for lvl in [\"genus\", \"family\", \"order\", \"class\", \"kingdom\"]:\n",
        "            if lvl in df.columns and clade_key in set(df[lvl].astype(str).unique()):\n",
        "                return df[df[lvl].astype(str) == clade_key]\n",
        "        # Not found â†’ empty\n",
        "        return df.iloc[0:0].copy()\n",
        "\n",
        "    def build(\n",
        "        self,\n",
        "        training_df: pd.DataFrame,\n",
        "        genes: List[str],\n",
        "        min_clade_samples: int = rex_min_clade_samples\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Build the tiered regex library from a normalized training dataframe.\n",
        "        \"\"\"\n",
        "        builder = RegExTractorBuilder()\n",
        "        for gene in genes:\n",
        "            gdf = training_df[training_df[\"gene_symbol\"] == gene]\n",
        "            if gdf.empty:\n",
        "                rex_log(f\"No training rows for {gene}\")\n",
        "                continue\n",
        "            for exon in sorted(gdf[\"exon_num_in_chain\"].dropna().astype(int).unique()):\n",
        "                edf = gdf[gdf[\"exon_num_in_chain\"] == exon]\n",
        "                if edf.empty:\n",
        "                    continue\n",
        "                clade_keys = self._clade_keys(edf)\n",
        "                # Always ensure 'pan' last\n",
        "                if \"pan\" not in clade_keys:\n",
        "                    clade_keys.append(\"pan\")\n",
        "                for ck in clade_keys:\n",
        "                    sub = self._subset_by_clade(edf, ck)\n",
        "                    if ck != \"pan\" and len(sub) < min_clade_samples:\n",
        "                        continue\n",
        "                    peps = sub[\"exon_peptide\"].astype(str).tolist()\n",
        "                    stats = builder.build_stats(peps)\n",
        "                    tiers = builder.build_tiers(peps, stats)\n",
        "                    if not tiers:\n",
        "                        continue\n",
        "                    key = (gene, exon, ck)\n",
        "                    self.entries[key] = ExonRegexLibraryEntry(\n",
        "                        gene_symbol=gene,\n",
        "                        exon_num_in_chain=exon,\n",
        "                        clade_key=ck,\n",
        "                        tiers=tiers\n",
        "                    )\n",
        "                    self.stats[key] = stats\n",
        "        rex_log(f\"Built {len(self.entries)} exon/clade entries across {len(genes)} genes.\")\n"
      ],
      "metadata": {
        "id": "o1a0QsH9WIBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 86 â€“ Matching & scoring\n",
        "\n",
        "- Twoâ€‘stage search: **anchorâ€‘first** (fast) then full regex in a local window.\n",
        "- Composite score combines tier, Gâ€‘head density, length plausibility, and anchors.\n"
      ],
      "metadata": {
        "id": "RjLErvJ9WN27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 86 =====\n",
        "@dataclass\n",
        "class RexHit:\n",
        "    accession: str\n",
        "    gene_symbol: str\n",
        "    exon_num_in_chain: int\n",
        "    clade_used: str\n",
        "    tier: str\n",
        "    start: int\n",
        "    end: int\n",
        "    matched_peptide: str\n",
        "    ghead_density: float\n",
        "    len_residual: int\n",
        "    anchors_hit: int\n",
        "    score: float\n",
        "\n",
        "class RegExTractorMatcher:\n",
        "    def __init__(self, library: RegExTractorLibrary):\n",
        "        self.lib = library\n",
        "\n",
        "    @staticmethod\n",
        "    def _tier_weight(tier: str) -> float:\n",
        "        return {\"A\": 1.0, \"B\": 0.85, \"C\": 0.6, \"D\": 0.3}.get(tier, 0.1)\n",
        "\n",
        "    @staticmethod\n",
        "    def _choose_clade_key(row: pd.Series, keys_for_exon: List[str]) -> str:\n",
        "        # Pick the most specific available clade key for this row; fallback to 'pan'\n",
        "        for lvl in REX_TAXON_LEVELS:\n",
        "            if lvl == \"pan\":\n",
        "                continue\n",
        "            if lvl in row.index and isinstance(row[lvl], str) and row[lvl]:\n",
        "                if row[lvl] in keys_for_exon:\n",
        "                    return row[lvl]\n",
        "        return \"pan\"\n",
        "\n",
        "    def _scan_with_anchors(\n",
        "        self, seq: str, pattern: ExonTierPattern, window_pad: int = rex_search_window_pad\n",
        "    ) -> List[Tuple[int, int, str]]:\n",
        "        \"\"\"\n",
        "        Return list of (start, end, match_text) using anchor priming to limit regex search.\n",
        "        \"\"\"\n",
        "        hits: List[Tuple[int, int, str]] = []\n",
        "        if pattern.anchors_literals:\n",
        "            # Find candidate windows around anchor occurrences\n",
        "            for lit in pattern.anchors_literals:\n",
        "                start_idx = 0\n",
        "                while True:\n",
        "                    idx = seq.find(lit, start_idx)\n",
        "                    if idx == -1:\n",
        "                        break\n",
        "                    wstart = max(0, idx - window_pad)\n",
        "                    wend = min(len(seq), idx + len(lit) + window_pad)\n",
        "                    m = pattern.regex.search(seq, wstart, wend)\n",
        "                    if m:\n",
        "                        hits.append((m.start(), m.end(), m.group(0)))\n",
        "                        # Advance past this match\n",
        "                        start_idx = m.end()\n",
        "                    else:\n",
        "                        start_idx = idx + 1\n",
        "        if (not hits) and rex_enable_fullregex_fallback:\n",
        "            for m in pattern.regex.finditer(seq):\n",
        "                hits.append((m.start(), m.end(), m.group(0)))\n",
        "        return hits\n",
        "\n",
        "    def _score_hit(self, pep: str, tier: ExonTierPattern) -> Tuple[float, float, int, int]:\n",
        "        gden = rex_ghead_density(pep)\n",
        "        len_res = min(abs(len(pep) - tier.len_range[0]), abs(len(pep) - tier.len_range[1]))\n",
        "        anchors_hit = 0\n",
        "        for lit in tier.anchors_literals:\n",
        "            if lit in pep:\n",
        "                anchors_hit += 1\n",
        "        # Composite score\n",
        "        score = (\n",
        "            0.45 * self._tier_weight(tier.tier) +\n",
        "            0.30 * max(0.0, (gden - (tier.ghead_min - 0.1)) / 0.2) +  # normalize in [0,1]\n",
        "            0.15 * (1.0 - min(len_res / 9.0, 1.0)) +\n",
        "            0.10 * (anchors_hit / max(len(tier.anchors_literals), 1))\n",
        "        )\n",
        "        return float(score), float(gden), int(len_res), int(anchors_hit)\n",
        "\n",
        "    def scan_sequence_row(\n",
        "        self,\n",
        "        row: pd.Series,\n",
        "        genes: List[str]\n",
        "    ) -> List[RexHit]:\n",
        "        \"\"\"\n",
        "        Scan a single rejected row for exon hits across the requested genes.\n",
        "        \"\"\"\n",
        "        seq = row[\"sequence\"]\n",
        "        accession = row[\"accession\"]\n",
        "        all_hits: List[RexHit] = []\n",
        "\n",
        "        for gene in genes:\n",
        "            # Gather available exons for this gene\n",
        "            entries = [(k, v) for k, v in self.lib.entries.items() if k[0] == gene]\n",
        "            exons = sorted(set([k[1] for k, _ in entries]))\n",
        "            for exon in exons:\n",
        "                # Find best clade key usable for this exon\n",
        "                keys_for_exon = [k[2] for k, _ in entries if k[1] == exon]\n",
        "                clade = self._choose_clade_key(row, keys_for_exon)\n",
        "                key = (gene, exon, clade) if (gene, exon, clade) in self.lib.entries else (gene, exon, \"pan\")\n",
        "                entry = self.lib.entries.get(key)\n",
        "                if not entry:\n",
        "                    continue\n",
        "                for tier in entry.tiers:\n",
        "                    # Anchor-primed search\n",
        "                    for s, e, mtxt in self._scan_with_anchors(seq, tier):\n",
        "                        score, gden, lres, ahit = self._score_hit(mtxt, tier)\n",
        "                        if gden < (tier.ghead_min - 0.05):\n",
        "                            continue\n",
        "                        all_hits.append(RexHit(\n",
        "                            accession=accession,\n",
        "                            gene_symbol=gene,\n",
        "                            exon_num_in_chain=exon,\n",
        "                            clade_used=entry.clade_key,\n",
        "                            tier=tier.tier,\n",
        "                            start=s,\n",
        "                            end=e,\n",
        "                            matched_peptide=mtxt,\n",
        "                            ghead_density=gden,\n",
        "                            len_residual=lres,\n",
        "                            anchors_hit=ahit,\n",
        "                            score=score\n",
        "                        ))\n",
        "                    # Early exit if we already found strong tier hits\n",
        "                    if any(h.tier == \"A\" and h.score >= 0.85 for h in all_hits):\n",
        "                        break\n",
        "        # Deduplicate overlapping hits by keeping the highest score per (gene, exon)\n",
        "        best: Dict[Tuple[str, int], RexHit] = {}\n",
        "        for h in sorted(all_hits, key=lambda x: x.score, reverse=True):\n",
        "            key = (h.gene_symbol, h.exon_num_in_chain)\n",
        "            if key not in best:\n",
        "                best[key] = h\n",
        "        return list(best.values())\n"
      ],
      "metadata": {
        "id": "NVvX6-RxWQce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 87 â€“ Chain reconstruction (anchorâ€‘andâ€‘walk)\n",
        "\n",
        "Given hits, we **seed** with the strongest, then walk downstream/upstream using\n",
        "expected exon order and nonâ€‘overlapping windows.\n"
      ],
      "metadata": {
        "id": "Hqf-By2iWT_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 87 =====\n",
        "@dataclass\n",
        "class RexChain:\n",
        "    accession: str\n",
        "    gene_symbol: str\n",
        "    seed_exon: int\n",
        "    exons: List[int]\n",
        "    coords: List[Tuple[int, int]]\n",
        "    tiers: List[str]\n",
        "    mean_score: float\n",
        "    consecutive_blocks: int\n",
        "\n",
        "def rex_exon_order_model(training_df: pd.DataFrame, gene: str) -> List[int]:\n",
        "    exons = (\n",
        "        training_df[training_df[\"gene_symbol\"] == gene][\"exon_num_in_chain\"]\n",
        "        .dropna().astype(int).sort_values().unique().tolist()\n",
        "    )\n",
        "    return exons\n",
        "\n",
        "def rex_walk_chain(\n",
        "    seq_len: int,\n",
        "    seed: RexHit,\n",
        "    hits_for_gene: Dict[int, RexHit],\n",
        "    exon_order: List[int],\n",
        "    window_pad: int = rex_search_window_pad\n",
        ") -> RexChain:\n",
        "    # Determine exon neighbors\n",
        "    if seed.exon_num_in_chain not in exon_order:\n",
        "        return RexChain(seed.accession, seed.gene_symbol, seed.exon_num_in_chain, [seed.exon_num_in_chain],\n",
        "                        [(seed.start, seed.end)], [seed.tier], seed.score, 1)\n",
        "\n",
        "    idx = exon_order.index(seed.exon_num_in_chain)\n",
        "    used = {seed.exon_num_in_chain}\n",
        "    exons = [seed.exon_num_in_chain]\n",
        "    coords = [(seed.start, seed.end)]\n",
        "    tiers = [seed.tier]\n",
        "\n",
        "    # Walk downstream\n",
        "    last_end = seed.end\n",
        "    consec = 1\n",
        "    for j in range(idx + 1, len(exon_order)):\n",
        "        ex = exon_order[j]\n",
        "        h = hits_for_gene.get(ex)\n",
        "        if not h:\n",
        "            break\n",
        "        # Require non-overlap and in-order progression within a window\n",
        "        if h.start >= last_end and h.start <= last_end + window_pad:\n",
        "            exons.append(ex)\n",
        "            coords.append((h.start, h.end))\n",
        "            tiers.append(h.tier)\n",
        "            last_end = h.end\n",
        "            consec += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    # Walk upstream\n",
        "    first_start = seed.start\n",
        "    for j in range(idx - 1, -1, -1):\n",
        "        ex = exon_order[j]\n",
        "        h = hits_for_gene.get(ex)\n",
        "        if not h:\n",
        "            break\n",
        "        if h.end <= first_start and h.end >= max(0, first_start - window_pad):\n",
        "            exons.insert(0, ex)\n",
        "            coords.insert(0, (h.start, h.end))\n",
        "            tiers.insert(0, h.tier)\n",
        "            first_start = h.start\n",
        "            consec += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    mean_score = float(np.mean([hits_for_gene[e].score if e in hits_for_gene else 0.0 for e in exons]))\n",
        "    return RexChain(seed.accession, seed.gene_symbol, seed.exon_num_in_chain, exons, coords, tiers, mean_score, consec)\n"
      ],
      "metadata": {
        "id": "72bQniiRWTuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 88 â€“ Orchestrator (build â†’ scan â†’ chain) + ledgers\n",
        "\n",
        "Produces:\n",
        "- `rex_hits_df`: perâ€‘exon best hit per gene\n",
        "- `rex_chains_df`: best chain per gene (seeded by top hit)\n"
      ],
      "metadata": {
        "id": "qS9JFC6pWYyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 88 =====\n",
        "def rex_run_regextractor(\n",
        "    training_df: pd.DataFrame,\n",
        "    rejected_df: pd.DataFrame,\n",
        "    genes: List[str] = GENE_SYMBOLS,\n",
        "    min_clade_samples: int = rex_min_clade_samples\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Full pipeline:\n",
        "      1) Build pattern library per gene/exon/clade\n",
        "      2) Scan rejected sequences â†’ hits\n",
        "      3) Build chains per sequence/gene using anchor-and-walk\n",
        "    \"\"\"\n",
        "    lib = RegExTractorLibrary()\n",
        "    lib.build(training_df, genes, min_clade_samples=min_clade_samples)\n",
        "    matcher = RegExTractorMatcher(lib)\n",
        "\n",
        "    hits_rows: List[Dict[str, Any]] = []\n",
        "    chains_rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Precompute exon order per gene\n",
        "    exon_orders = {g: rex_exon_order_model(training_df, g) for g in genes}\n",
        "\n",
        "    for _, row in rejected_df.iterrows():\n",
        "        seq_hits = matcher.scan_sequence_row(row, genes=genes)\n",
        "        if not seq_hits:\n",
        "            continue\n",
        "\n",
        "        # Record best hit per exon/gene\n",
        "        for h in seq_hits:\n",
        "            hits_rows.append({\n",
        "                \"accession\": h.accession,\n",
        "                \"gene_symbol\": h.gene_symbol,\n",
        "                \"exon_num_in_chain\": h.exon_num_in_chain,\n",
        "                \"clade_used\": h.clade_used,\n",
        "                \"tier\": h.tier,\n",
        "                \"start\": h.start,\n",
        "                \"end\": h.end,\n",
        "                \"length\": h.end - h.start,\n",
        "                \"matched_peptide\": h.matched_peptide,\n",
        "                \"ghead_density\": h.ghead_density,\n",
        "                \"len_residual\": h.len_residual,\n",
        "                \"anchors_hit\": h.anchors_hit,\n",
        "                \"score\": h.score\n",
        "            })\n",
        "\n",
        "        # Build chains for each gene separately, pick strongest seed\n",
        "        by_gene: Dict[str, List[RexHit]] = {}\n",
        "        for h in seq_hits:\n",
        "            by_gene.setdefault(h.gene_symbol, []).append(h)\n",
        "\n",
        "        for gene, ghits in by_gene.items():\n",
        "            # Index by exon for quick lookup\n",
        "            hits_by_exon = {h.exon_num_in_chain: h for h in ghits}\n",
        "            # Choose seed = highest-score hit\n",
        "            seed = sorted(ghits, key=lambda x: x.score, reverse=True)[0]\n",
        "            chain = rex_walk_chain(\n",
        "                seq_len=len(row[\"sequence\"]),\n",
        "                seed=seed,\n",
        "                hits_for_gene=hits_by_exon,\n",
        "                exon_order=exon_orders.get(gene, [])\n",
        "            )\n",
        "            chains_rows.append({\n",
        "                \"accession\": chain.accession,\n",
        "                \"gene_symbol\": chain.gene_symbol,\n",
        "                \"seed_exon\": chain.seed_exon,\n",
        "                \"exons\": chain.exons,\n",
        "                \"coords\": chain.coords,\n",
        "                \"tiers\": chain.tiers,\n",
        "                \"mean_score\": chain.mean_score,\n",
        "                \"consecutive_blocks\": chain.consecutive_blocks\n",
        "            })\n",
        "\n",
        "    rex_hits_df = pd.DataFrame(hits_rows)\n",
        "    rex_chains_df = pd.DataFrame(chains_rows)\n",
        "\n",
        "    # Optional: filter chains by minimal consecutive blocks\n",
        "    if not rex_chains_df.empty:\n",
        "        rex_chains_df = rex_chains_df[\n",
        "            rex_chains_df[\"consecutive_blocks\"].astype(int) >= rex_chain_min_consecutive\n",
        "        ].reset_index(drop=True)\n",
        "\n",
        "    rex_log(f\"Emitted {len(rex_hits_df)} exon hits and {len(rex_chains_df)} chains.\")\n",
        "    return rex_hits_df, rex_chains_df\n"
      ],
      "metadata": {
        "id": "KHc4Z61vWYfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 89 â€“ DNA rescue stub (optional integration point)\n",
        "\n",
        "After chains are placed, you can bracket a missing exon by two neighboring\n",
        "exons and apply a lightweight DNA translation scan. This cell is **scaffold**\n",
        "only â€” plug in your genome accessor when ready.\n"
      ],
      "metadata": {
        "id": "_GluznwoWdms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 89 =====\n",
        "def rex_dna_rescue_stub(\n",
        "    accession: str,\n",
        "    bracket_left: Tuple[int, int],\n",
        "    bracket_right: Tuple[int, int],\n",
        "    genome_accessor: Any\n",
        ") -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Placeholder for DNA rescue step (to be wired to your genome/FASTA accessor):\n",
        "\n",
        "    - Extract DNA between `bracket_left` and `bracket_right` genomic coordinates.\n",
        "    - Translate 3 frames on the coding strand.\n",
        "    - Pick frame with highest G-head density and suitable length.\n",
        "    - Return peptide if recovered, else None.\n",
        "\n",
        "    This function is NOT implemented here to keep this section dependency-free.\n",
        "    \"\"\"\n",
        "    _ = (accession, bracket_left, bracket_right, genome_accessor)\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "svRUktstWgnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90 â€“ How to call this in your notebook\n",
        "\n",
        "1) Prepare/normalize inputs:\n",
        "\n",
        "```python\n",
        "cmap = RexColumnMap(\n",
        "    gene_col=\"gene_symbol\",\n",
        "    exon_col=\"exon_num_in_chain\",\n",
        "    pep_col=\"exon_peptide\",\n",
        "    seq_col=\"sequence\",\n",
        "    acc_col=\"accession\",\n",
        "    tax_cols={\"genus\":\"genus\",\"family\":\"family\",\"order\":\"order\",\"class\":\"class\",\"kingdom\":\"kingdom\"}\n",
        ")\n",
        "\n",
        "training_norm = rex_normalize_training_df(mapped_exon_df, cmap)\n",
        "rejected_norm = rex_normalize_rejected_df(rejected_df, cmap)\n"
      ],
      "metadata": {
        "id": "KejKxbJHWkJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap = RexColumnMap(\n",
        "    gene_col=\"gene_symbol\",\n",
        "    exon_col=\"exon_num_in_chain\",\n",
        "    pep_col=\"exon_peptide\",\n",
        "    seq_col=\"sequence\",\n",
        "    acc_col=\"accession\",\n",
        "    tax_cols={\"genus\":\"genus\",\"family\":\"family\",\"order\":\"order\",\"class\":\"class\",\"kingdom\":\"kingdom\"}\n",
        ")\n",
        "\n",
        "training_norm = rex_normalize_training_df(mapped_exon_df, cmap)\n",
        "rejected_norm = rex_normalize_rejected_df(rejected_df, cmap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "KzcsQOqOXd3g",
        "outputId": "7cf84caf-d7ab-40e8-de01-4dfde049968c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'mapped_exon_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-652674383.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtraining_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrex_normalize_training_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped_exon_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mrejected_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrex_normalize_rejected_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrejected_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mapped_exon_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part-8-header"
      },
      "source": [
        "# **Part 9: Reproducibility & Manifest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cell-81-md"
      },
      "source": [
        "## Cell 91 â€“ Manifest writer\n",
        "\n",
        "Writes a plaintext manifest with core counts and SHA256 hashes of outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cell-81-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 91 =====\n",
        "# Manifest writer\n",
        "def sha256_file(p: Path) -> str:\n",
        "    if not p.exists(): return \"\"\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b''): h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "manifest_lines = [\n",
        "    f\"RUN_ID: {RUN_ID}\",\n",
        "    f\"TIME_UTC: {RUN_TIMESTAMP}\",\n",
        "    f\"WORKING_ROWS: {len(working_df) if 'working_df' in globals() else 0}\",\n",
        "    f\"CHAIN_ROWS: {len(chain_df) if 'chain_df' in globals() else 0}\",\n",
        "    f\"MAPPED_ROWS: {len(exon_df) if 'exon_df' in globals() else 0}\",\n",
        "    f\"CONSENSUS_ROWS: {len(consensus_long) if 'consensus_long' in globals() else 0}\",\n",
        "    f\"WIDE_ROWS: {len(wide_df) if 'wide_df' in globals() else 0}\",\n",
        "    \"FILES:\"\n",
        "]\n",
        "for p in [WORKING_SNAPSHOT, REJECTED_SNAPSHOT, MAPPED_SNAPSHOT,\n",
        "          CONSENSUS_LONG_SNAPSHOT, WIDE_ARCH_SNAPSHOT, ENTROPY_TSV, RESCUE_LOG_TSV]:\n",
        "    manifest_lines.append(f\" - {p.name}: {sha256_file(p)}\")\n",
        "\n",
        "with open(MANIFEST_PATH, 'w') as f:\n",
        "    f.write(\"\\n\".join(manifest_lines))\n",
        "logger.info(f\"Manifest written: {MANIFEST_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "D0gie3LIUy1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}