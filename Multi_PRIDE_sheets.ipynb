{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyMeq/kbyNI/+YKRZ2bjFxKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PRIDE/blob/main/Multi_PRIDE_sheets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PRIDE Files Download Program\n",
        "\n",
        "This Colab notebook provides an automated solution for downloading proteomics data files from the PRIDE (PRoteomics IDEntifications) database using the [pridepy](https://github.com/PRIDE-Archive/pridepy) package.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Retrieves PRIDE project IDs from a specified Google Sheet\n",
        "- Downloads essential proteomics files (.fasta, .mgf) and README files\n",
        "- Supports multiple download protocols (aspera, ftp, globus)\n",
        "- Organizes downloads in a structured directory hierarchy\n",
        "- Integrates with Google Drive for storage\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Google Colab environment\n",
        "- Access to Google Drive\n",
        "- Google Sheets containing PRIDE project IDs\n",
        "- `pridepy` package (automatically installed by the notebook)\n",
        "\n",
        "## Configuration Parameters\n",
        "\n",
        "The following parameters can be configured in the notebook:\n",
        "\n",
        "- `sheet_name`: Name of the worksheet containing PRIDE IDs\n",
        "- `repository`: Repository name (currently set to 'PRIDE')\n",
        "- `file_types`: File types to download (default: 'mgf')\n",
        "- `download_raw`: Boolean flag for downloading RAW files\n",
        "- `protocol`: Download protocol ('aspera', 'ftp', or 'globus')\n",
        "- `folder_name`: Name of the folder where files will be stored\n",
        "- `spreadsheet_id`: Google Sheets ID containing PRIDE project IDs\n",
        "- `shared_drive_base_dir_str`: Base directory path in Google Drive\n",
        "\n",
        "## Google Sheet Structure\n",
        "\n",
        "The program expects a Google Sheet with:\n",
        "- PRIDE project IDs in column A\n",
        "- Data starting from row 2 (row 1 assumed to be headers)\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Open the notebook in Google Colab\n",
        "2. Mount your Google Drive\n",
        "3. Configure the parameters as needed\n",
        "4. Run all cells\n",
        "\n",
        "The program will:\n",
        "1. Authenticate and access Google Drive\n",
        "2. Create necessary directories\n",
        "3. Retrieve PRIDE IDs from the specified Google Sheet\n",
        "4. Download files for each PRIDE project\n",
        "5. Organize files in project-specific folders\n",
        "\n",
        "## File Selection\n",
        "\n",
        "The program automatically downloads:\n",
        "- `.fasta` files\n",
        "- `.mgf` files\n",
        "- README files\n",
        "- `.raw` files (optional, controlled by `download_raw` parameter)\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "Files are organized in the following structure:\n",
        "Use code with caution.\n",
        "Python\n",
        "shared_drive_base_dir/\n",
        "└── folder_name/\n",
        "├── PRIDE_ID_1/\n",
        "│ ├── file1.mgf\n",
        "│ ├── file2.fasta\n",
        "│ └── readme.txt\n",
        "└── PRIDE_ID_2/\n",
        "├── file1.mgf\n",
        "└── file2.fasta\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The program includes error handling for:\n",
        "- Google Sheets API errors\n",
        "- File download failures\n",
        "- JSON parsing errors\n",
        "- Directory creation issues\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "- `pridepy`\n",
        "- `google.colab`\n",
        "- `googleapiclient`\n",
        "- `pathlib`\n",
        "- `subprocess`\n",
        "- `json`\n",
        "\n",
        "## Notes\n",
        "\n",
        "- The program uses the `pridepy` command-line interface for file downloads\n",
        "- Progress and errors are logged to the notebook output\n",
        "- Failed downloads are reported but don't stop the entire process\n",
        "- Existing files may be overwritten\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "q-lmpiYQaqk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download pridepy\n",
        "!pip install --upgrade pridepy tqdm\n",
        "\n",
        "\"\"\"To learn more about pridepy\"\"\"\n",
        "\n",
        "# !pridepy --help\n",
        "# !pridepy stream-files-metadata --help\n",
        "# !pridepy --help | grep download\n",
        "# !pip install --upgrade pridepy tqdm"
      ],
      "metadata": {
        "id": "G3I5MY3SbDrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# --- Module Parameters ---\n",
        "sheet_name = 'PX Hominins'  # @param {type:\"string\"}\n",
        "repository = 'PRIDE'  # @param {type:\"string\"}\n",
        "file_types = 'mgf, fasta, txt, raw'  # @param {type:\"string\"}\n",
        "protocol = 'aspera'  # @param ['aspera', 'ftp', 'globus']\n",
        "folder_name = 'Hominins'  # @param {type:\"string\"}\n",
        "shared_drive_base_dir_str = \"/content/drive/Shareddrives/MS_data/PRIDE\"  # @param {type:\"string\"}\n",
        "spreadsheet_id = 'put the ID of your spreadhsheet here'  # @param {type:\"string\"}\n",
        "\n",
        "# --- Configure Logging ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ],
      "metadata": {
        "id": "J2eWVoekQBRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"Data class for file information.\"\"\"\n",
        "    filename: str\n",
        "    size: int\n",
        "    project_id: str\n",
        "    file_type: str\n",
        "\n",
        "    @property\n",
        "    def size_in_gb(self) -> float:\n",
        "        return self.size / 1e9\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], project_id: str) -> 'FileInfo':\n",
        "        \"\"\"Create FileInfo instance from PRIDE metadata dictionary.\"\"\"\n",
        "        # Try different size fields that might exist in PRIDE metadata\n",
        "        size_fields = ['fileSize', 'publicFileSize', 'fileSizeBytes']\n",
        "        file_size = 0\n",
        "        for field in size_fields:\n",
        "            if field in data and data[field]:\n",
        "                try:\n",
        "                    # Some fields might store size as string\n",
        "                    file_size = int(str(data[field]).replace(',', ''))\n",
        "                    break\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        return cls(\n",
        "            filename=data[\"fileName\"],\n",
        "            size=file_size,\n",
        "            project_id=project_id,\n",
        "            file_type=Path(data[\"fileName\"]).suffix.lower()[1:]\n",
        "        )\n",
        "\n",
        "def get_pride_ids_from_sheet(spreadsheet_id: str, sheet_name: str) -> list:\n",
        "    \"\"\"Retrieves PRIDE IDs from Google Sheet.\"\"\"\n",
        "    try:\n",
        "        service = build('sheets', 'v4')\n",
        "        sheet = service.spreadsheets()\n",
        "        range_name = f\"{sheet_name}!A2:A\"\n",
        "        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
        "        values = result.get('values', [])\n",
        "\n",
        "        if not values:\n",
        "            logger.warning('No data found in the Google Sheet.')\n",
        "            return []\n",
        "\n",
        "        pride_ids = [row[0] for row in values if row]\n",
        "        logger.info(f\"Retrieved {len(pride_ids)} PRIDE IDs\")\n",
        "        for pride_id in pride_ids:\n",
        "            logger.info(f\"Found PRIDE ID: {pride_id}\")\n",
        "        return pride_ids\n",
        "\n",
        "    except HttpError as err:\n",
        "        logger.error(f\"Failed to retrieve data from Google Sheets: {err}\")\n",
        "        return []\n",
        "\n",
        "def get_project_files(project_id: str, download_dir: Path) -> List[FileInfo]:\n",
        "    \"\"\"Get list of files available for a PRIDE project.\"\"\"\n",
        "    project_dir = download_dir / project_id\n",
        "    project_dir.mkdir(parents=True, exist_ok=True)\n",
        "    metadata_file = project_dir / f\"{project_id}_metadata.json\"\n",
        "\n",
        "    command = [\n",
        "        'pridepy',\n",
        "        'stream-files-metadata',\n",
        "        '-a', project_id,\n",
        "        '-o', str(metadata_file)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command,\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        logger.info(f\"Retrieved metadata for project {project_id}\")\n",
        "\n",
        "        with open(metadata_file, \"r\") as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        file_infos = []\n",
        "        for data in metadata:\n",
        "            file_info = FileInfo.from_dict(data, project_id)\n",
        "            file_infos.append(file_info)\n",
        "            logger.info(f\"Found file: {file_info.filename} ({file_info.size_in_gb:.2f} GB)\")\n",
        "\n",
        "        return file_infos\n",
        "\n",
        "    except (subprocess.CalledProcessError, json.JSONDecodeError, IOError) as err:\n",
        "        logger.error(f\"Failed to get files for project {project_id}: {err}\")\n",
        "        return []\n",
        "\n",
        "def download_file(file_info: FileInfo, output_dir: Path, protocol: str = 'aspera') -> bool:\n",
        "    \"\"\"Download a single file using pridepy.\"\"\"\n",
        "    project_dir = output_dir / file_info.project_id\n",
        "    project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    command = [\n",
        "        'pridepy',\n",
        "        'download-file-by-name',\n",
        "        '-a', file_info.project_id,\n",
        "        '-f', file_info.filename,\n",
        "        '-o', str(project_dir),\n",
        "        '-p', protocol\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Downloading {file_info.filename} with {protocol}\")\n",
        "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        logger.info(f\"Successfully downloaded {file_info.filename}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(f\"Failed to download {file_info.filename} with {protocol}: {e.stderr}\")\n",
        "        # Try FTP if Aspera fails\n",
        "        if protocol == 'aspera':\n",
        "            logger.info(f\"Retrying with FTP: {file_info.filename}\")\n",
        "            return download_file(file_info, output_dir, 'ftp')\n",
        "        return False\n",
        "\n",
        "def download_files_by_type(pride_id: str, file_type: str, all_files: Dict[str, List[FileInfo]],\n",
        "                          output_dir: Path, protocol: str = 'aspera'):\n",
        "    \"\"\"Download all files of a specific type for a PRIDE project.\"\"\"\n",
        "    if pride_id not in all_files:\n",
        "        logger.error(f\"No files found for {pride_id}\")\n",
        "        return\n",
        "\n",
        "    files = all_files[pride_id]\n",
        "    files_to_download = [f for f in files if f.file_type == file_type]\n",
        "\n",
        "    if not files_to_download:\n",
        "        logger.warning(f\"No {file_type} files found for {pride_id}\")\n",
        "        return\n",
        "\n",
        "    logger.info(f\"Starting download of {len(files_to_download)} {file_type} files for {pride_id}\")\n",
        "\n",
        "    successful = 0\n",
        "    for file_info in files_to_download:\n",
        "        if download_file(file_info, output_dir, protocol):\n",
        "            successful += 1\n",
        "\n",
        "    logger.info(f\"Successfully downloaded {successful}/{len(files_to_download)} {file_type} files for {pride_id}\")\n",
        "\n",
        "def group_files_by_type(files: List[FileInfo]) -> Dict[str, List[FileInfo]]:\n",
        "    \"\"\"Group files by their type.\"\"\"\n",
        "    grouped = {}\n",
        "    for file in files:\n",
        "        if file.file_type not in grouped:\n",
        "            grouped[file.file_type] = []\n",
        "        grouped[file.file_type].append(file)\n",
        "    return grouped\n",
        "\n",
        "def print_file_summary(pride_id: str, files: List[FileInfo]):\n",
        "    \"\"\"Print summary of available files for a PRIDE project.\"\"\"\n",
        "    print(f\"\\nFiles available for {pride_id}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if not files:\n",
        "        print(\"No files found\")\n",
        "        return\n",
        "\n",
        "    grouped_files = group_files_by_type(files)\n",
        "\n",
        "    for file_type, file_list in sorted(grouped_files.items()):\n",
        "        total_size = sum(f.size_in_gb for f in file_list)\n",
        "        print(f\"\\nFile type: .{file_type}\")\n",
        "        print(f\"Number of files: {len(file_list)}\")\n",
        "        print(f\"Total size: {total_size:.2f} GB\")\n",
        "        print(\"\\nFiles:\")\n",
        "        for f in sorted(file_list, key=lambda x: x.filename):\n",
        "            print(f\"- {f.filename} ({f.size_in_gb:.2f} GB)\")\n",
        "\n",
        "def save_file_summary(pride_id: str, files: List[FileInfo], summary_dir: Path):\n",
        "    \"\"\"Save file summary to a text file.\"\"\"\n",
        "    summary_path = summary_dir / pride_id / \"available_files_summary.txt\"\n",
        "    summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    grouped_files = group_files_by_type(files)\n",
        "\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        f.write(f\"Files available for {pride_id}:\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "        if not files:\n",
        "            f.write(\"\\nNo files found\\n\")\n",
        "            return\n",
        "\n",
        "        for file_type, file_list in sorted(grouped_files.items()):\n",
        "            total_size = sum(f.size_in_gb for f in file_list)\n",
        "            f.write(f\"\\nFile type: .{file_type}\\n\")\n",
        "            f.write(f\"Number of files: {len(file_list)}\\n\")\n",
        "            f.write(f\"Total size: {total_size:.2f} GB\\n\")\n",
        "            f.write(\"\\nFiles:\\n\")\n",
        "            for file_info in sorted(file_list, key=lambda x: x.filename):\n",
        "                f.write(f\"- {file_info.filename} ({file_info.size_in_gb:.2f} GB)\\n\")\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U72Gb7jEPtNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------Main\n",
        "def main():\n",
        "    \"\"\"Main execution function with verification and download steps.\"\"\"\n",
        "    try:\n",
        "        # Get PRIDE IDs\n",
        "        pride_ids = get_pride_ids_from_sheet(spreadsheet_id, sheet_name)\n",
        "        if not pride_ids:\n",
        "            logger.error(\"No PRIDE IDs found\")\n",
        "            return\n",
        "\n",
        "        # Setup directories\n",
        "        base_dir = Path(shared_drive_base_dir_str)\n",
        "        download_dir = base_dir / folder_name\n",
        "        download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # First list all available files\n",
        "        print(\"\\nChecking available files for each PRIDE project...\")\n",
        "        all_files = {}\n",
        "\n",
        "        for pride_id in pride_ids:\n",
        "            files = get_project_files(pride_id, download_dir)\n",
        "            all_files[pride_id] = files\n",
        "            print_file_summary(pride_id, files)\n",
        "            save_file_summary(pride_id, files, download_dir)\n",
        "\n",
        "        # Print overall summary\n",
        "        print(\"\\nOverall Summary:\")\n",
        "        print(\"=\" * 50)\n",
        "        for pride_id, files in all_files.items():\n",
        "            grouped = group_files_by_type(files)\n",
        "            print(f\"\\n{pride_id}:\")\n",
        "            for file_type, file_list in sorted(grouped.items()):\n",
        "                print(f\"  .{file_type}: {len(file_list)} files\")\n",
        "\n",
        "        # Add download functionality\n",
        "        print(\"\\nWould you like to proceed with downloads? (y/n)\")\n",
        "        response = input().lower()\n",
        "        if response == 'y':\n",
        "            print(\"\\nSelect file type to download:\")\n",
        "            available_types = set()\n",
        "            for files in all_files.values():\n",
        "                for file in files:\n",
        "                    available_types.add(file.file_type)\n",
        "\n",
        "            print(\"Available file types:\", \", \".join(sorted(available_types)))\n",
        "            file_type = input(\"Enter file type (without dot): \").lower()\n",
        "\n",
        "            if file_type not in available_types:\n",
        "                logger.error(f\"Invalid file type. Must be one of: {', '.join(sorted(available_types))}\")\n",
        "                return\n",
        "\n",
        "            print(f\"\\nDownloading {file_type} files using {protocol} protocol...\")\n",
        "            for pride_id in pride_ids:\n",
        "                download_files_by_type(pride_id, file_type, all_files, download_dir, protocol)\n",
        "\n",
        "        print(\"\\nFile summaries have been saved. You can now proceed with downloads.\")\n",
        "        print(f\"Current file types to download: {file_types}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Process failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "8NtWYU_BQMy_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}