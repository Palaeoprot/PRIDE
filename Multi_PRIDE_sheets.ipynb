{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PRIDE/blob/main/Multi_PRIDE_sheets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lmpiYQaqk2"
      },
      "source": [
        "#PRIDE Files Download Program\n",
        "\n",
        "This Colab notebook provides an automated solution for downloading proteomics data files from the PRIDE (PRoteomics IDEntifications) database using the [pridepy](https://github.com/PRIDE-Archive/pridepy) package.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Retrieves PRIDE project IDs from a specified Google Sheet\n",
        "- Downloads essential proteomics files (.fasta, .mgf) and README files\n",
        "- Supports multiple download protocols (aspera, ftp, globus)\n",
        "- Organizes downloads in a structured directory hierarchy\n",
        "- Integrates with Google Drive for storage\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Google Colab environment\n",
        "- Access to Google Drive\n",
        "- Google Sheets containing PRIDE project IDs\n",
        "- `pridepy` package (automatically installed by the notebook)\n",
        "\n",
        "## Configuration Parameters\n",
        "\n",
        "The following parameters can be configured in the notebook:\n",
        "\n",
        "- `sheet_name`: Name of the worksheet containing PRIDE IDs\n",
        "- `repository`: Repository name (currently set to 'PRIDE')\n",
        "- `file_types`: File types to download (default: 'mgf')\n",
        "- `download_raw`: Boolean flag for downloading RAW files\n",
        "- `protocol`: Download protocol ('aspera', 'ftp', or 'globus')\n",
        "- `folder_name`: Name of the folder where files will be stored\n",
        "- `spreadsheet_id`: Google Sheets ID containing PRIDE project IDs\n",
        "- `shared_drive_base_dir_str`: Base directory path in Google Drive\n",
        "\n",
        "## Google Sheet Structure\n",
        "\n",
        "The program expects a Google Sheet with:\n",
        "- PRIDE project IDs in column A\n",
        "- Data starting from row 2 (row 1 assumed to be headers)\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Open the notebook in Google Colab\n",
        "2. Mount your Google Drive\n",
        "3. Configure the parameters as needed\n",
        "4. Run all cells\n",
        "\n",
        "The program will:\n",
        "1. Authenticate and access Google Drive\n",
        "2. Create necessary directories\n",
        "3. Retrieve PRIDE IDs from the specified Google Sheet\n",
        "4. Download files for each PRIDE project\n",
        "5. Organize files in project-specific folders\n",
        "\n",
        "## File Selection\n",
        "\n",
        "The program automatically downloads:\n",
        "- `.fasta` files\n",
        "- `.mgf` files\n",
        "- README files\n",
        "- `.raw` files (optional, controlled by `download_raw` parameter)\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "Files are organized in the following structure:\n",
        "Use code with caution.\n",
        "Python\n",
        "shared_drive_base_dir/\n",
        "└── folder_name/\n",
        "├── PRIDE_ID_1/\n",
        "│ ├── file1.mgf\n",
        "│ ├── file2.fasta\n",
        "│ └── readme.txt\n",
        "└── PRIDE_ID_2/\n",
        "├── file1.mgf\n",
        "└── file2.fasta\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The program includes error handling for:\n",
        "- Google Sheets API errors\n",
        "- File download failures\n",
        "- JSON parsing errors\n",
        "- Directory creation issues\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "- `pridepy`\n",
        "- `google.colab`\n",
        "- `googleapiclient`\n",
        "- `pathlib`\n",
        "- `subprocess`\n",
        "- `json`\n",
        "\n",
        "## Notes\n",
        "\n",
        "- The program uses the `pridepy` command-line interface for file downloads\n",
        "- Progress and errors are logged to the notebook output\n",
        "- Failed downloads are reported but don't stop the entire process\n",
        "- Existing files may be overwritten\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "G3I5MY3SbDrG",
        "outputId": "2723f500-161f-4fa0-b381-a8ad98dd0c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pridepy\n",
            "  Downloading pridepy-0.0.7-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting boto3<2.0.0,>=1.34.0 (from pridepy)\n",
            "  Downloading boto3-1.36.25-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting botocore<2.0.0,>=1.34.0 (from pridepy)\n",
            "  Downloading botocore-1.36.25-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from pridepy) (8.1.8)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from pridepy)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.18.0 in /usr/local/lib/python3.11/dist-packages (from pridepy) (5.24.1)\n",
            "Collecting ratelimit<3.0.0,>=2.2.1 (from pridepy)\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from pridepy) (2.32.3)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->pridepy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3<2.0.0,>=1.34.0->pridepy)\n",
            "  Downloading s3transfer-0.11.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.34.0->pridepy) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<2.0.0,>=1.34.0->pridepy) (2.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->pridepy) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->pridepy) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->pridepy) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->pridepy) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->pridepy) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->pridepy) (0.14.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.18.0->pridepy) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.18.0->pridepy) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->pridepy) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.34.0->pridepy) (1.17.0)\n",
            "Downloading pridepy-0.0.7-py3-none-any.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.36.25-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.36.25-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.11.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ratelimit\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5895 sha256=f9e953d6f349ec522ceb5b5542a69f559d2369ff7272868fd2c18ffeddde6b80\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/d5/e5/8fbffe089140fb498987b7709becf861086daace105d243475\n",
            "Successfully built ratelimit\n",
            "Installing collected packages: ratelimit, jmespath, httpx, botocore, s3transfer, boto3, pridepy\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "Successfully installed boto3-1.36.25 botocore-1.36.25 httpx-0.27.2 jmespath-1.0.1 pridepy-0.0.7 ratelimit-2.2.1 s3transfer-0.11.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'To learn more about pridepy'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#download pridepy\n",
        "!pip install --upgrade pridepy tqdm\n",
        "\n",
        "\"\"\"To learn more about pridepy\"\"\"\n",
        "\n",
        "# !pridepy --help\n",
        "# !pridepy stream-files-metadata --help\n",
        "# !pridepy --help | grep download\n",
        "# !pip install --upgrade pridepy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2eWVoekQBRk",
        "outputId": "2e44051f-67e2-451a-f460-016824316333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import auth, drive\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import subprocess\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# --- Module Parameters ---\n",
        "sheet_name = 'PX Hominins'  # @param {type:\"string\"}\n",
        "repository = 'PRIDE'  # @param {type:\"string\"}\n",
        "file_types = 'mgf, fasta, txt, raw'  # @param {type:\"string\"}\n",
        "protocol = 'aspera'  # @param ['aspera', 'ftp', 'globus']\n",
        "folder_name = 'Hominins'  # @param {type:\"string\"}\n",
        "shared_drive_base_dir_str = \"/content/drive/Shareddrives/ZooMS_Data/PRIDE\"  # @param {type:\"string\"}\n",
        "spreadsheet_id = '127K6zdl5y46DRqUwRr-V32nUDoceaddbhG9XyozJs-4'  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Authenticate ---\n",
        "auth.authenticate_user()\n",
        "\n",
        "# --- Configure Logging ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @dataclass\n",
        "# class FileInfo:\n",
        "#     \"\"\"Data class for file information.\"\"\"\n",
        "#     filename: str\n",
        "#     size: int\n",
        "#     project_id: str\n",
        "#     file_type: str\n",
        "#     category: str = \"OTHER\"  # Default category if none specified\n",
        "\n",
        "#     @property\n",
        "#     def size_in_gb(self) -> float:\n",
        "#         \"\"\"Return file size in gigabytes.\"\"\"\n",
        "#         return self.size / 1e9\n",
        "\n",
        "#     @classmethod\n",
        "#     def from_dict(cls, data: Dict[str, Any], project_id: str) -> 'FileInfo':\n",
        "#         \"\"\"Create FileInfo instance from PRIDE metadata dictionary.\"\"\"\n",
        "#         # Try different size fields that might exist in PRIDE metadata\n",
        "#         size_fields = ['fileSize', 'publicFileSize', 'fileSizeBytes']\n",
        "#         file_size = 0\n",
        "#         for field in size_fields:\n",
        "#             if field in data and data[field]:\n",
        "#                 try:\n",
        "#                     file_size = int(str(data[field]).replace(',', ''))\n",
        "#                     break\n",
        "#                 except (ValueError, TypeError):\n",
        "#                     continue\n",
        "\n",
        "#         # Determine file category based on extension\n",
        "#         filename = data[\"fileName\"]\n",
        "#         file_ext = Path(filename).suffix.lower()\n",
        "\n",
        "#         # Map file extensions to PRIDE categories\n",
        "#         category_map = {\n",
        "#             '.raw': 'RAW',\n",
        "#             '.wiff': 'RAW',\n",
        "#             '.d': 'RAW',\n",
        "#             '.mgf': 'PEAK',\n",
        "#             '.mzml': 'PEAK',\n",
        "#             '.mzidentml': 'RESULT',\n",
        "#             '.mztab': 'RESULT',\n",
        "#             '.fasta': 'FASTA',\n",
        "#             '.txt': 'OTHER',\n",
        "#             '.pdf': 'OTHER'\n",
        "#         }\n",
        "\n",
        "#         category = category_map.get(file_ext, 'OTHER')\n",
        "\n",
        "#         # Special case for README files\n",
        "#         if 'readme' in filename.lower():\n",
        "#             category = 'OTHER'\n",
        "\n",
        "#         return cls(\n",
        "#             filename=filename,\n",
        "#             size=file_size,\n",
        "#             project_id=project_id,\n",
        "#             file_type=file_ext[1:] if file_ext else '',  # Remove the dot from extension\n",
        "#             category=category\n",
        "#         )"
      ],
      "metadata": {
        "id": "pRp-CAsgr2Iw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"Data class for file information.\"\"\"\n",
        "    filename: str\n",
        "    size: int\n",
        "    project_id: str\n",
        "    file_type: str\n",
        "    category: str = \"OTHER\"  # Default category if none specified\n",
        "\n",
        "    @property\n",
        "    def size_in_gb(self) -> float:\n",
        "        \"\"\"Return file size in gigabytes.\"\"\"\n",
        "        return self.size / 1e9\n",
        "\n",
        "    @property\n",
        "    def size_in_mb(self) -> float:\n",
        "        \"\"\"Return file size in megabytes.\"\"\"\n",
        "        return self.size / (1024 * 1024)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], project_id: str) -> 'FileInfo':\n",
        "        \"\"\"Create FileInfo instance from PRIDE metadata dictionary.\"\"\"\n",
        "        # Try different size fields that might exist in PRIDE metadata\n",
        "        size_fields = ['fileSize', 'publicFileSize', 'fileSizeBytes']\n",
        "        file_size = 0\n",
        "        for field in size_fields:\n",
        "            if field in data and data[field]:\n",
        "                try:\n",
        "                    file_size = int(str(data[field]).replace(',', ''))\n",
        "                    break\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        # Determine file category based on extension\n",
        "        filename = data[\"fileName\"]\n",
        "        file_ext = Path(filename).suffix.lower()\n",
        "\n",
        "        # Map file extensions to PRIDE categories\n",
        "        category_map = {\n",
        "            '.raw': 'RAW',\n",
        "            '.wiff': 'RAW',\n",
        "            '.d': 'RAW',\n",
        "            '.mgf': 'PEAK',\n",
        "            '.mzml': 'PEAK',\n",
        "            '.mzidentml': 'RESULT',\n",
        "            '.mztab': 'RESULT',\n",
        "            '.fasta': 'FASTA',\n",
        "            '.txt': 'OTHER',\n",
        "            '.pdf': 'OTHER'\n",
        "        }\n",
        "\n",
        "        category = category_map.get(file_ext, 'OTHER')\n",
        "\n",
        "        # Special case for README files\n",
        "        if 'readme' in filename.lower():\n",
        "            category = 'OTHER'\n",
        "\n",
        "        return cls(\n",
        "            filename=filename,\n",
        "            size=file_size,\n",
        "            project_id=project_id,\n",
        "            file_type=file_ext[1:] if file_ext else '',  # Remove the dot from extension\n",
        "            category=category\n",
        "        )\n",
        "\n",
        "def should_download_file(file_info: FileInfo, download_large_text_files: bool = False) -> bool:\n",
        "    \"\"\"\n",
        "    Determine if a file should be downloaded based on its type and size.\n",
        "\n",
        "    Args:\n",
        "        file_info: FileInfo object containing file metadata\n",
        "        download_large_text_files: Flag to control downloading of large text files\n",
        "\n",
        "    Returns:\n",
        "        bool: True if file should be downloaded, False otherwise\n",
        "    \"\"\"\n",
        "    # Always download non-text files\n",
        "    if not file_info.filename.lower().endswith('.txt'):\n",
        "        return True\n",
        "\n",
        "    # For text files, check size limit unless override is set\n",
        "    if not download_large_text_files and file_info.size_in_mb > 1:\n",
        "        logger.info(f\"Skipping large text file: {file_info.filename} ({file_info.size_in_mb:.2f} MB)\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def download_file(file_info: FileInfo, output_dir: Path, protocol: str = 'aspera',\n",
        "                 download_large_text_files: bool = False) -> bool:\n",
        "    \"\"\"Download a single file using pridepy with size limit checks.\"\"\"\n",
        "    # First check if we should download this file\n",
        "    if not should_download_file(file_info, download_large_text_files):\n",
        "        logger.info(f\"Skipping {file_info.filename} due to size restrictions\")\n",
        "        return False\n",
        "\n",
        "    # Create category-based subdirectory\n",
        "    project_dir = output_dir / file_info.project_id / file_info.category\n",
        "    project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if file already exists\n",
        "    file_path = project_dir / file_info.filename\n",
        "    if file_path.exists():\n",
        "        logger.info(f\"Skipping {file_info.filename}, already present in {project_dir}\")\n",
        "        print(f\"[SKIP] {file_info.filename} (Already exists)\")\n",
        "        return True\n",
        "\n",
        "    command = [\n",
        "        'pridepy',\n",
        "        'download-file-by-name',\n",
        "        '-a', file_info.project_id,\n",
        "        '-f', file_info.filename,\n",
        "        '-o', str(project_dir),\n",
        "        '-p', protocol\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(f\"[DOWNLOADING] {file_info.filename} ...\")\n",
        "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        print(f\"[SUCCESS] {file_info.filename}\")\n",
        "        logger.info(f\"Successfully downloaded {file_info.filename}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"[FAILED] {file_info.filename} (Error: {e.stderr})\")\n",
        "        logger.error(f\"Failed to download {file_info.filename} with {protocol}: {e.stderr}\")\n",
        "\n",
        "        # Try FTP if Aspera fails\n",
        "        if protocol == 'aspera':\n",
        "            logger.info(f\"Retrying with FTP: {file_info.filename}\")\n",
        "            return download_file(file_info, output_dir, 'ftp', download_large_text_files)\n",
        "        return False"
      ],
      "metadata": {
        "id": "lribTsXGvQYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @dataclass\n",
        "# class FileInfo:\n",
        "#     \"\"\"Data class for file information.\"\"\"\n",
        "#     filename: str\n",
        "#     size: int\n",
        "#     project_id: str\n",
        "#     file_type: str\n",
        "\n",
        "#     @property\n",
        "#     def size_in_gb(self) -> float:\n",
        "#         return self.size / 1e9\n",
        "\n",
        "#     @classmethod\n",
        "#     def from_dict(cls, data: Dict[str, Any], project_id: str) -> 'FileInfo':\n",
        "#         \"\"\"Create FileInfo instance from PRIDE metadata dictionary.\"\"\"\n",
        "#         # Try different size fields that might exist in PRIDE metadata\n",
        "#         size_fields = ['fileSize', 'publicFileSize', 'fileSizeBytes']\n",
        "#         file_size = 0\n",
        "#         for field in size_fields:\n",
        "#             if field in data and data[field]:\n",
        "#                 try:\n",
        "#                     # Some fields might store size as string\n",
        "#                     file_size = int(str(data[field]).replace(',', ''))\n",
        "#                     break\n",
        "#                 except (ValueError, TypeError):\n",
        "#                     continue\n",
        "\n",
        "#         return cls(\n",
        "#             filename=data[\"fileName\"],\n",
        "#             size=file_size,\n",
        "#             project_id=project_id,\n",
        "#             file_type=Path(data[\"fileName\"]).suffix.lower()[1:]\n",
        "#         )\n"
      ],
      "metadata": {
        "id": "Fbx_Q3KanXti"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "U72Gb7jEPtNd"
      },
      "outputs": [],
      "source": [
        "# #----Functions\n",
        "\n",
        "# def get_pride_ids_from_sheet(spreadsheet_id: str, sheet_name: str) -> list:\n",
        "#     \"\"\"Retrieves PRIDE IDs from Google Sheet.\"\"\"\n",
        "#     try:\n",
        "#         service = build('sheets', 'v4')\n",
        "#         sheet = service.spreadsheets()\n",
        "#         range_name = f\"{sheet_name}!A2:A\"\n",
        "#         result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
        "#         values = result.get('values', [])\n",
        "\n",
        "#         if not values:\n",
        "#             logger.warning('No data found in the Google Sheet.')\n",
        "#             return []\n",
        "\n",
        "#         pride_ids = [row[0] for row in values if row]\n",
        "#         logger.info(f\"Retrieved {len(pride_ids)} PRIDE IDs\")\n",
        "#         for pride_id in pride_ids:\n",
        "#             logger.info(f\"Found PRIDE ID: {pride_id}\")\n",
        "#         return pride_ids\n",
        "\n",
        "#     except HttpError as err:\n",
        "#         logger.error(f\"Failed to retrieve data from Google Sheets: {err}\")\n",
        "#         return []\n",
        "\n",
        "# # def get_project_files(project_id: str, download_dir: Path) -> List[FileInfo]:\n",
        "# #     \"\"\"Get list of files available for a PRIDE project.\"\"\"\n",
        "# #     project_dir = download_dir / project_id\n",
        "# #     project_dir.mkdir(parents=True, exist_ok=True)\n",
        "# #     metadata_file = project_dir / f\"{project_id}_metadata.json\"\n",
        "\n",
        "# #     command = [\n",
        "# #         'pridepy',\n",
        "# #         'stream-files-metadata',\n",
        "# #         '-a', project_id,\n",
        "# #         '-o', str(metadata_file)\n",
        "# #     ]\n",
        "\n",
        "# #     try:\n",
        "# #         result = subprocess.run(\n",
        "# #             command,\n",
        "# #             check=True,\n",
        "# #             stdout=subprocess.PIPE,\n",
        "# #             stderr=subprocess.PIPE,\n",
        "# #             text=True\n",
        "# #         )\n",
        "# #         logger.info(f\"Retrieved metadata for project {project_id}\")\n",
        "\n",
        "# #         with open(metadata_file, \"r\") as f:\n",
        "# #             metadata = json.load(f)\n",
        "\n",
        "# #         file_infos = []\n",
        "# #         for data in metadata:\n",
        "# #             file_info = FileInfo.from_dict(data, project_id)\n",
        "# #             file_infos.append(file_info)\n",
        "# #             logger.info(f\"Found file: {file_info.filename} ({file_info.size_in_gb:.2f} GB)\")\n",
        "\n",
        "# #         return file_infos\n",
        "\n",
        "# #     except (subprocess.CalledProcessError, json.JSONDecodeError, IOError) as err:\n",
        "# #         logger.error(f\"Failed to get files for project {project_id}: {err}\")\n",
        "# #         return []\n",
        "\n",
        "# # def download_file(file_info: FileInfo, output_dir: Path, protocol: str = 'aspera') -> bool:\n",
        "# #     \"\"\"Download a single file using pridepy if it is not already present.\"\"\"\n",
        "\n",
        "# #     project_dir = output_dir / file_info.project_id\n",
        "# #     project_dir.mkdir(parents=True, exist_ok=True)\n",
        "# #     file_path = project_dir / file_info.filename\n",
        "\n",
        "# #     # Check if file already exists\n",
        "# #     if file_path.exists():\n",
        "# #         logger.info(f\"Skipping {file_info.filename}, already present in {project_dir}\")\n",
        "# #         print(f\"[SKIP] {file_info.filename} (Already exists)\")\n",
        "# #         return True  # Treat as successful since the file is already there\n",
        "\n",
        "# #     command = [\n",
        "# #         'pridepy',\n",
        "# #         'download-file-by-name',\n",
        "# #         '-a', file_info.project_id,\n",
        "# #         '-f', file_info.filename,\n",
        "# #         '-o', str(project_dir),\n",
        "# #         '-p', protocol\n",
        "# #     ]\n",
        "\n",
        "# #     try:\n",
        "# #         print(f\"[DOWNLOADING] {file_info.filename} ...\")\n",
        "# #         result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "# #         print(f\"[SUCCESS] {file_info.filename}\")\n",
        "# #         logger.info(f\"Successfully downloaded {file_info.filename}\")\n",
        "# #         return True\n",
        "# #     except subprocess.CalledProcessError as e:\n",
        "# #         print(f\"[FAILED] {file_info.filename} (Error: {e.stderr})\")\n",
        "# #         logger.error(f\"Failed to download {file_info.filename} with {protocol}: {e.stderr}\")\n",
        "\n",
        "# #         # Try FTP if Aspera fails\n",
        "# #         if protocol == 'aspera':\n",
        "# #             logger.info(f\"Retrying with FTP: {file_info.filename}\")\n",
        "# #             return download_file(file_info, output_dir, 'ftp')\n",
        "# #         return False\n",
        "\n",
        "# # def download_files_by_type(pride_id: str, file_type: str, all_files: Dict[str, List[FileInfo]],\n",
        "# #                           output_dir: Path, protocol: str = 'aspera'):\n",
        "# #     \"\"\"Download all files of a specific type for a PRIDE project.\"\"\"\n",
        "# #     if pride_id not in all_files:\n",
        "# #         logger.error(f\"No files found for {pride_id}\")\n",
        "# #         return\n",
        "\n",
        "# #     files = all_files[pride_id]\n",
        "# #     files_to_download = [f for f in files if f.file_type == file_type]\n",
        "\n",
        "# #     if not files_to_download:\n",
        "# #         logger.warning(f\"No {file_type} files found for {pride_id}\")\n",
        "# #         return\n",
        "\n",
        "# #     logger.info(f\"Starting download of {len(files_to_download)} {file_type} files for {pride_id}\")\n",
        "\n",
        "# #     successful = 0\n",
        "# #     for file_info in files_to_download:\n",
        "# #         if download_file(file_info, output_dir, protocol):\n",
        "# #             successful += 1\n",
        "\n",
        "# #     logger.info(f\"Successfully downloaded {successful}/{len(files_to_download)} {file_type} files for {pride_id}\")\n",
        "\n",
        "\n",
        "# def get_project_files(project_id: str, download_dir: Path) -> List[FileInfo]:\n",
        "#     \"\"\"Get list of files available for a PRIDE project with category tracking.\"\"\"\n",
        "#     project_dir = download_dir / project_id\n",
        "#     project_dir.mkdir(parents=True, exist_ok=True)\n",
        "#     metadata_file = project_dir / f\"{project_id}_metadata.json\"\n",
        "\n",
        "#     command = [\n",
        "#         'pridepy',\n",
        "#         'stream-files-metadata',\n",
        "#         '-a', project_id,\n",
        "#         '-o', str(metadata_file)\n",
        "#     ]\n",
        "\n",
        "#     try:\n",
        "#         result = subprocess.run(\n",
        "#             command,\n",
        "#             check=True,\n",
        "#             stdout=subprocess.PIPE,\n",
        "#             stderr=subprocess.PIPE,\n",
        "#             text=True\n",
        "#         )\n",
        "#         logger.info(f\"Retrieved metadata for project {project_id}\")\n",
        "\n",
        "#         with open(metadata_file, \"r\") as f:\n",
        "#             metadata = json.load(f)\n",
        "\n",
        "#         file_infos = []\n",
        "#         category_counts = {}\n",
        "\n",
        "#         for data in metadata:\n",
        "#             try:\n",
        "#                 file_info = FileInfo.from_dict(data, project_id)\n",
        "#                 file_infos.append(file_info)\n",
        "\n",
        "#                 # Track file categories\n",
        "#                 if file_info.category not in category_counts:\n",
        "#                     category_counts[file_info.category] = 0\n",
        "#                 category_counts[file_info.category] += 1\n",
        "\n",
        "#                 logger.info(f\"Found {file_info.category} file: {file_info.filename} ({file_info.size_in_gb:.2f} GB)\")\n",
        "#             except Exception as e:\n",
        "#                 logger.error(f\"Error processing file metadata: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#         # Log category summary\n",
        "#         if file_infos:\n",
        "#             logger.info(f\"\\nFiles found by category for {project_id}:\")\n",
        "#             for cat, count in sorted(category_counts.items()):\n",
        "#                 logger.info(f\"- {cat}: {count} files\")\n",
        "#         else:\n",
        "#             logger.warning(f\"No valid files found for project {project_id}\")\n",
        "\n",
        "#         return file_infos\n",
        "\n",
        "#     except Exception as err:\n",
        "#         logger.error(f\"Failed to get files for project {project_id}: {err}\")\n",
        "#         return []\n",
        "\n",
        "# def download_file(file_info: FileInfo, output_dir: Path, protocol: str = 'aspera') -> bool:\n",
        "#     \"\"\"Download a single file using pridepy with better organization.\"\"\"\n",
        "#     # Create category-based subdirectory\n",
        "#     project_dir = output_dir / file_info.project_id / file_info.category\n",
        "#     project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     # Check if file already exists\n",
        "#     file_path = project_dir / file_info.filename\n",
        "#     if file_path.exists():\n",
        "#         logger.info(f\"Skipping {file_info.filename}, already present in {project_dir}\")\n",
        "#         print(f\"[SKIP] {file_info.filename} (Already exists)\")\n",
        "#         return True\n",
        "\n",
        "#     command = [\n",
        "#         'pridepy',\n",
        "#         'download-file-by-name',\n",
        "#         '-a', file_info.project_id,\n",
        "#         '-f', file_info.filename,\n",
        "#         '-o', str(project_dir),\n",
        "#         '-p', protocol\n",
        "#     ]\n",
        "\n",
        "#     try:\n",
        "#         print(f\"[DOWNLOADING] {file_info.filename} ...\")\n",
        "#         result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "#         print(f\"[SUCCESS] {file_info.filename}\")\n",
        "#         logger.info(f\"Successfully downloaded {file_info.filename}\")\n",
        "#         return True\n",
        "#     except subprocess.CalledProcessError as e:\n",
        "#         print(f\"[FAILED] {file_info.filename} (Error: {e.stderr})\")\n",
        "#         logger.error(f\"Failed to download {file_info.filename} with {protocol}: {e.stderr}\")\n",
        "\n",
        "#         # Try FTP if Aspera fails\n",
        "#         if protocol == 'aspera':\n",
        "#             logger.info(f\"Retrying with FTP: {file_info.filename}\")\n",
        "#             return download_file(file_info, output_dir, 'ftp')\n",
        "#         return False\n",
        "\n",
        "\n",
        "def group_files_by_type(files: List[FileInfo]) -> Dict[str, List[FileInfo]]:\n",
        "    \"\"\"Group files by their type.\"\"\"\n",
        "    grouped = {}\n",
        "    for file in files:\n",
        "        if file.file_type not in grouped:\n",
        "            grouped[file.file_type] = []\n",
        "        grouped[file.file_type].append(file)\n",
        "    return grouped\n",
        "\n",
        "# def print_file_summary(pride_id: str, files: List[FileInfo]):\n",
        "#     \"\"\"Print summary of available files for a PRIDE project.\"\"\"\n",
        "#     print(f\"\\nFiles available for {pride_id}:\")\n",
        "#     print(\"=\" * 50)\n",
        "\n",
        "#     if not files:\n",
        "#         print(\"No files found\")\n",
        "#         return\n",
        "\n",
        "#     grouped_files = group_files_by_type(files)\n",
        "\n",
        "#     for file_type, file_list in sorted(grouped_files.items()):\n",
        "#         total_size = sum(f.size_in_gb for f in file_list)\n",
        "#         print(f\"\\nFile type: .{file_type}\")\n",
        "#         print(f\"Number of files: {len(file_list)}\")\n",
        "#         print(f\"Total size: {total_size:.2f} GB\")\n",
        "#         print(\"\\nFiles:\")\n",
        "#         for f in sorted(file_list, key=lambda x: x.filename):\n",
        "#             print(f\"- {f.filename} ({f.size_in_gb:.2f} GB)\")\n",
        "\n",
        "# def save_file_summary(pride_id: str, files: List[FileInfo], summary_dir: Path):\n",
        "#     \"\"\"Save file summary to a text file.\"\"\"\n",
        "#     summary_path = summary_dir / pride_id / \"available_files_summary.txt\"\n",
        "#     summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#     grouped_files = group_files_by_type(files)\n",
        "\n",
        "#     with open(summary_path, \"w\") as f:\n",
        "#         f.write(f\"Files available for {pride_id}:\\n\")\n",
        "#         f.write(\"=\" * 50 + \"\\n\")\n",
        "\n",
        "#         if not files:\n",
        "#             f.write(\"\\nNo files found\\n\")\n",
        "#             return\n",
        "\n",
        "#         for file_type, file_list in sorted(grouped_files.items()):\n",
        "#             total_size = sum(f.size_in_gb for f in file_list)\n",
        "#             f.write(f\"\\nFile type: .{file_type}\\n\")\n",
        "#             f.write(f\"Number of files: {len(file_list)}\\n\")\n",
        "#             f.write(f\"Total size: {total_size:.2f} GB\\n\")\n",
        "#             f.write(\"\\nFiles:\\n\")\n",
        "#             for file_info in sorted(file_list, key=lambda x: x.filename):\n",
        "#                 f.write(f\"- {file_info.filename} ({file_info.size_in_gb:.2f} GB)\\n\")\n",
        "\n",
        "def print_file_summary(pride_id: str, files: List[FileInfo]):\n",
        "    \"\"\"Print summary of available files for a PRIDE project.\"\"\"\n",
        "    print(f\"\\nFiles available for {pride_id}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if not files:\n",
        "        print(\"No files found\")\n",
        "        return\n",
        "\n",
        "    # Group files by extension\n",
        "    grouped_files = {}\n",
        "    for f in files:\n",
        "        ext = Path(f.filename).suffix.lower()\n",
        "        if ext not in grouped_files:\n",
        "            grouped_files[ext] = []\n",
        "        grouped_files[ext].append(f)\n",
        "\n",
        "    for ext, file_list in sorted(grouped_files.items()):\n",
        "        total_size = sum(f.size_in_gb for f in file_list)\n",
        "        print(f\"\\nFile type: {ext}\")\n",
        "        print(f\"Number of files: {len(file_list)}\")\n",
        "        print(f\"Total size: {total_size:.2f} GB\")\n",
        "        print(\"\\nFiles:\")\n",
        "        for file_info in sorted(file_list, key=lambda x: x.filename):\n",
        "            print(f\"- {file_info.filename} ({file_info.size_in_gb:.2f} GB)\")\n",
        "\n",
        "def save_file_summary(pride_id: str, files: List[FileInfo], summary_dir: Path):\n",
        "    \"\"\"Save file summary to a text file.\"\"\"\n",
        "    summary_path = summary_dir / pride_id / \"available_files_summary.txt\"\n",
        "    summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Group files by extension\n",
        "    grouped_files = {}\n",
        "    for f in files:\n",
        "        ext = Path(f.filename).suffix.lower()\n",
        "        if ext not in grouped_files:\n",
        "            grouped_files[ext] = []\n",
        "        grouped_files[ext].append(f)\n",
        "\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        f.write(f\"Files available for {pride_id}:\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        if not files:\n",
        "            f.write(\"No files found\\n\")\n",
        "            return\n",
        "\n",
        "        for ext, file_list in sorted(grouped_files.items()):\n",
        "            total_size = sum(f.size_in_gb for f in file_list)\n",
        "            f.write(f\"\\nFile type: {ext}\\n\")\n",
        "            f.write(f\"Number of files: {len(file_list)}\\n\")\n",
        "            f.write(f\"Total size: {total_size:.2f} GB\\n\")\n",
        "            f.write(\"\\nFiles:\\n\")\n",
        "            for file_info in sorted(file_list, key=lambda x: x.filename):\n",
        "                f.write(f\"- {file_info.filename} ({file_info.size_in_gb:.2f} GB)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_large_text_files = False  # @param {type:\"boolean\"}\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with enhanced download options and size limits.\"\"\"\n",
        "    try:\n",
        "        # Get PRIDE IDs and check files\n",
        "        pride_ids = get_pride_ids_from_sheet(spreadsheet_id, sheet_name)\n",
        "        if not pride_ids:\n",
        "            logger.error(\"No PRIDE IDs found\")\n",
        "            return\n",
        "\n",
        "        # Setup directories\n",
        "        base_dir = Path(shared_drive_base_dir_str)\n",
        "        download_dir = base_dir / folder_name\n",
        "        download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # First list all available files\n",
        "        print(\"\\nChecking available files for each PRIDE project...\")\n",
        "        all_files = {}\n",
        "        downloaded_types = set()  # Track which file types have been downloaded\n",
        "\n",
        "        for pride_id in pride_ids:\n",
        "            files = get_project_files(pride_id, download_dir)\n",
        "            all_files[pride_id] = files\n",
        "            print_file_summary(pride_id, files)\n",
        "            save_file_summary(pride_id, files, download_dir)\n",
        "\n",
        "        while True:  # Continue until user is done\n",
        "            # Get all available file types that haven't been downloaded\n",
        "            available_types = set()\n",
        "            for files in all_files.values():\n",
        "                for file in files:\n",
        "                    ext = Path(file.filename).suffix.lower()[1:]  # Get extension without dot\n",
        "                    if ext and ext not in downloaded_types:  # Only add if extension exists\n",
        "                        available_types.add(ext)\n",
        "\n",
        "            if not available_types:\n",
        "                print(\"\\nAll file types have been downloaded!\")\n",
        "                break\n",
        "\n",
        "            # Print remaining file types with size information for text files\n",
        "            print(\"\\nFile types not yet downloaded:\")\n",
        "            for ext in sorted(available_types):\n",
        "                file_count = 0\n",
        "                total_size_mb = 0\n",
        "                skipped_count = 0\n",
        "                for files in all_files.values():\n",
        "                    for file in files:\n",
        "                        if Path(file.filename).suffix.lower()[1:] == ext:\n",
        "                            if ext == 'txt' and not download_large_text_files and file.size_in_mb > 1:\n",
        "                                skipped_count += 1\n",
        "                            else:\n",
        "                                file_count += 1\n",
        "                                total_size_mb += file.size_in_mb\n",
        "\n",
        "                print(f\"- {ext}: {file_count} files ({total_size_mb:.2f} MB total)\")\n",
        "                if ext == 'txt' and skipped_count > 0:\n",
        "                    print(f\"  Note: {skipped_count} text files > 1MB will be skipped\")\n",
        "\n",
        "            # Ask if user wants to download more files\n",
        "            print(\"\\nWould you like to download additional file types? (y/n)\")\n",
        "            response = input().lower()\n",
        "            if response != 'y':\n",
        "                break\n",
        "\n",
        "            # Get file type selection\n",
        "            print(\"\\nSelect file type to download:\")\n",
        "            print(\"Available types:\", \", \".join(sorted(available_types)))\n",
        "            file_type = input(\"Enter file type (without dot): \").lower()\n",
        "\n",
        "            if file_type not in available_types:\n",
        "                logger.error(f\"Invalid file type. Must be one of: {', '.join(sorted(available_types))}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nDownloading {file_type} files using {protocol} protocol...\")\n",
        "            for pride_id, files in all_files.items():\n",
        "                # Filter files by type and download\n",
        "                type_files = [f for f in files if Path(f.filename).suffix.lower()[1:] == file_type]\n",
        "                for file in type_files:\n",
        "                    download_file(file, download_dir, protocol, download_large_text_files)\n",
        "\n",
        "            # Add to downloaded types\n",
        "            downloaded_types.add(file_type)\n",
        "\n",
        "            # Show progress\n",
        "            remaining = len(available_types) - len(downloaded_types)\n",
        "            print(f\"\\nProgress: {len(downloaded_types)} file types downloaded, {remaining} remaining\")\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\nDownload session complete!\")\n",
        "        print(\"Downloaded file types:\", \", \".join(sorted(downloaded_types)))\n",
        "        if available_types - downloaded_types:\n",
        "            print(\"Remaining file types:\", \", \".join(sorted(available_types - downloaded_types)))\n",
        "\n",
        "        # Print size limit information\n",
        "        if not download_large_text_files:\n",
        "            print(\"\\nNote: Text files larger than 1MB were skipped. Set download_large_text_files = True to download all text files.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Process failed: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "q1eg657YvsCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "#     \"\"\"Main execution function with enhanced download options.\"\"\"\n",
        "#     try:\n",
        "#         # Get PRIDE IDs and check files\n",
        "#         pride_ids = get_pride_ids_from_sheet(spreadsheet_id, sheet_name)\n",
        "#         if not pride_ids:\n",
        "#             logger.error(\"No PRIDE IDs found\")\n",
        "#             return\n",
        "\n",
        "#         # Setup directories\n",
        "#         base_dir = Path(shared_drive_base_dir_str)\n",
        "#         download_dir = base_dir / folder_name\n",
        "#         download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # First list all available files\n",
        "#         print(\"\\nChecking available files for each PRIDE project...\")\n",
        "#         all_files = {}\n",
        "#         downloaded_types = set()  # Track which file types have been downloaded\n",
        "\n",
        "#         for pride_id in pride_ids:\n",
        "#             files = get_project_files(pride_id, download_dir)\n",
        "#             all_files[pride_id] = files\n",
        "#             print_file_summary(pride_id, files)\n",
        "#             save_file_summary(pride_id, files, download_dir)\n",
        "\n",
        "#         while True:  # Continue until user is done\n",
        "#             # Get all available file types that haven't been downloaded\n",
        "#             available_types = set()\n",
        "#             for files in all_files.values():\n",
        "#                 for file in files:\n",
        "#                     ext = Path(file.filename).suffix.lower()[1:]  # Get extension without dot\n",
        "#                     if ext and ext not in downloaded_types:  # Only add if extension exists\n",
        "#                         available_types.add(ext)\n",
        "\n",
        "#             if not available_types:\n",
        "#                 print(\"\\nAll file types have been downloaded!\")\n",
        "#                 break\n",
        "\n",
        "#             # Print remaining file types\n",
        "#             print(\"\\nFile types not yet downloaded:\")\n",
        "#             for ext in sorted(available_types):\n",
        "#                 file_count = sum(1 for files in all_files.values()\n",
        "#                                for file in files\n",
        "#                                if Path(file.filename).suffix.lower()[1:] == ext)\n",
        "#                 print(f\"- {ext}: {file_count} files\")\n",
        "\n",
        "#             # Ask if user wants to download more files\n",
        "#             print(\"\\nWould you like to download additional file types? (y/n)\")\n",
        "#             response = input().lower()\n",
        "#             if response != 'y':\n",
        "#                 break\n",
        "\n",
        "#             # Get file type selection\n",
        "#             print(\"\\nSelect file type to download:\")\n",
        "#             print(\"Available types:\", \", \".join(sorted(available_types)))\n",
        "#             file_type = input(\"Enter file type (without dot): \").lower()\n",
        "\n",
        "#             if file_type not in available_types:\n",
        "#                 logger.error(f\"Invalid file type. Must be one of: {', '.join(sorted(available_types))}\")\n",
        "#                 continue\n",
        "\n",
        "#             print(f\"\\nDownloading {file_type} files using {protocol} protocol...\")\n",
        "#             for pride_id, files in all_files.items():\n",
        "#                 # Filter files by type and download\n",
        "#                 type_files = [f for f in files if Path(f.filename).suffix.lower()[1:] == file_type]\n",
        "#                 for file in type_files:\n",
        "#                     download_file(file, download_dir, protocol)\n",
        "\n",
        "#             # Add to downloaded types\n",
        "#             downloaded_types.add(file_type)\n",
        "\n",
        "#             # Show progress\n",
        "#             remaining = len(available_types) - len(downloaded_types)\n",
        "#             print(f\"\\nProgress: {len(downloaded_types)} file types downloaded, {remaining} remaining\")\n",
        "\n",
        "#         # Final summary\n",
        "#         print(\"\\nDownload session complete!\")\n",
        "#         print(\"Downloaded file types:\", \", \".join(sorted(downloaded_types)))\n",
        "#         if available_types - downloaded_types:\n",
        "#             print(\"Remaining file types:\", \", \".join(sorted(available_types - downloaded_types)))\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Process failed: {str(e)}\")\n",
        "#         raise"
      ],
      "metadata": {
        "id": "jR7BJIC5qJL7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm9ez4I_q2d7",
        "outputId": "634c34a7-6f8e-4b2e-c7a3-c9e3e0651e57"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking available files for each PRIDE project...\n",
            "\n",
            "Files available for PXD045412:\n",
            "==================================================\n",
            "\n",
            "File type: .fasta\n",
            "Number of files: 1\n",
            "Total size: 0.00 GB\n",
            "\n",
            "Files:\n",
            "- COL1_Bovidae_concatenated_filled_AJonly.fasta (0.00 GB)\n",
            "\n",
            "File type: .pdf\n",
            "Number of files: 1\n",
            "Total size: 0.00 GB\n",
            "\n",
            "Files:\n",
            "- tables.pdf (0.00 GB)\n",
            "\n",
            "File type: .raw\n",
            "Number of files: 8\n",
            "Total size: 11.47 GB\n",
            "\n",
            "Files:\n",
            "- 20190203_QE7_nLC7_MEM_COLLAB_TJ_UofY_JORD12.raw (1.39 GB)\n",
            "- 20190203_QE7_nLC7_MEM_COLLAB_TJ_UofY_JORD23.raw (1.37 GB)\n",
            "- 20211027_EXPL2_nLC3_MEM_collab_77min_DDA_Ancient_bone_SM10.raw (1.71 GB)\n",
            "- 20211027_EXPL2_nLC3_MEM_collab_77min_DDA_Ancient_bone_SM18.raw (1.34 GB)\n",
            "- 20211027_EXPL2_nLC3_MEM_collab_77min_DDA_Ancient_bone_SM26.raw (0.95 GB)\n",
            "- 20211027_EXPL2_nLC3_MEM_collab_77min_DDA_Ancient_bone_SM4.raw (1.48 GB)\n",
            "- 20211027_EXPL2_nLC3_MEM_collab_77min_DDA_Ancient_bone_SM5.raw (1.31 GB)\n",
            "- 20230807_EXPL5_nLC11_GT_collab_77min_DDA_P0578.raw (1.92 GB)\n",
            "\n",
            "File type: .txt\n",
            "Number of files: 13\n",
            "Total size: 0.55 GB\n",
            "\n",
            "Files:\n",
            "- Deamidation_NQ_Sites.txt (0.00 GB)\n",
            "- HydroxyprolineSites.txt (0.00 GB)\n",
            "- Oxidation_M_Sites.txt (0.00 GB)\n",
            "- allPeptides.txt (0.23 GB)\n",
            "- evidence.txt (0.03 GB)\n",
            "- modificationSpecificPeptides.txt (0.00 GB)\n",
            "- msms.txt (0.08 GB)\n",
            "- msmsScans.txt (0.20 GB)\n",
            "- mzRange.txt (0.00 GB)\n",
            "- parameters.txt (0.00 GB)\n",
            "- peptides.txt (0.00 GB)\n",
            "- proteinGroups.txt (0.00 GB)\n",
            "- summary.txt (0.00 GB)\n",
            "\n",
            "File type: .xml\n",
            "Number of files: 1\n",
            "Total size: 0.00 GB\n",
            "\n",
            "Files:\n",
            "- mqpar.xml (0.00 GB)\n",
            "\n",
            "Files available for PXD011377:\n",
            "==================================================\n",
            "\n",
            "File type: .gz\n",
            "Number of files: 14\n",
            "Total size: 0.09 GB\n",
            "\n",
            "Files:\n",
            "- Acid.mzid.gz (0.01 GB)\n",
            "- Acid.pride.mztab.gz (0.00 GB)\n",
            "- Ambic.mzid.gz (0.01 GB)\n",
            "- Ambic.pride.mztab.gz (0.00 GB)\n",
            "- Blank.mzid.gz (0.00 GB)\n",
            "- Blank.pride.mztab.gz (0.00 GB)\n",
            "- FL0473_MSQ1009_FWelker_1_6ul_XiaheAmbic.pride.mgf.gz (0.03 GB)\n",
            "- FL0473_MSQ1009_FWelker_1_XiaheAmbic.pride.mgf.gz (0.01 GB)\n",
            "- FL0473_MSQ1009_FWelker_2_6ul_XiaheAcid.pride.mgf.gz (0.01 GB)\n",
            "- FL0473_MSQ1009_FWelker_2_XiaheAcid.pride.mgf.gz (0.00 GB)\n",
            "- FL0473_MSQ1009_FWelker_3_6ul_XiaheBlank.pride.mgf.gz (0.01 GB)\n",
            "- FL0473_MSQ1009_FWelker_3_XiaheBlank.pride.mgf.gz (0.00 GB)\n",
            "- FL0563_MSQ1080_FWelker_XiaheAcid2.pride.mgf.gz (0.00 GB)\n",
            "- FL0563_MSQ1080_FWelker_XiaheAmbic2.pride.mgf.gz (0.01 GB)\n",
            "\n",
            "File type: .mgf\n",
            "Number of files: 8\n",
            "Total size: 0.10 GB\n",
            "\n",
            "Files:\n",
            "- FL0473_MSQ1009_FWelker_1_6ul_XiaheAmbic.mgf (0.04 GB)\n",
            "- FL0473_MSQ1009_FWelker_1_XiaheAmbic.mgf (0.01 GB)\n",
            "- FL0473_MSQ1009_FWelker_2_6ul_XiaheAcid.mgf (0.02 GB)\n",
            "- FL0473_MSQ1009_FWelker_2_XiaheAcid.mgf (0.01 GB)\n",
            "- FL0473_MSQ1009_FWelker_3_6ul_XiaheBlank.mgf (0.01 GB)\n",
            "- FL0473_MSQ1009_FWelker_3_XiaheBlank.mgf (0.00 GB)\n",
            "- FL0563_MSQ1080_FWelker_XiaheAcid2.mgf (0.00 GB)\n",
            "- FL0563_MSQ1080_FWelker_XiaheAmbic2.mgf (0.01 GB)\n",
            "\n",
            "File type: .raw\n",
            "Number of files: 8\n",
            "Total size: 11.25 GB\n",
            "\n",
            "Files:\n",
            "- FL0473_MSQ1009_FWelker_1_6ul_XiaheAmbic.raw (1.42 GB)\n",
            "- FL0473_MSQ1009_FWelker_1_XiaheAmbic.raw (1.61 GB)\n",
            "- FL0473_MSQ1009_FWelker_2_6ul_XiaheAcid.raw (1.48 GB)\n",
            "- FL0473_MSQ1009_FWelker_2_XiaheAcid.raw (1.40 GB)\n",
            "- FL0473_MSQ1009_FWelker_3_6ul_XiaheBlank.raw (1.37 GB)\n",
            "- FL0473_MSQ1009_FWelker_3_XiaheBlank.raw (1.21 GB)\n",
            "- FL0563_MSQ1080_FWelker_XiaheAcid2.raw (1.13 GB)\n",
            "- FL0563_MSQ1080_FWelker_XiaheAmbic2.raw (1.62 GB)\n",
            "\n",
            "File types not yet downloaded:\n",
            "- fasta: 1 files\n",
            "- gz: 14 files\n",
            "- mgf: 8 files\n",
            "- pdf: 1 files\n",
            "- raw: 16 files\n",
            "- txt: 13 files\n",
            "- xml: 1 files\n",
            "\n",
            "Would you like to download additional file types? (y/n)\n",
            "y\n",
            "\n",
            "Select file type to download:\n",
            "Available types: fasta, gz, mgf, pdf, raw, txt, xml\n",
            "Enter file type (without dot): txt\n",
            "\n",
            "Downloading txt files using aspera protocol...\n",
            "[DOWNLOADING] summary.txt ...\n",
            "[SUCCESS] summary.txt\n",
            "[DOWNLOADING] evidence.txt ...\n",
            "[SUCCESS] evidence.txt\n",
            "[DOWNLOADING] msms.txt ...\n",
            "[SUCCESS] msms.txt\n",
            "[DOWNLOADING] HydroxyprolineSites.txt ...\n",
            "[SUCCESS] HydroxyprolineSites.txt\n",
            "[DOWNLOADING] allPeptides.txt ...\n",
            "[SUCCESS] allPeptides.txt\n",
            "[DOWNLOADING] parameters.txt ...\n",
            "[SUCCESS] parameters.txt\n",
            "[DOWNLOADING] mzRange.txt ...\n",
            "[SUCCESS] mzRange.txt\n",
            "[DOWNLOADING] peptides.txt ...\n",
            "[SUCCESS] peptides.txt\n",
            "[DOWNLOADING] modificationSpecificPeptides.txt ...\n",
            "[SUCCESS] modificationSpecificPeptides.txt\n",
            "[DOWNLOADING] Oxidation_M_Sites.txt ...\n",
            "[SUCCESS] Oxidation_M_Sites.txt\n",
            "[DOWNLOADING] Deamidation_NQ_Sites.txt ...\n",
            "[SUCCESS] Deamidation_NQ_Sites.txt\n",
            "[DOWNLOADING] proteinGroups.txt ...\n",
            "[SUCCESS] proteinGroups.txt\n",
            "[DOWNLOADING] msmsScans.txt ...\n",
            "[SUCCESS] msmsScans.txt\n",
            "\n",
            "Progress: 1 file types downloaded, 6 remaining\n",
            "\n",
            "File types not yet downloaded:\n",
            "- fasta: 1 files\n",
            "- gz: 14 files\n",
            "- mgf: 8 files\n",
            "- pdf: 1 files\n",
            "- raw: 16 files\n",
            "- xml: 1 files\n",
            "\n",
            "Would you like to download additional file types? (y/n)\n",
            "fasta\n",
            "\n",
            "Download session complete!\n",
            "Downloaded file types: txt\n",
            "Remaining file types: fasta, gz, mgf, pdf, raw, xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8NtWYU_BQMy_"
      },
      "outputs": [],
      "source": [
        "# #-----------------Main\n",
        "# def main():\n",
        "#     \"\"\"Main execution function with verification and download steps.\"\"\"\n",
        "#     try:\n",
        "#         # Get PRIDE IDs\n",
        "#         pride_ids = get_pride_ids_from_sheet(spreadsheet_id, sheet_name)\n",
        "#         if not pride_ids:\n",
        "#             logger.error(\"No PRIDE IDs found\")\n",
        "#             return\n",
        "\n",
        "#         # Setup directories\n",
        "#         base_dir = Path(shared_drive_base_dir_str)\n",
        "#         download_dir = base_dir / folder_name\n",
        "#         download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # First list all available files\n",
        "#         print(\"\\nChecking available files for each PRIDE project...\")\n",
        "#         all_files = {}\n",
        "\n",
        "#         for pride_id in pride_ids:\n",
        "#             files = get_project_files(pride_id, download_dir)\n",
        "#             all_files[pride_id] = files\n",
        "#             print_file_summary(pride_id, files)\n",
        "#             save_file_summary(pride_id, files, download_dir)\n",
        "\n",
        "#         # Print overall summary\n",
        "#         print(\"\\nOverall Summary:\")\n",
        "#         print(\"=\" * 50)\n",
        "#         for pride_id, files in all_files.items():\n",
        "#             grouped = group_files_by_type(files)\n",
        "#             print(f\"\\n{pride_id}:\")\n",
        "#             for file_type, file_list in sorted(grouped.items()):\n",
        "#                 print(f\"  .{file_type}: {len(file_list)} files\")\n",
        "\n",
        "#         # Add download functionality\n",
        "#         print(\"\\nWould you like to proceed with downloads? (y/n)\")\n",
        "#         response = input().lower()\n",
        "#         if response == 'y':\n",
        "#             print(\"\\nSelect file type to download:\")\n",
        "#             available_types = set()\n",
        "#             for files in all_files.values():\n",
        "#                 for file in files:\n",
        "#                     available_types.add(file.file_type)\n",
        "\n",
        "#             print(\"Available file types:\", \", \".join(sorted(available_types)))\n",
        "#             file_type = input(\"Enter file type (without dot): \").lower()\n",
        "\n",
        "#             if file_type not in available_types:\n",
        "#                 logger.error(f\"Invalid file type. Must be one of: {', '.join(sorted(available_types))}\")\n",
        "#                 return\n",
        "\n",
        "#             print(f\"\\nDownloading {file_type} files using {protocol} protocol...\")\n",
        "#             for pride_id in pride_ids:\n",
        "#                 download_files_by_type(pride_id, file_type, all_files, download_dir, protocol)\n",
        "\n",
        "#         print(\"\\nFile summaries have been saved. You can now proceed with downloads.\")\n",
        "#         print(f\"Current file types to download: {file_types}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Process failed: {str(e)}\")\n",
        "#         raise\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNglBh+14G3WHHTu4rbYh7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}