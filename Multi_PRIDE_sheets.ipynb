{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PRIDE/blob/main/Multi_PRIDE_sheets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-lmpiYQaqk2"
      },
      "source": [
        "# PRIDE Files Download Program\n",
        "\n",
        "This Colab notebook provides an automated solution for downloading proteomics data files from the PRIDE (PRoteomics IDEntifications) database using the [pridepy](https://github.com/PRIDE-Archive/pridepy) package.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Retrieves PRIDE project IDs from a specified Google Sheet\n",
        "- Downloads essential proteomics files (.fasta, .mgf) and README files\n",
        "- Supports multiple download protocols (aspera, ftp, globus)\n",
        "- Organizes downloads in a structured directory hierarchy by file category\n",
        "- Integrates with Google Drive for storage\n",
        "- Smart handling of large text files with configurable size limits\n",
        "- Interactive file type selection and download tracking\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Google Colab environment\n",
        "- Access to Google Drive\n",
        "- Google Sheets containing PRIDE project IDs\n",
        "- `pridepy` package (automatically installed by the notebook)\n",
        "\n",
        "## Configuration Parameters\n",
        "\n",
        "The following parameters can be configured in the notebook:\n",
        "\n",
        "- `sheet_name`: Name of the worksheet containing PRIDE IDs\n",
        "- `repository`: Repository name (currently set to 'PRIDE')\n",
        "- `file_types`: File types to download (default: 'mgf, fasta, txt, raw')\n",
        "- `protocol`: Download protocol ('aspera', 'ftp', or 'globus')\n",
        "- `folder_name`: Name of the folder where files will be stored\n",
        "- `download_large_text_files`: Boolean flag to control downloading of text files > 1MB\n",
        "- `shared_drive_base_dir_str`: Base directory path in Google Drive\n",
        "- `spreadsheet_id`: Google Sheets ID containing PRIDE project IDs\n",
        "\n",
        "## File Categories\n",
        "\n",
        "Files are automatically organized into the following categories:\n",
        "\n",
        "- `RAW`: Raw instrument data files (.raw, .wiff, .d)\n",
        "- `PEAK`: Peak list files (.mgf, .mzml)\n",
        "- `RESULT`: Analysis result files (.mzidentml, .mztab)\n",
        "- `FASTA`: Sequence database files (.fasta)\n",
        "- `OTHER`: Documentation and miscellaneous files (.txt, .pdf)\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "Files are organized in a category-based structure:\n",
        "```\n",
        "shared_drive_base_dir/\n",
        "└── folder_name/\n",
        "    └── PRIDE_ID/\n",
        "        ├── RAW/\n",
        "        │   └── raw_files...\n",
        "        ├── PEAK/\n",
        "        │   └── peak_files...\n",
        "        ├── RESULT/\n",
        "        │   └── result_files...\n",
        "        ├── FASTA/\n",
        "        │   └── fasta_files...\n",
        "        └── OTHER/\n",
        "            └── documentation_files...\n",
        "```\n",
        "\n",
        "## Interactive Features\n",
        "\n",
        "- Lists available file types with size information\n",
        "- Tracks downloaded file types across sessions\n",
        "- Shows progress and remaining file types\n",
        "- Optional size limits for text files (default 1MB limit)\n",
        "- Automatic retry with FTP if Aspera download fails\n",
        "\n",
        "## Error Handling\n",
        "\n",
        "The program includes comprehensive error handling for:\n",
        "- Google Sheets API errors\n",
        "- File download failures with protocol fallback\n",
        "- JSON parsing errors\n",
        "- Directory creation issues\n",
        "- Size limit violations\n",
        "- Invalid file type selections\n",
        "\n",
        "## Dependencies\n",
        "\n",
        "- `pridepy`\n",
        "- `google.colab`\n",
        "- `googleapiclient`\n",
        "- `pathlib`\n",
        "- `subprocess`\n",
        "- `json`\n",
        "- `tqdm` (for progress bars)\n",
        "\n",
        "## Notes\n",
        "\n",
        "- The program uses the `pridepy` command-line interface for file downloads\n",
        "- Progress and errors are logged to both console and log files\n",
        "- Failed downloads are reported but don't stop the entire process\n",
        "- Existing files are skipped to avoid unnecessary downloads\n",
        "- Text files > 1MB are skipped by default unless explicitly enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3I5MY3SbDrG"
      },
      "outputs": [],
      "source": [
        "#download pridepy\n",
        "!pip install --upgrade pridepy tqdm\n",
        "\n",
        "\"\"\"To learn more about pridepy\"\"\"\n",
        "\n",
        "# !pridepy --help\n",
        "# !pridepy stream-files-metadata --help\n",
        "# !pridepy --help | grep download\n",
        "# !pip install --upgrade pridepy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2eWVoekQBRk"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth, drive\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "import subprocess\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dataclasses import dataclass\n",
        "import pandas as pd\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "\n",
        "# --- Module Parameters ---\n",
        "sheet_name = 'PX Hominins'  # @param {type:\"string\"}\n",
        "repository = 'PRIDE'  # @param {type:\"string\"}\n",
        "file_types = 'mgf, fasta, txt, raw'  # @param {type:\"string\"}\n",
        "protocol = 'aspera'  # @param ['aspera', 'ftp', 'globus']\n",
        "folder_name = 'Hominins'  # @param {type:\"string\"}\n",
        "download_large_text_files = False  # @param {type:\"boolean\"}\n",
        "shared_drive_base_dir_str = \"/content/drive/Shareddrives/ZooMS_Data/PRIDE\"  # @param {type:\"string\"}\n",
        "spreadsheet_id = '127K6zdl5y46DRqUwRr-V32nUDoceaddbhG9XyozJs-4'  # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Authenticate ---\n",
        "auth.authenticate_user()\n",
        "\n",
        "# --- Configure Logging ---\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FileInfo:\n",
        "    \"\"\"Data class for file information.\"\"\"\n",
        "    filename: str\n",
        "    size: int\n",
        "    project_id: str\n",
        "    file_type: str\n",
        "    category: str = \"OTHER\"  # Default category if none specified\n",
        "\n",
        "    @property\n",
        "    def size_in_gb(self) -> float:\n",
        "        \"\"\"Return file size in gigabytes.\"\"\"\n",
        "        return self.size / 1e9\n",
        "\n",
        "    @property\n",
        "    def size_in_mb(self) -> float:\n",
        "        \"\"\"Return file size in megabytes.\"\"\"\n",
        "        return self.size / (1024 * 1024)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], project_id: str) -> 'FileInfo':\n",
        "        \"\"\"Create FileInfo instance from PRIDE metadata dictionary.\"\"\"\n",
        "        # Try different size fields that might exist in PRIDE metadata\n",
        "        size_fields = ['fileSize', 'publicFileSize', 'fileSizeBytes']\n",
        "        file_size = 0\n",
        "        for field in size_fields:\n",
        "            if field in data and data[field]:\n",
        "                try:\n",
        "                    file_size = int(str(data[field]).replace(',', ''))\n",
        "                    break\n",
        "                except (ValueError, TypeError):\n",
        "                    continue\n",
        "\n",
        "        # Determine file category based on extension\n",
        "        filename = data[\"fileName\"]\n",
        "        file_ext = Path(filename).suffix.lower()\n",
        "\n",
        "        # Map file extensions to PRIDE categories\n",
        "        category_map = {\n",
        "            '.raw': 'RAW',\n",
        "            '.wiff': 'RAW',\n",
        "            '.d': 'RAW',\n",
        "            '.mgf': 'PEAK',\n",
        "            '.mzml': 'PEAK',\n",
        "            '.mzidentml': 'RESULT',\n",
        "            '.mztab': 'RESULT',\n",
        "            '.fasta': 'FASTA',\n",
        "            '.txt': 'OTHER',\n",
        "            '.pdf': 'OTHER'\n",
        "        }\n",
        "\n",
        "        category = category_map.get(file_ext, 'OTHER')\n",
        "\n",
        "        # Special case for README files\n",
        "        if 'readme' in filename.lower():\n",
        "            category = 'OTHER'\n",
        "\n",
        "        return cls(\n",
        "            filename=filename,\n",
        "            size=file_size,\n",
        "            project_id=project_id,\n",
        "            file_type=file_ext[1:] if file_ext else '',  # Remove the dot from extension\n",
        "            category=category\n",
        "        )\n",
        "\n",
        "def should_download_file(file_info: FileInfo, download_large_text_files: bool = False) -> bool:\n",
        "    \"\"\"\n",
        "    Determine if a file should be downloaded based on its type and size.\n",
        "\n",
        "    Args:\n",
        "        file_info: FileInfo object containing file metadata\n",
        "        download_large_text_files: Flag to control downloading of large text files\n",
        "\n",
        "    Returns:\n",
        "        bool: True if file should be downloaded, False otherwise\n",
        "    \"\"\"\n",
        "    # Always download non-text files\n",
        "    if not file_info.filename.lower().endswith('.txt'):\n",
        "        return True\n",
        "\n",
        "    # For text files, check size limit unless override is set\n",
        "    if not download_large_text_files and file_info.size_in_mb > 1:\n",
        "        logger.info(f\"Skipping large text file: {file_info.filename} ({file_info.size_in_mb:.2f} MB)\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def download_file(file_info: FileInfo, output_dir: Path, protocol: str = 'aspera',\n",
        "                 download_large_text_files: bool = False) -> bool:\n",
        "    \"\"\"Download a single file using pridepy with size limit checks.\"\"\"\n",
        "    # First check if we should download this file\n",
        "    if not should_download_file(file_info, download_large_text_files):\n",
        "        logger.info(f\"Skipping {file_info.filename} due to size restrictions\")\n",
        "        return False\n",
        "\n",
        "    # Create category-based subdirectory\n",
        "    project_dir = output_dir / file_info.project_id / file_info.category\n",
        "    project_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Check if file already exists\n",
        "    file_path = project_dir / file_info.filename\n",
        "    if file_path.exists():\n",
        "        logger.info(f\"Skipping {file_info.filename}, already present in {project_dir}\")\n",
        "        print(f\"[SKIP] {file_info.filename} (Already exists)\")\n",
        "        return True\n",
        "\n",
        "    command = [\n",
        "        'pridepy',\n",
        "        'download-file-by-name',\n",
        "        '-a', file_info.project_id,\n",
        "        '-f', file_info.filename,\n",
        "        '-o', str(project_dir),\n",
        "        '-p', protocol\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(f\"[DOWNLOADING] {file_info.filename} ...\")\n",
        "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "        print(f\"[SUCCESS] {file_info.filename}\")\n",
        "        logger.info(f\"Successfully downloaded {file_info.filename}\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"[FAILED] {file_info.filename} (Error: {e.stderr})\")\n",
        "        logger.error(f\"Failed to download {file_info.filename} with {protocol}: {e.stderr}\")\n",
        "\n",
        "        # Try FTP if Aspera fails\n",
        "        if protocol == 'aspera':\n",
        "            logger.info(f\"Retrying with FTP: {file_info.filename}\")\n",
        "            return download_file(file_info, output_dir, 'ftp', download_large_text_files)\n",
        "        return False"
      ],
      "metadata": {
        "id": "lribTsXGvQYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "U72Gb7jEPtNd"
      },
      "outputs": [],
      "source": [
        "# #----Functions\n",
        "\n",
        "def get_pride_ids_from_sheet(spreadsheet_id: str, sheet_name: str) -> list:\n",
        "    \"\"\"Retrieves PRIDE IDs from Google Sheet.\"\"\"\n",
        "    try:\n",
        "        service = build('sheets', 'v4')\n",
        "        sheet = service.spreadsheets()\n",
        "        range_name = f\"{sheet_name}!A2:A\"\n",
        "        result = sheet.values().get(spreadsheetId=spreadsheet_id, range=range_name).execute()\n",
        "        values = result.get('values', [])\n",
        "\n",
        "        if not values:\n",
        "            logger.warning('No data found in the Google Sheet.')\n",
        "            return []\n",
        "\n",
        "        pride_ids = [row[0] for row in values if row]\n",
        "        logger.info(f\"Retrieved {len(pride_ids)} PRIDE IDs\")\n",
        "        for pride_id in pride_ids:\n",
        "            logger.info(f\"Found PRIDE ID: {pride_id}\")\n",
        "        return pride_ids\n",
        "\n",
        "    except HttpError as err:\n",
        "        logger.error(f\"Failed to retrieve data from Google Sheets: {err}\")\n",
        "        return []\n",
        "\n",
        "def get_project_files(project_id: str, download_dir: Path) -> List[FileInfo]:\n",
        "    \"\"\"Get list of files available for a PRIDE project.\"\"\"\n",
        "    project_dir = download_dir / project_id\n",
        "    project_dir.mkdir(parents=True, exist_ok=True)\n",
        "    metadata_file = project_dir / f\"{project_id}_metadata.json\"\n",
        "\n",
        "    command = [\n",
        "        'pridepy',\n",
        "        'stream-files-metadata',\n",
        "        '-a', project_id,\n",
        "        '-o', str(metadata_file)\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            command,\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        logger.info(f\"Retrieved metadata for project {project_id}\")\n",
        "\n",
        "        with open(metadata_file, \"r\") as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        file_infos = []\n",
        "        for data in metadata:\n",
        "            file_info = FileInfo.from_dict(data, project_id)\n",
        "            file_infos.append(file_info)\n",
        "            logger.info(f\"Found file: {file_info.filename} ({file_info.size_in_gb:.2f} GB)\")\n",
        "\n",
        "        return file_infos\n",
        "\n",
        "    except (subprocess.CalledProcessError, json.JSONDecodeError, IOError) as err:\n",
        "        logger.error(f\"Failed to get files for project {project_id}: {err}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "def group_files_by_type(files: List[FileInfo]) -> Dict[str, List[FileInfo]]:\n",
        "    \"\"\"Group files by their type.\"\"\"\n",
        "    grouped = {}\n",
        "    for file in files:\n",
        "        if file.file_type not in grouped:\n",
        "            grouped[file.file_type] = []\n",
        "        grouped[file.file_type].append(file)\n",
        "    return grouped\n",
        "\n",
        "\n",
        "def print_file_summary(pride_id: str, files: List[FileInfo]):\n",
        "    \"\"\"Print summary of available files for a PRIDE project.\"\"\"\n",
        "    print(f\"\\nFiles available for {pride_id}:\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if not files:\n",
        "        print(\"No files found\")\n",
        "        return\n",
        "\n",
        "    # Group files by extension\n",
        "    grouped_files = {}\n",
        "    for f in files:\n",
        "        ext = Path(f.filename).suffix.lower()\n",
        "        if ext not in grouped_files:\n",
        "            grouped_files[ext] = []\n",
        "        grouped_files[ext].append(f)\n",
        "\n",
        "    for ext, file_list in sorted(grouped_files.items()):\n",
        "        total_size = sum(f.size_in_gb for f in file_list)\n",
        "        print(f\"\\nFile type: {ext}\")\n",
        "        print(f\"Number of files: {len(file_list)}\")\n",
        "        print(f\"Total size: {total_size:.2f} GB\")\n",
        "        print(\"\\nFiles:\")\n",
        "        for file_info in sorted(file_list, key=lambda x: x.filename):\n",
        "            print(f\"- {file_info.filename} ({file_info.size_in_gb:.2f} GB)\")\n",
        "\n",
        "def save_file_summary(pride_id: str, files: List[FileInfo], summary_dir: Path):\n",
        "    \"\"\"Save file summary to a text file.\"\"\"\n",
        "    summary_path = summary_dir / pride_id / \"available_files_summary.txt\"\n",
        "    summary_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Group files by extension\n",
        "    grouped_files = {}\n",
        "    for f in files:\n",
        "        ext = Path(f.filename).suffix.lower()\n",
        "        if ext not in grouped_files:\n",
        "            grouped_files[ext] = []\n",
        "        grouped_files[ext].append(f)\n",
        "\n",
        "    with open(summary_path, \"w\") as f:\n",
        "        f.write(f\"Files available for {pride_id}:\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "\n",
        "        if not files:\n",
        "            f.write(\"No files found\\n\")\n",
        "            return\n",
        "\n",
        "        for ext, file_list in sorted(grouped_files.items()):\n",
        "            total_size = sum(f.size_in_gb for f in file_list)\n",
        "            f.write(f\"\\nFile type: {ext}\\n\")\n",
        "            f.write(f\"Number of files: {len(file_list)}\\n\")\n",
        "            f.write(f\"Total size: {total_size:.2f} GB\\n\")\n",
        "            f.write(\"\\nFiles:\\n\")\n",
        "            for file_info in sorted(file_list, key=lambda x: x.filename):\n",
        "                f.write(f\"- {file_info.filename} ({file_info.size_in_gb:.2f} GB)\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main execution function with enhanced download options and size limits.\"\"\"\n",
        "    try:\n",
        "        # Get PRIDE IDs and check files\n",
        "        pride_ids = get_pride_ids_from_sheet(spreadsheet_id, sheet_name)\n",
        "        if not pride_ids:\n",
        "            logger.error(\"No PRIDE IDs found\")\n",
        "            return\n",
        "\n",
        "        # Setup directories\n",
        "        base_dir = Path(shared_drive_base_dir_str)\n",
        "        download_dir = base_dir / folder_name\n",
        "        download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # First list all available files\n",
        "        print(\"\\nChecking available files for each PRIDE project...\")\n",
        "        all_files = {}\n",
        "        downloaded_types = set()  # Track which file types have been downloaded\n",
        "\n",
        "        for pride_id in pride_ids:\n",
        "            files = get_project_files(pride_id, download_dir)\n",
        "            all_files[pride_id] = files\n",
        "            print_file_summary(pride_id, files)\n",
        "            save_file_summary(pride_id, files, download_dir)\n",
        "\n",
        "        while True:  # Continue until user is done\n",
        "            # Get all available file types that haven't been downloaded\n",
        "            available_types = set()\n",
        "            for files in all_files.values():\n",
        "                for file in files:\n",
        "                    ext = Path(file.filename).suffix.lower()[1:]  # Get extension without dot\n",
        "                    if ext and ext not in downloaded_types:  # Only add if extension exists\n",
        "                        available_types.add(ext)\n",
        "\n",
        "            if not available_types:\n",
        "                print(\"\\nAll file types have been downloaded!\")\n",
        "                break\n",
        "\n",
        "            # Print remaining file types with size information for text files\n",
        "            print(\"\\nFile types not yet downloaded:\")\n",
        "            for ext in sorted(available_types):\n",
        "                file_count = 0\n",
        "                total_size_mb = 0\n",
        "                skipped_count = 0\n",
        "                for files in all_files.values():\n",
        "                    for file in files:\n",
        "                        if Path(file.filename).suffix.lower()[1:] == ext:\n",
        "                            if ext == 'txt' and not download_large_text_files and file.size_in_mb > 1:\n",
        "                                skipped_count += 1\n",
        "                            else:\n",
        "                                file_count += 1\n",
        "                                total_size_mb += file.size_in_mb\n",
        "\n",
        "                print(f\"- {ext}: {file_count} files ({total_size_mb:.2f} MB total)\")\n",
        "                if ext == 'txt' and skipped_count > 0:\n",
        "                    print(f\"  Note: {skipped_count} text files > 1MB will be skipped\")\n",
        "\n",
        "            # Ask if user wants to download more files\n",
        "            print(\"\\nWould you like to download additional file types? (y/n)\")\n",
        "            response = input().lower()\n",
        "            if response != 'y':\n",
        "                break\n",
        "\n",
        "            # Get file type selection\n",
        "            print(\"\\nSelect file type to download:\")\n",
        "            print(\"Available types:\", \", \".join(sorted(available_types)))\n",
        "            file_type = input(\"Enter file type (without dot): \").lower()\n",
        "\n",
        "            if file_type not in available_types:\n",
        "                logger.error(f\"Invalid file type. Must be one of: {', '.join(sorted(available_types))}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nDownloading {file_type} files using {protocol} protocol...\")\n",
        "            for pride_id, files in all_files.items():\n",
        "                # Filter files by type and download\n",
        "                type_files = [f for f in files if Path(f.filename).suffix.lower()[1:] == file_type]\n",
        "                for file in type_files:\n",
        "                    download_file(file, download_dir, protocol, download_large_text_files)\n",
        "\n",
        "            # Add to downloaded types\n",
        "            downloaded_types.add(file_type)\n",
        "\n",
        "            # Show progress\n",
        "            remaining = len(available_types) - len(downloaded_types)\n",
        "            print(f\"\\nProgress: {len(downloaded_types)} file types downloaded, {remaining} remaining\")\n",
        "\n",
        "        # Final summary\n",
        "        print(\"\\nDownload session complete!\")\n",
        "        print(\"Downloaded file types:\", \", \".join(sorted(downloaded_types)))\n",
        "        if available_types - downloaded_types:\n",
        "            print(\"Remaining file types:\", \", \".join(sorted(available_types - downloaded_types)))\n",
        "\n",
        "        # Print size limit information\n",
        "        if not download_large_text_files:\n",
        "            print(\"\\nNote: Text files larger than 1MB were skipped. Set download_large_text_files = True to download all text files.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Process failed: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "q1eg657YvsCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pm9ez4I_q2d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NtWYU_BQMy_"
      },
      "outputs": [],
      "source": [
        "# #-----------------Main\n",
        "# def main():\n",
        "#     \"\"\"Main execution function with verification and download steps.\"\"\"\n",
        "#     try:\n",
        "#         # Get PRIDE IDs\n",
        "#         pride_ids = get_pride_ids_from_sheet(spreadsheet_id, sheet_name)\n",
        "#         if not pride_ids:\n",
        "#             logger.error(\"No PRIDE IDs found\")\n",
        "#             return\n",
        "\n",
        "#         # Setup directories\n",
        "#         base_dir = Path(shared_drive_base_dir_str)\n",
        "#         download_dir = base_dir / folder_name\n",
        "#         download_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#         # First list all available files\n",
        "#         print(\"\\nChecking available files for each PRIDE project...\")\n",
        "#         all_files = {}\n",
        "\n",
        "#         for pride_id in pride_ids:\n",
        "#             files = get_project_files(pride_id, download_dir)\n",
        "#             all_files[pride_id] = files\n",
        "#             print_file_summary(pride_id, files)\n",
        "#             save_file_summary(pride_id, files, download_dir)\n",
        "\n",
        "#         # Print overall summary\n",
        "#         print(\"\\nOverall Summary:\")\n",
        "#         print(\"=\" * 50)\n",
        "#         for pride_id, files in all_files.items():\n",
        "#             grouped = group_files_by_type(files)\n",
        "#             print(f\"\\n{pride_id}:\")\n",
        "#             for file_type, file_list in sorted(grouped.items()):\n",
        "#                 print(f\"  .{file_type}: {len(file_list)} files\")\n",
        "\n",
        "#         # Add download functionality\n",
        "#         print(\"\\nWould you like to proceed with downloads? (y/n)\")\n",
        "#         response = input().lower()\n",
        "#         if response == 'y':\n",
        "#             print(\"\\nSelect file type to download:\")\n",
        "#             available_types = set()\n",
        "#             for files in all_files.values():\n",
        "#                 for file in files:\n",
        "#                     available_types.add(file.file_type)\n",
        "\n",
        "#             print(\"Available file types:\", \", \".join(sorted(available_types)))\n",
        "#             file_type = input(\"Enter file type (without dot): \").lower()\n",
        "\n",
        "#             if file_type not in available_types:\n",
        "#                 logger.error(f\"Invalid file type. Must be one of: {', '.join(sorted(available_types))}\")\n",
        "#                 return\n",
        "\n",
        "#             print(f\"\\nDownloading {file_type} files using {protocol} protocol...\")\n",
        "#             for pride_id in pride_ids:\n",
        "#                 download_files_by_type(pride_id, file_type, all_files, download_dir, protocol)\n",
        "\n",
        "#         print(\"\\nFile summaries have been saved. You can now proceed with downloads.\")\n",
        "#         print(f\"Current file types to download: {file_types}\")\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"Process failed: {str(e)}\")\n",
        "#         raise\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmaRGIH7JZXjm8l5lEikd7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}