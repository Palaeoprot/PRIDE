{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Palaeoprot/PRIDE/blob/main/BLAST_Collagen_Exon_Mapper_v1_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-v14"
      },
      "source": [
        "# ðŸ§¬ **Collagen Exon Mapper v1.9 (Colab Edition)**\n",
        "\n",
        "A production-ready, Colab-focused pipeline for mapping collagen exon\n",
        "architectures across taxa, with robust caching, phylogeny-weighted\n",
        "consensus, optional rescue of misread sequences, and clear per-step\n",
        "reporting for new users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymWxDBSwfOt9"
      },
      "source": [
        "# **Part 1: Paths & Project Layout (Harmonised)**\n",
        "\n",
        "This notebook uses **two roots**:\n",
        "\n",
        "- **WORK_ROOT (ephemeral)**: all perâ€‘run outputs under `CollagenExonMapper/`.  \n",
        "  These can be deleted after each run without affecting other users.\n",
        "\n",
        "- **SHARED_ROOT (durable)**: all caches, master datasets, manifests, and archives\n",
        "  under `_SHARED_DATA/ExonMaps/collagens/`.  \n",
        "  These persist across runs and are used by all users.\n",
        "\n",
        "Compatibility:\n",
        "- We keep legacy aliases (`EXON_CACHE_PATH`, `FILTERED_UNIPROT_CACHE_PATH`, â€¦)\n",
        "  so v1.4 and v1.5 code paths continue to work.\n",
        "- We also define v1.5 names (`RAW_EXONS_CACHE`, `FILTERED_UNIPROT_TSV`, â€¦).\n",
        "\n",
        "Deletion policy:\n",
        "- It is safe to delete **only** `CollagenExonMapper/run_*` after a run.\n",
        "- Never delete `_SHARED_DATA/ExonMaps/collagens/cache` or `â€¦/runs_archive`.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 1: Dependencies =====\n",
        "# Description: Install and import all required libraries for the notebook.\n",
        "\n",
        "# --- Installations ---\n",
        "# Use pip to install necessary third-party libraries quietly (-q).\n",
        "!pip install -q biopython ete3 requests pandas numpy matplotlib tqdm psutil\n",
        "\n",
        "# --- Core Imports ---\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- Environment Check & Setup ---\n",
        "# Determine if running in Google Colab for environment-specific logic.\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"âœ… Running in Google Colab environment.\")\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    print(\"âš ï¸ Not in a Google Colab environment.\")\n",
        "    IN_COLAB = False\n",
        "\n",
        "# --- Version Information ---\n",
        "# Print library versions for reproducibility.\n",
        "print(f\"Python: {sys.version.split()[0]}\")\n",
        "print(f\"pandas: {pd.__version__}, numpy: {np.__version__}\")"
      ],
      "metadata": {
        "id": "n2ZLXWRM63dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCOWHcw473zg"
      },
      "source": [
        "## Cell 10 â€“ Install Dependencies & Mount Drive\n",
        "\n",
        "Installs required libraries and mounts your Google Drive for persistent storage. If you are not in a Google Colab environment, mounting will be skipped."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Mount Google Drive =====\n",
        "# Description: Mounts your Google Drive to the Colab virtual machine's\n",
        "#              filesystem at the '/content/drive' directory.\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"ðŸ’¾ Mounting Google Drive...\")\n",
        "    # The force_remount=True option will re-mount the drive if it's already mounted.\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    IN_COLAB = True\n",
        "    print(\"âœ… Google Drive mounted successfully at /content/drive\")\n",
        "except ImportError:\n",
        "    # This block runs if the code is not in a Google Colab environment.\n",
        "    IN_COLAB = False\n",
        "    print(\"âš ï¸ Not in a Google Colab environment. Drive mounting skipped.\")"
      ],
      "metadata": {
        "id": "Zlj5L2yU8JZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTKv3uZY75lO"
      },
      "source": [
        "## Cell 11 â€“ Central Configuration Panel\n",
        "\n",
        "Defines all user-adjustable parameters for the run. These variables are used by subsequent cells to control gene selection, filtering thresholds, and optional features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1jXO8iHg7_zC"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 11 =====\n",
        "# Central configuration panel and dynamic directory setup.\n",
        "\n",
        "import os, re, io, time, json, hashlib, logging, psutil\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timezone\n",
        "from typing import Optional, List, Dict, Tuple, Set, Iterable\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import sys\n",
        "from Bio import Entrez\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ---------------- User-Configurable Parameters ----------------\n",
        "#@markdown #### **Core Settings**\n",
        "PROJECT_NAME = \"CollagenExonMapper\"  #@param {type:\"string\"}\n",
        "USER_EMAIL   = \"matthew@palaeome.org\"  #@param {type:\"string\"}\n",
        "Entrez.email = USER_EMAIL\n",
        "\n",
        "#@markdown #### **Gene Selection**\n",
        "PROCESS_ALL_GENES = True  #@param {type:\"boolean\"}\n",
        "GENE_SYMBOLS     = \"COL1A1,COL1A2\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### **Taxonomic Filtering**\n",
        "CLADE_TAXIDS = {\n",
        "    \"Metazoa\": 33208, \"Vertebrata\": 7742, \"Mammalia\": 40674, \"Aves\": 8782,\n",
        "    \"Reptilia\": 8504, \"Amphibia\": 8292, \"Tetrapoda\": 32523,\n",
        "    \"Bony fish\": 117570, \"Cartilaginous fish\": 7777, \"Catarrhini\": 9526\n",
        "}\n",
        "TAXONOMIC_FILTER_NAME = \"Metazoa\" #@param [\"Metazoa\",\"Vertebrata\",\"Mammalia\",\"Aves\",\"Reptilia\",\"Amphibia\",\"Tetrapoda\",\"Bony fish\",\"Cartilaginous fish\",\"Catarrhini\"]\n",
        "TARGET_TAXID = CLADE_TAXIDS[TAXONOMIC_FILTER_NAME]\n",
        "\n",
        "#@markdown #### **Thresholds (Sequences & Mapping)**\n",
        "MIN_LEN_AA          = 600  #@param {type:\"integer\"}\n",
        "MIN_GXY_TRIPLETS    = 30   #@param {type:\"integer\"}\n",
        "CHAIN_LENGTH_THRESHOLD      = 0.90  #@param {type:\"slider\", min:0.5, max:1.0, step:0.05}\n",
        "MAX_ALLOWED_GAP_PERCENTAGE  = 0.10  #@param {type:\"slider\", min:0.05, max:0.5, step:0.05}\n",
        "\n",
        "# Rescue controls\n",
        "\n",
        "#@markdown #### **Identify & rescue additional sequences (FAST; peptide-only)**\n",
        "ENABLE_RESCUE             = True   #@param {type:\"boolean\"}\n",
        "MINIMUM_RESCUE_SCORE      = 0.50   #@param {type:\"number\"}\n",
        "RESCUE_SCORE_IMPROVEMENT  = 0.10   #@param {type:\"number\"}\n",
        "GXY_CONTENT_THRESHOLD     = 0.80   #@param {type:\"number\"}\n",
        "\n",
        "#@markdown #### **DNA fetch for rescue (SLOW; Ensembl REST)**\n",
        "ENABLE_DNA_FETCH          = False  #@param {type:\"boolean\"}\n",
        "ENSEMBL_REQUESTS_PER_SEC  = 5      #@param {type:\"integer\"}\n",
        "ENSEMBL_TIMEOUT_SECS      = 20     #@param {type:\"integer\"}\n",
        "ENSEMBL_BASE              = \"https://rest.ensembl.org\"  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown #### **Debugging**\n",
        "DEBUG_SAMPLE_SIZE = -1  #@param {type:\"integer\"}\n",
        "\n",
        "# Optional taxonomy engine (not required when UniProt lineage present)\n",
        "USE_TAXONOMY_ENGINE = False  #@param {type:\"boolean\"}\n",
        "#@markdown #### **Phylogenetic Cache Settings**\n",
        "#@markdown Enable this only if you have updated the Newick tree and need to\n",
        "#@markdown regenerate the node age lookup table. This is a slow, one-time process.\n",
        "REGENERATE_NODE_AGE_CACHE = True #@param {type:\"boolean\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLy-71V98FEN"
      },
      "source": [
        "## Cell 12 â€“ Two-Root Path Model & Project Setup\n",
        "\n",
        "This cell establishes the core directory structure for the project, separating ephemeral run-specific outputs from durable shared data and caches. It also initializes logging. **All subsequent cells depend on these path variables.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unevke-T8Hbn"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 12 =====\n",
        "# Two-root path model & project setup\n",
        "\n",
        "# --- ROOTS ---\n",
        "# Ephemeral outputs for this specific run. Safe to delete.\n",
        "WORK_ROOT   = Path(\"/content/drive/MyDrive/CollagenExonMapper\")\n",
        "# Durable, shared data across all runs and users. Do not delete.\n",
        "SHARED_ROOT = Path(\"/content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/ExonMaps/collagens\")\n",
        "\n",
        "# --- TIMESTAMPED RUN DIRECTORY (EPHEMERAL) ---\n",
        "RUN_TIMESTAMP = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
        "RUN_DIR = WORK_ROOT / f\"run_{RUN_TIMESTAMP}\"\n",
        "RUN_ID = RUN_DIR.name # For use in filenames\n",
        "\n",
        "# --- SHARED, DURABLE DIRECTORIES ---\n",
        "CACHE_DIR        = SHARED_ROOT / \"cache\"\n",
        "RUNS_ARCHIVE_DIR = SHARED_ROOT / \"runs_archive\"\n",
        "DICT_DIR         = Path(\"/content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/DICTIONARIES\")\n",
        "TAXO_DIR         = Path(\"/content/drive/MyDrive/Colab_Notebooks/GitHub/_SHARED_DATA/TAXONOMY\")\n",
        "\n",
        "# --- DURABLE FILES (AUTHORITATIVE) ---\n",
        "FILTERED_UNIPROT_TSV = CACHE_DIR / \"filtered_uniprot_cache.tsv\"\n",
        "MASTER_TSV_PATH      = CACHE_DIR / \"master_collagen_dataset.tsv\"\n",
        "RAW_EXONS_CACHE      = CACHE_DIR / \"raw_exons_cache.tsv\"\n",
        "REJECTED_IDS_PATH    = CACHE_DIR / \"rejected_ids.tsv\"\n",
        "NODE_AGES_CSV_PATH = DICT_DIR / \"metazoan_genus_node_ages.csv\"\n",
        "SYNTENY_CACHE_TSV    = CACHE_DIR / \"synteny_location_cache.tsv\"\n",
        "\n",
        "# --- EPHEMERAL RUN-SPECIFIC FILES ---\n",
        "RUN_LOG_FILE         = RUN_DIR / f\"exonmapper_{RUN_TIMESTAMP}.log\"\n",
        "WORKING_SNAPSHOT     = RUN_DIR / \"working_snapshot.tsv\"\n",
        "MAPPED_SNAPSHOT      = RUN_DIR / \"raw_exons_this_run.tsv\"\n",
        "CONSENSUS_LONG_TSV   = RUN_DIR / \"consensus_long.tsv\"\n",
        "CONSENSUS_TABLE_TSV  = RUN_DIR / \"consensus_table.tsv\"\n",
        "EVOLUTION_EVENTS_TSV = RUN_DIR / \"exon_evolution_events.tsv\"\n",
        "WIDE_ARCH_TSV        = RUN_DIR / \"exon_wide.tsv\"\n",
        "ENTROPY_TSV          = RUN_DIR / \"entropy_stats.tsv\"\n",
        "RESCUE_LOG_TSV       = RUN_DIR / \"rescue_log.tsv\"\n",
        "RESCUE_HITS_TSV      = RUN_DIR / \"rex_hits.tsv\"\n",
        "RESCUE_CHAINS_TSV    = RUN_DIR / \"rex_chains.tsv\"\n",
        "ERROR_REPORT_PATH    = RUN_DIR / \"error_analysis_report.json\"\n",
        "SESSION_REJECTED_PATH= RUN_DIR / f\"rejected_ids_{RUN_ID}.txt\"\n",
        "RUN_MANIFEST_JSON    = RUN_DIR / \"manifest.json\"\n",
        "\n",
        "# --- REFERENCES (SOFT DEPENDENCIES) ---\n",
        "DRIVE_ARCHITECTURES_PATH = DICT_DIR / \"GeneFamily/collagens/COLLAGEN_GXY_REPEAT_STRUCTURE.json\"\n",
        "DRIVE_METAZOAN_TREE_PATH = DICT_DIR / \"multicellular animals_genus.nwk\"\n",
        "DRIVE_TAXONOMY_PATH      = TAXO_DIR / \"ete3_ncbi_taxa.sqlite\"\n",
        "\n",
        "# --- INITIALIZATION ---\n",
        "# Create directories (idempotent)\n",
        "for p in [RUN_DIR, CACHE_DIR, RUNS_ARCHIVE_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Configure logging to both console and a run-specific file\n",
        "# Remove any existing handlers to prevent duplicate logs in re-runs\n",
        "for h in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(h)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
        "    handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler(RUN_LOG_FILE)]\n",
        ")\n",
        "logger = logging.getLogger(\"exon-mapper\")\n",
        "\n",
        "logger.info(\"Two-root path model initialised successfully.\")\n",
        "logger.info(f\"WORK_ROOT (ephemeral outputs): {WORK_ROOT}\")\n",
        "logger.info(f\"SHARED_ROOT (durable data): {SHARED_ROOT}\")\n",
        "logger.info(f\"RUN_DIR (this run's outputs): {RUN_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fREBRMXN8NLt"
      },
      "source": [
        "## Cell 13 â€“ Manifest & Taxonomy Engine Setup\n",
        "\n",
        "This cell provides a helper function to write a JSON manifest for run provenance and initializes the optional taxonomy engine. The manifest is written once at the beginning of the run and can be updated at the end with final file hashes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKSgxwYp8Pbw"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 13 =====\n",
        "# Manifest helper function & optional taxonomy engine\n",
        "\n",
        "def sha256_file(p: Path) -> str:\n",
        "    \"\"\"Computes SHA256 hash of a file, returns empty string if not found.\"\"\"\n",
        "    if not p.is_file():\n",
        "        return \"\"\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, 'rb') as f:\n",
        "        # Read in chunks to handle large files\n",
        "        for chunk in iter(lambda: f.read(65536), b''):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def write_run_manifest(extra: dict = None, final_files: list[Path] = None) -> None:\n",
        "    \"\"\"\n",
        "    Writes/updates a JSON manifest for the run, including parameters and file hashes.\n",
        "    \"\"\"\n",
        "    manifest_data = {}\n",
        "    # If manifest exists, load it to update it\n",
        "    if RUN_MANIFEST_JSON.exists():\n",
        "        try:\n",
        "            with open(RUN_MANIFEST_JSON, 'r') as f:\n",
        "                manifest_data = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            logger.warning(\"Could not parse existing manifest; creating a new one.\")\n",
        "            manifest_data = {}\n",
        "\n",
        "    # Initial data\n",
        "    if not manifest_data:\n",
        "        manifest_data = {\n",
        "            \"run_id\": RUN_ID,\n",
        "            \"timestamp_utc\": RUN_TIMESTAMP,\n",
        "            \"python_version\": sys.version.split()[0],\n",
        "            \"parameters\": {\n",
        "                \"PROJECT_NAME\": PROJECT_NAME,\n",
        "                \"USER_EMAIL\": USER_EMAIL,\n",
        "                \"PROCESS_ALL_GENES\": PROCESS_ALL_GENES,\n",
        "                \"GENE_SYMBOLS\": GENE_SYMBOLS,\n",
        "                \"TAXONOMIC_FILTER_NAME\": TAXONOMIC_FILTER_NAME,\n",
        "                \"TARGET_TAXID\": TARGET_TAXID,\n",
        "                \"MIN_LEN_AA\": MIN_LEN_AA,\n",
        "                \"MIN_GXY_TRIPLETS\": MIN_GXY_TRIPLETS,\n",
        "                \"DEBUG_SAMPLE_SIZE\": DEBUG_SAMPLE_SIZE,\n",
        "            },\n",
        "            \"paths\": {\n",
        "                \"run_dir\": str(RUN_DIR),\n",
        "                \"cache_dir\": str(CACHE_DIR),\n",
        "            }\n",
        "        }\n",
        "\n",
        "    if extra:\n",
        "        manifest_data.update(extra)\n",
        "\n",
        "    if final_files:\n",
        "        manifest_data['output_files'] = [\n",
        "            {\"name\": p.name, \"path\": str(p), \"sha256\": sha256_file(p)}\n",
        "            for p in final_files\n",
        "        ]\n",
        "\n",
        "    with open(RUN_MANIFEST_JSON, \"w\") as f:\n",
        "        json.dump(manifest_data, f, indent=2)\n",
        "    logger.info(f\"Run manifest written/updated â†’ {RUN_MANIFEST_JSON}\")\n",
        "\n",
        "# Write the initial manifest for this run\n",
        "write_run_manifest()\n",
        "\n",
        "# --- (Optional) Taxonomy Engine ---\n",
        "class NCBITaxonomyEngine:\n",
        "    def __init__(self, enabled: bool, ncbi_db_path: Optional[Path] = None):\n",
        "        self._ok = False\n",
        "        self._source = \"uniprot_lineage_only\"\n",
        "        self.ncbi=None\n",
        "        if not enabled: return\n",
        "        try:\n",
        "            from ete3 import NCBITaxa as _ETE_NCBITaxa  # type: ignore\n",
        "            if ncbi_db_path and ncbi_db_path.exists():\n",
        "                self.ncbi = _ETE_NCBITaxa(dbfile=str(ncbi_db_path))\n",
        "                self._source = f\"ete3:{ncbi_db_path}\"\n",
        "            else:\n",
        "                self.ncbi = _ETE_NCBITaxa()\n",
        "                self._source = \"ete3:default\"\n",
        "            # Test query\n",
        "            _ = self.ncbi.get_lineage(1)\n",
        "            self._ok = True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"ETE3 taxonomy engine disabled or unavailable: {e}\")\n",
        "\n",
        "    def ok(self) -> bool: return self._ok\n",
        "\n",
        "tax_engine = NCBITaxonomyEngine(USE_TAXONOMY_ENGINE, DRIVE_TAXONOMY_PATH)\n",
        "logger.info(f\"Taxonomy mode: {'ETE3' if tax_engine.ok() else 'UniProt-lineage only'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBVDw1XkL67a"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 13 =====\n",
        "# Manifest helper function & optional taxonomy engine\n",
        "\n",
        "def safe_read_tsv(p: Path) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Safely reads a TSV file, returning an empty DataFrame if the file\n",
        "    does not exist or an error occurs, preventing pipeline crashes.\n",
        "    \"\"\"\n",
        "    if not p.exists():\n",
        "        logger.warning(f\"Cache file not found: {p.name}. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "    try:\n",
        "        return pd.read_csv(p, sep='\\t', low_memory=False)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to read {p.name}: {e}. Returning empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def sha256_file(p: Path) -> str:\n",
        "    \"\"\"Computes SHA256 hash of a file, returns empty string if not found.\"\"\"\n",
        "    if not p.is_file():\n",
        "        return \"\"\n",
        "    h = hashlib.sha256()\n",
        "    with open(p, 'rb') as f:\n",
        "        for chunk in iter(lambda: f.read(65536), b''):\n",
        "            h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def write_run_manifest(extra: dict = None, final_files: list[Path] = None) -> None:\n",
        "    \"\"\"\n",
        "    Writes/updates a JSON manifest for the run, including parameters and file hashes.\n",
        "    \"\"\"\n",
        "    manifest_data = {}\n",
        "    if RUN_MANIFEST_JSON.exists():\n",
        "        try:\n",
        "            with open(RUN_MANIFEST_JSON, 'r') as f:\n",
        "                manifest_data = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            logger.warning(\"Could not parse existing manifest; creating a new one.\")\n",
        "            manifest_data = {}\n",
        "\n",
        "    if not manifest_data:\n",
        "        manifest_data = {\n",
        "            \"run_id\": RUN_ID,\n",
        "            \"timestamp_utc\": RUN_TIMESTAMP,\n",
        "            \"python_version\": sys.version.split(),\n",
        "            \"parameters\": {\n",
        "                \"PROJECT_NAME\": PROJECT_NAME,\n",
        "                \"USER_EMAIL\": USER_EMAIL,\n",
        "                \"PROCESS_ALL_GENES\": PROCESS_ALL_GENES,\n",
        "                \"GENE_SYMBOLS\": GENE_SYMBOLS,\n",
        "                \"TAXONOMIC_FILTER_NAME\": TAXONOMIC_FILTER_NAME,\n",
        "                \"TARGET_TAXID\": TARGET_TAXID,\n",
        "                \"MIN_LEN_AA\": MIN_LEN_AA,\n",
        "                \"MIN_GXY_TRIPLETS\": MIN_GXY_TRIPLETS,\n",
        "                \"DEBUG_SAMPLE_SIZE\": DEBUG_SAMPLE_SIZE,\n",
        "            },\n",
        "            \"paths\": { \"run_dir\": str(RUN_DIR), \"cache_dir\": str(CACHE_DIR) }\n",
        "        }\n",
        "\n",
        "    if extra:\n",
        "        manifest_data.update(extra)\n",
        "\n",
        "    if final_files:\n",
        "        manifest_data['output_files'] = [\n",
        "            {\"name\": p.name, \"path\": str(p), \"sha256\": sha256_file(p)}\n",
        "            for p in final_files\n",
        "        ]\n",
        "\n",
        "    with open(RUN_MANIFEST_JSON, \"w\") as f:\n",
        "        json.dump(manifest_data, f, indent=2)\n",
        "    logger.info(f\"Run manifest written/updated â†’ {RUN_MANIFEST_JSON}\")\n",
        "\n",
        "# Write the initial manifest for this run\n",
        "write_run_manifest()\n",
        "\n",
        "# --- (Optional) Taxonomy Engine ---\n",
        "class NCBITaxonomyEngine:\n",
        "    def __init__(self, enabled: bool, ncbi_db_path: Optional[Path] = None):\n",
        "        self._ok = False\n",
        "        self._source = \"uniprot_lineage_only\"\n",
        "        self.ncbi=None\n",
        "        if not enabled: return\n",
        "        try:\n",
        "            from ete3 import NCBITaxa as _ETE_NCBITaxa  # type: ignore\n",
        "            if ncbi_db_path and ncbi_db_path.exists():\n",
        "                self.ncbi = _ETE_NCBITaxa(dbfile=str(ncbi_db_path))\n",
        "                self._source = f\"ete3:{ncbi_db_path}\"\n",
        "            else:\n",
        "                self.ncbi = _ETE_NCBITaxa()\n",
        "                self._source = \"ete3:default\"\n",
        "            _ = self.ncbi.get_lineage(1)\n",
        "            self._ok = True\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"ETE3 taxonomy engine disabled or unavailable: {e}\")\n",
        "\n",
        "    def ok(self) -> bool: return self._ok\n",
        "\n",
        "tax_engine = NCBITaxonomyEngine(USE_TAXONOMY_ENGINE, DRIVE_TAXONOMY_PATH)\n",
        "logger.info(f\"Taxonomy mode: {'ETE3' if tax_engine.ok() else 'UniProt-lineage only'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSsvvdEBzfab"
      },
      "source": [
        "# **Part 1A: (Optional) Phylogenetic Cache Generation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waFMQWphzk62"
      },
      "source": [
        "## Cell 15 â€“ Rebuild Node Age Cache from Newick Tree\n",
        "\n",
        "This cell contains the logic to traverse the entire Newick tree (`metazoan_genus_timtree.nwk`) and calculate the age (distance from the root) for every node. The output is a CSV file that maps node names and NCBI TaxIDs to their ages in millions of years.\n",
        "\n",
        "**This is a slow, one-time operation.** It should only be run if the `REGENERATE_NODE_AGE_CACHE` flag in Cell 11 is set to `True`, which is typically only necessary after updating the source Newick tree. All subsequent cells will use the generated CSV for fast age lookups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE5zbaE-8kR6"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 15 =====\n",
        "# Rebuild node age cache from Newick file (run-on-demand)\n",
        "\n",
        "if REGENERATE_NODE_AGE_CACHE:\n",
        "    logger.info(\"â–¶ï¸ REGENERATING NODE AGE CACHE. This may take several minutes.\")\n",
        "\n",
        "    if not DRIVE_METAZOAN_TREE_PATH.exists():\n",
        "        logger.error(f\"CRITICAL: Cannot find Newick tree at {DRIVE_METAZOAN_TREE_PATH}.\")\n",
        "        logger.warning(\"Aborting node age cache regeneration. The notebook will continue, \"\n",
        "                       \"but phylogenetic features may be disabled or use uniform weights.\")\n",
        "        # This is no longer a fatal error; execution continues.\n",
        "    else:\n",
        "        try:\n",
        "            from ete3 import Tree\n",
        "\n",
        "            def generate_node_age_cache(nwk_path: Path, output_csv: Path):\n",
        "                \"\"\"\n",
        "                Traverses a Newick tree to calculate and save the age of each node.\n",
        "                \"\"\"\n",
        "                logger.info(f\"Loading tree from: {nwk_path}\")\n",
        "                tree = Tree(str(nwk_path), format=1)\n",
        "\n",
        "                # The root has no distance to itself, so its age is 0\n",
        "                tree.dist = 0\n",
        "\n",
        "                node_data = []\n",
        "\n",
        "                # Traverse the tree from root to leaves\n",
        "                # Note: The input tree branch lengths should be in 'support'\n",
        "                for node in tqdm(tree.traverse(\"preorder\"), desc=\"Calculating node ages\"):\n",
        "                    # Calculate age of children based on parent's age + branch length\n",
        "                    parent_age = node.dist\n",
        "                    for child in node.children:\n",
        "                        child.dist = parent_age + child.support\n",
        "\n",
        "                    # Store data for the current node\n",
        "                    taxid = None\n",
        "                    try:\n",
        "                        # ETE3 often stores taxid in the name like 'Genus_name_9606'\n",
        "                        if '_' in node.name and node.name.split('_')[-1].isdigit():\n",
        "                            taxid = int(node.name.split('_')[-1])\n",
        "                    except (ValueError, IndexError):\n",
        "                        pass\n",
        "\n",
        "                    node_data.append({\n",
        "                        \"node_name\": node.name,\n",
        "                        \"age\": node.dist,\n",
        "                        \"taxid\": taxid\n",
        "                    })\n",
        "\n",
        "                df_ages = pd.DataFrame(node_data)\n",
        "                df_ages.to_csv(output_csv, index=False)\n",
        "                logger.info(f\"âœ… Successfully generated and saved node age cache to: {output_csv}\")\n",
        "\n",
        "            # Execute the function\n",
        "            generate_node_age_cache(DRIVE_METAZOAN_TREE_PATH, NODE_AGES_CSV_PATH)\n",
        "\n",
        "        except ImportError:\n",
        "            logger.error(\"`ete3` is required for this step. Please ensure it is installed.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"An error occurred during cache generation: {e}\")\n",
        "\n",
        "else:\n",
        "    logger.info(\"â˜‘ï¸ Skipping node age cache regeneration (flag is False).\")\n",
        "    if not NODE_AGES_CSV_PATH.exists():\n",
        "        logger.warning(f\"Node age cache not found at {NODE_AGES_CSV_PATH}. \"\n",
        "                       \"The dating engine may not work. \"\n",
        "                       \"Consider setting REGENERATE_NODE_AGE_CACHE to True.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltq6IsD38gAG"
      },
      "source": [
        "# **Part 2: Data Loading & Pre-processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8O5lk9oJ8iei"
      },
      "source": [
        "## Cell 21 â€“ Unified Data Import, Normalization, and Healing\n",
        "\n",
        "This cell is the primary data entry point for the pipeline. It loads data from multiple sources and applies a **comprehensive, multi-stage normalization routine restored from previous, robust versions.** This includes:\n",
        "1.  **Data Healing:** Proactively reconstructs the essential `'Organism (ID)'` column if it is missing from older cache files.\n",
        "2.  **Variant Identification:** Identifies and separates gene variants (e.g., `COL1A1A`, `COL1A1_L`) into dedicated columns.\n",
        "3.  **Robust Classification:** Uses a tiered regex system to infer gene symbols from protein names and classify probable but un-named collagens.\n",
        "\n",
        "The result is a clean, reliable, and fully annotated dataset that is then filtered to create the `working_df` for the current run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldLSkG0SMamT"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 21 =====\n",
        "# Unified import, normalization, and variant handling (with Data Healing & Restored Logic)\n",
        "\n",
        "from Bio import SeqIO\n",
        "\n",
        "# --- Restored, more robust normalization helper functions ---\n",
        "def roman_to_int(s: str) -> int:\n",
        "    roman_map = {'I': 1, 'V': 5, 'X': 10}; val = 0; s = s.upper()\n",
        "    for i in range(len(s)):\n",
        "        if i > 0 and roman_map.get(s[i], 0) > roman_map.get(s[i - 1], 0):\n",
        "            val += roman_map.get(s[i], 0) - 2 * roman_map.get(s[i - 1], 0)\n",
        "        else: val += roman_map.get(s[i], 0)\n",
        "    return val\n",
        "\n",
        "def infer_specific_collagen_symbol(name: str) -> Optional[str]:\n",
        "    if not isinstance(name, str) or 'COLLAGEN' not in name.upper(): return None\n",
        "    match = re.search(r'ALPHA[ -]?(\\d+)\\s*\\((.*?)\\)|TYPE\\s+([IVXLCDM]+).*?ALPHA\\s+(\\d+)', name.upper())\n",
        "    if not match: return None\n",
        "    chain_num = match.group(1) or match.group(4); type_roman = match.group(2) or match.group(3)\n",
        "    if not chain_num or not type_roman: return None\n",
        "    try:\n",
        "        type_roman_cleaned = re.sub(r'[^IVXLCDM]', '', type_roman)\n",
        "        if not type_roman_cleaned: return None\n",
        "        return f\"COL{roman_to_int(type_roman_cleaned)}A{chain_num}\"\n",
        "    except (KeyError, IndexError): return None\n",
        "\n",
        "RX_GXY = re.compile(r\"(?i)\\b(g(?:ly)?\\s*[- ]?\\s*x\\s*[- ]?\\s*y\\b|gxy\\b)\")\n",
        "def classify_as_probable_collagen(name: str) -> bool:\n",
        "    if not isinstance(name, str): return False\n",
        "    if 'COLLAGEN' in name.upper() and RX_GXY.search(name) and not re.search(r'RECEPTOR|BINDING|ASE', name.upper()):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def consolidated_gene_normalization(row: pd.Series) -> pd.Series:\n",
        "    \"\"\"Applies the full, robust normalization logic to a DataFrame row.\"\"\"\n",
        "    primary_name = row.get('Gene Names (primary)', '')\n",
        "    protein_names = row.get('Protein names', '')\n",
        "    base_gene, variant_type, note = \"UNKNOWN\", \"\", \"\"\n",
        "\n",
        "    if isinstance(primary_name, str) and primary_name.strip():\n",
        "        # --- FIX: Get the first word, then convert to upper case ---\n",
        "        words = primary_name.split()\n",
        "        if words: # Ensure the list is not empty\n",
        "            first_word = words[0].upper()\n",
        "\n",
        "            variant_match = re.search(r'^(COL\\d+A\\d+)(?:_([A-Z0-9_]+)|([A-Z]))$', first_word)\n",
        "            if variant_match:\n",
        "                base_gene = variant_match.group(1)\n",
        "                variant = next((v for v in variant_match.groups()[1:] if v is not None), \"\")\n",
        "                return pd.Series([base_gene, variant, f\"gene_variant_{variant}\"], index=['gene_symbol_norm', 'variant_type', 'species_note'])\n",
        "\n",
        "            if re.match(r'^COL\\d+A\\d+$', first_word):\n",
        "                base_gene = first_word\n",
        "\n",
        "    if base_gene == 'UNKNOWN':\n",
        "        inferred = infer_specific_collagen_symbol(protein_names)\n",
        "        if inferred:\n",
        "            base_gene = inferred\n",
        "\n",
        "    if base_gene == 'UNKNOWN' and classify_as_probable_collagen(protein_names):\n",
        "        base_gene = 'PROBABLE_COLLAGEN'\n",
        "\n",
        "    return pd.Series([base_gene, variant_type, note], index=['gene_symbol_norm', 'variant_type', 'species_note'])\n",
        "\n",
        "def load_new_inputs_from_content() -> pd.DataFrame:\n",
        "    \"\"\"Scans /content/ for new FASTA or TSV.GZ files and loads them.\"\"\"\n",
        "    loaded_dfs = []\n",
        "    content_dir = Path(\"/content\").resolve()\n",
        "\n",
        "    def parse_fasta_header(record, file_path):\n",
        "        h, e = record.description, record.id\n",
        "        gn = re.search(r\"GN=([^ ]+)\", h); os = re.search(r\"OS=([^(]+)\", h)\n",
        "        gene = gn.group(1) if gn else \"\"\n",
        "        org = os.group(1).strip().replace('_', ' ') if os else \"\"\n",
        "        return {'Entry':e, 'Gene Names (primary)':gene, 'Organism':org, 'Sequence':str(record.seq), 'Reviewed':'unreviewed', 'source_file':file_path.name}\n",
        "\n",
        "    for p in content_dir.glob(\"*.fasta\"):\n",
        "        logger.info(f\"Found FASTA for dynamic loading: {p.name}\")\n",
        "        recs = [parse_fasta_header(rec, p) for rec in SeqIO.parse(p, \"fasta\")]\n",
        "        loaded_dfs.append(pd.DataFrame([r for r in recs if r]))\n",
        "\n",
        "    for p in content_dir.glob(\"uniprot*.tsv.gz\"):\n",
        "        logger.info(f\"Found TSV.GZ for dynamic loading: {p.name}\")\n",
        "        try:\n",
        "            df = pd.read_csv(p, sep='\\t', compression='gzip', low_memory=False)\n",
        "            df['source_file'] = p.name\n",
        "            loaded_dfs.append(df)\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Could not load TSV {p.name}: {e}\")\n",
        "\n",
        "    if not loaded_dfs:\n",
        "        return pd.DataFrame()\n",
        "    return pd.concat(loaded_dfs, ignore_index=True)\n",
        "\n",
        "# --- Main Data Loading and Processing Logic ---\n",
        "logger.info(\"--- Starting Unified Data Import, Normalization, and Healing ---\")\n",
        "\n",
        "df_master = safe_read_tsv(MASTER_TSV_PATH)\n",
        "df_new = load_new_inputs_from_content()\n",
        "full_df = pd.concat([df_master, df_new], ignore_index=True)\n",
        "full_df.drop_duplicates(subset=['Entry'], keep='last', inplace=True)\n",
        "\n",
        "if 'Organism (ID)' not in full_df.columns:\n",
        "    logger.warning(\"Column 'Organism (ID)' not found. Attempting to heal from legacy columns.\")\n",
        "    if 'Organism' in full_df.columns and 'Organism ID' in full_df.columns:\n",
        "        full_df['Organism (ID)'] = full_df['Organism'].astype(str) + \" (\" + full_df['Organism ID'].astype(str) + \")\"\n",
        "        logger.info(\"Successfully reconstructed 'Organism (ID)'.\")\n",
        "\n",
        "logger.info(f\"Normalizing gene symbols for {len(full_df)} entries...\")\n",
        "tqdm.pandas(desc=\"Normalizing Gene Symbols\")\n",
        "norm_cols = full_df.progress_apply(consolidated_gene_normalization, axis=1)\n",
        "full_df[['gene_symbol_norm', 'variant_type', 'species_note']] = norm_cols\n",
        "\n",
        "target_genes = [s.strip().upper() for s in GENE_SYMBOLS.split(',')]\n",
        "if PROCESS_ALL_GENES:\n",
        "    working_df = full_df.copy()\n",
        "    logger.info(\"Processing all identified collagen genes.\")\n",
        "else:\n",
        "    working_df = full_df[full_df['gene_symbol_norm'].isin(target_genes)].copy()\n",
        "    logger.info(f\"Filtering for specific genes: {target_genes}\")\n",
        "\n",
        "if 'Taxonomic lineage (Ids)' in working_df.columns:\n",
        "    mask = working_df['Taxonomic lineage (Ids)'].astype(str).str.contains(f\"\\\\b{TARGET_TAXID}\\\\b\")\n",
        "    working_df = working_df[mask].copy()\n",
        "    logger.info(f\"Filtered for Taxon ID {TARGET_TAXID} ({TAXONOMIC_FILTER_NAME}), {len(working_df)} entries remaining.\")\n",
        "\n",
        "if DEBUG_SAMPLE_SIZE > 0:\n",
        "    logger.warning(f\"DEBUG MODE: Sampling down to {DEBUG_SAMPLE_SIZE} entries.\")\n",
        "    working_df = working_df.sample(n=min(DEBUG_SAMPLE_SIZE, len(working_df)), random_state=42)\n",
        "\n",
        "working_df.to_csv(WORKING_SNAPSHOT, sep='\\t', index=False)\n",
        "logger.info(f\"âœ… Unified import complete. Working set has {len(working_df)} entries.\")\n",
        "logger.info(f\"Snapshot for this run saved to: {WORKING_SNAPSHOT.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7AOpOsQPGv-"
      },
      "source": [
        "## Cell 21A â€“ Diagnostic Check for Data Normalization\n",
        "\n",
        "This cell provides a crucial verification step to inspect the output of the complex normalization and classification logic in Cell 21. It generates a summary profile of the `working_df`, allowing for a quick assessment of whether gene symbols and variants were processed correctly before the pipeline proceeds to the more intensive mapping and analysis stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BRguOz_PFzp"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 21A =====\n",
        "# Diagnostic Check for Data Normalization\n",
        "\n",
        "if 'working_df' in globals() and not working_df.empty:\n",
        "    logger.info(\"--- ðŸ“Š DIAGNOSTIC PROFILE of Cell 21 Output ---\")\n",
        "\n",
        "    # 1. Profile the main output: gene_symbol_norm\n",
        "    logger.info(\"\\n[1] Distribution of Normalized Gene Symbols in `working_df`:\")\n",
        "    gene_counts = working_df['gene_symbol_norm'].value_counts()\n",
        "    with pd.option_context('display.max_rows', 20):\n",
        "        display(gene_counts)\n",
        "\n",
        "    # 2. Profile the variant/paralog identification\n",
        "    if 'variant_type' in working_df.columns:\n",
        "        logger.info(\"\\n[2] Profile of Identified Gene Variants/Paralogs:\")\n",
        "        variant_counts = working_df['variant_type'].dropna().value_counts()\n",
        "        if not variant_counts.empty:\n",
        "            display(variant_counts)\n",
        "        else:\n",
        "            logger.info(\"   -> No variants with suffixes were identified in the working set.\")\n",
        "\n",
        "    # 3. Display a sample of the key columns to visually inspect the results\n",
        "    logger.info(\"\\n[3] Sample of DataFrame showing key normalization columns:\")\n",
        "    display_cols = [\n",
        "        'Entry',\n",
        "        'Organism',\n",
        "        'Gene Names (primary)',\n",
        "        'Protein names',\n",
        "        'gene_symbol_norm', # The final base gene\n",
        "        'variant_type',     # The extracted variant\n",
        "        'species_note'\n",
        "    ]\n",
        "    # Ensure all columns exist before trying to display them\n",
        "    final_display_cols = [col for col in display_cols if col in working_df.columns]\n",
        "    display(working_df[final_display_cols].head(10))\n",
        "\n",
        "else:\n",
        "    logger.warning(\"`working_df` is empty. Cannot generate a diagnostic profile.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzUMU-DOnwgp"
      },
      "source": [
        "## Cell 22 â€“ Proactive Ensembl Gene ID Discovery (Hybrid Strategy)\n",
        "\n",
        "This cell enriches our dataset with high-quality Ensembl data using a robust, two-pass hybrid strategy.\n",
        "\n",
        "### Process\n",
        "\n",
        "1.  It builds a comprehensive map of all Ensembl gene family members.\n",
        "2.  **Pass 1 (Protein ID Link):** It first attempts to link UniProt entries to Ensembl data using the stable Ensembl Protein ID found in the cross-reference column. This is the most reliable method.\n",
        "3.  **Pass 2 (Species/Gene Name Link):** For any entries that could not be matched via Protein ID, it falls back to a second attempt, matching on the combination of the species name and the normalized gene symbol.\n",
        "4.  The results are coalesced, creating the final `ensembl_id` and `ensembl_genus` columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjgRBrKFbEh0"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 22 =====\n",
        "# Proactive Ensembl Gene ID Discovery via Homology API (Hybrid Strategy)\n",
        "\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Optional, Dict, List, Set, Any, Tuple\n",
        "\n",
        "# --- Data Structures and API Client ---\n",
        "@dataclass\n",
        "class GeneResult:\n",
        "    \"\"\"Container for an Ensembl homology member.\"\"\"\n",
        "    ensembl_gene_id: str\n",
        "    gene_symbol: str\n",
        "    species: str\n",
        "    ensembl_protein_id: str\n",
        "    is_paralog: bool = False\n",
        "\n",
        "\n",
        "class EnsemblClient:\n",
        "    \"\"\"\n",
        "    Robust, rate-limited client for the Ensembl REST API with retry logic.\n",
        "    Notes\n",
        "    -----\n",
        "    * Treats HTTP 400 as a 'clean miss' (e.g., symbol not defined in species).\n",
        "    * Retries on 429/5xx with backoff; respects a minimal inter-request delay.\n",
        "    \"\"\"\n",
        "    def __init__(self, rate_limit_delay: float = 0.1) -> None:\n",
        "        self.base_url = \"https://rest.ensembl.org\"\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\"Accept\": \"application/json\"})\n",
        "        self.last_request_time = 0.0\n",
        "        self.rate_limit_delay = rate_limit_delay\n",
        "\n",
        "    def get(self, endpoint: str, params: Optional[Dict[str, Any]] = None) -> Optional[requests.Response]:\n",
        "        \"\"\"Perform a rate-limited GET request with bounded retries; return None on definitive failure.\"\"\"\n",
        "        # Respect a minimal delay between requests\n",
        "        elapsed = time.time() - self.last_request_time\n",
        "        if elapsed < self.rate_limit_delay:\n",
        "            time.sleep(self.rate_limit_delay - elapsed)\n",
        "\n",
        "        for attempt in range(3):\n",
        "            try:\n",
        "                full_url = self.base_url + endpoint\n",
        "                r = self.session.get(full_url, params=params, timeout=30)\n",
        "                self.last_request_time = time.time()\n",
        "\n",
        "                # Explicit handling of rate limiting\n",
        "                if r.status_code == 429:\n",
        "                    retry_after = int(r.headers.get(\"Retry-After\", 2))\n",
        "                    logger.warning(f\"Rate limit hit on {endpoint}. Retrying after {retry_after}s...\")\n",
        "                    time.sleep(retry_after)\n",
        "                    continue\n",
        "\n",
        "                # Treat 400 as a definite miss (e.g., bad symbol for species)\n",
        "                if r.status_code == 400:\n",
        "                    logger.error(f\"HTTP 400 for {full_url} (likely undefined symbol/species combo); skipping.\")\n",
        "                    return None\n",
        "\n",
        "                r.raise_for_status()\n",
        "                return r\n",
        "\n",
        "            except requests.RequestException as e:\n",
        "                logger.error(f\"Ensembl API request failed on attempt {attempt + 1}: {e}\")\n",
        "                if attempt < 2:\n",
        "                    time.sleep(2 ** attempt)  # backoff\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class EnsemblGeneTreeFinder:\n",
        "    \"\"\"Discover gene family members using the Ensembl Homology API.\"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        self.client = EnsemblClient()\n",
        "\n",
        "    def discover_family_members(self, seed_symbol: str, seed_species: str = \"homo_sapiens\") -> List[GeneResult]:\n",
        "        \"\"\"\n",
        "        Query Ensembl for orthologues and paralogues of a seed gene.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        seed_symbol : str\n",
        "            HGNC-like gene symbol (e.g., 'COL1A1').\n",
        "        seed_species : str\n",
        "            Ensembl species name (default 'homo_sapiens').\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[GeneResult]\n",
        "            Zero or more family members discovered for the seed.\n",
        "        \"\"\"\n",
        "        endpoint = f\"/homology/symbol/{seed_species}/{seed_symbol}\"\n",
        "        params = {\"content-type\": \"application/json\", \"sequence\": \"protein\", \"type\": \"all\", \"format\": \"full\"}\n",
        "\n",
        "        logger.info(f\"Discovering family for '{seed_symbol}'...\")\n",
        "        r = self.client.get(endpoint, params=params)\n",
        "        if not r:\n",
        "            logger.error(f\"Failed to retrieve data for {seed_symbol}.\")\n",
        "            return []\n",
        "\n",
        "        payload = r.json()\n",
        "        data = payload.get(\"data\", [])\n",
        "        if not data:\n",
        "            logger.warning(f\"No homology data for {seed_symbol}.\")\n",
        "            return []\n",
        "\n",
        "        results: List[GeneResult] = []\n",
        "        seen: Set[str] = set()\n",
        "\n",
        "        for group in data:\n",
        "            for homology in group.get(\"homologies\", []):\n",
        "                # capture both source and target entries\n",
        "                for side in (\"source\", \"target\"):\n",
        "                    member = homology.get(side, {}) or {}\n",
        "                    pid = member.get(\"protein_id\")\n",
        "                    if not pid:\n",
        "                        continue\n",
        "                    # safeguard for pathological objects\n",
        "                    if member.get(\"cigar_line\") is None:\n",
        "                        continue\n",
        "                    if pid in seen:\n",
        "                        continue\n",
        "\n",
        "                    species_raw = member.get(\"species\", \"unknown\")\n",
        "                    # Normalise species to Ensembl style 'genus_species'\n",
        "                    species_norm = str(species_raw).replace(\" \", \"_\").lower()\n",
        "\n",
        "                    results.append(\n",
        "                        GeneResult(\n",
        "                            ensembl_gene_id=member.get(\"id\", \"\"),\n",
        "                            gene_symbol=member.get(\"symbol\", seed_symbol),\n",
        "                            species=species_norm,\n",
        "                            ensembl_protein_id=pid,\n",
        "                            is_paralog=(\"paralog\" in str(homology.get(\"type\", \"\")).lower()),\n",
        "                        )\n",
        "                    )\n",
        "                    seen.add(pid)\n",
        "\n",
        "        logger.info(f\"   Discovered {len(results)} members for {seed_symbol}.\")\n",
        "        return results\n",
        "\n",
        "\n",
        "def _normalise_species_from_organism(organism: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert UniProt 'Organism' field to Ensembl-like 'genus_species' key.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    'Homo sapiens (Human)' -> 'homo_sapiens'\n",
        "    'Gallus gallus (Chicken)' -> 'gallus_gallus'\n",
        "    \"\"\"\n",
        "    s = str(organism)\n",
        "    # Drop parenthetical synonyms, trailing/leading whitespace\n",
        "    s = re.sub(r\"\\s*\\(.*?\\)\", \"\", s).strip()\n",
        "    # Collapse multiple spaces and convert\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    parts = s.split(\" \")\n",
        "    if len(parts) >= 2:\n",
        "        s = (parts[0] + \"_\" + parts[1]).lower()\n",
        "    else:\n",
        "        s = s.replace(\" \", \"_\").lower()\n",
        "    return s\n",
        "\n",
        "\n",
        "# --- Main Logic ---\n",
        "if 'working_df' in globals() and isinstance(working_df, pd.DataFrame) and not working_df.empty:\n",
        "    logger.info(\"--- Starting Proactive Ensembl Gene ID Discovery (Hybrid Strategy) ---\")\n",
        "    finder = EnsemblGeneTreeFinder()\n",
        "\n",
        "    # Identify canonical collagen symbols from your normalised column\n",
        "    all_symbols = working_df['gene_symbol_norm'].dropna().astype(str).unique()\n",
        "    canonical_seeds = {re.sub(r'(?:_[A-Z0-9_]+|[A-Z])$', '', s) for s in all_symbols}\n",
        "    seed_genes = sorted([s for s in canonical_seeds if re.match(r'^COL\\d+A\\d+$', s)])\n",
        "\n",
        "    logger.info(f\"Identified {len(seed_genes)} canonical gene families to query.\")\n",
        "\n",
        "    # Discover across all seeds, skipping those that 400 (undefined in human)\n",
        "    all_discovered: List[GeneResult] = []\n",
        "    for gene_symbol in tqdm(seed_genes, desc=\"Discovering Gene Families\"):\n",
        "        all_discovered.extend(finder.discover_family_members(gene_symbol))\n",
        "\n",
        "    if all_discovered:\n",
        "        # Build DataFrame robustly from dicts to avoid column/index surprises\n",
        "        df_ensembl_map = pd.DataFrame([asdict(x) for x in all_discovered])\n",
        "\n",
        "        # Defensive normalisation of expected columns\n",
        "        expected_cols = {'ensembl_gene_id', 'gene_symbol', 'species', 'ensembl_protein_id', 'is_paralog'}\n",
        "        missing = expected_cols.difference(df_ensembl_map.columns)\n",
        "        if missing:\n",
        "            logger.error(f\"Missing expected columns in df_ensembl_map: {sorted(missing)}\")\n",
        "            # Create placeholders if truly absent (should not happen with asdict)\n",
        "            for col in missing:\n",
        "                df_ensembl_map[col] = pd.NA\n",
        "\n",
        "        # 1) Map ENSP -> ENSG\n",
        "        protein_to_gene_map: Dict[str, str] = {}\n",
        "        for row in df_ensembl_map.itertuples(index=False):\n",
        "            if getattr(row, 'ensembl_protein_id', None) and getattr(row, 'ensembl_gene_id', None):\n",
        "                protein_to_gene_map[row.ensembl_protein_id] = row.ensembl_gene_id\n",
        "\n",
        "        # 2) Map ENSP -> genus (first token of species)\n",
        "        protein_to_genus_map: Dict[str, str] = {}\n",
        "        for row in df_ensembl_map.itertuples(index=False):\n",
        "            pid = getattr(row, 'ensembl_protein_id', None)\n",
        "            sp = getattr(row, 'species', None)\n",
        "            if pid and sp:\n",
        "                genus = str(sp).split('_', 1)[0]\n",
        "                protein_to_genus_map[pid] = genus\n",
        "\n",
        "        # Ensure Cross-reference column exists\n",
        "        if 'Cross-reference (Ensembl)' not in working_df.columns:\n",
        "            logger.warning(\"'Cross-reference (Ensembl)' column not found. Creating it as empty.\")\n",
        "            working_df['Cross-reference (Ensembl)'] = \"\"\n",
        "\n",
        "        # Extract ENSP accessions from Cross-reference (Ensembl)\n",
        "        def extract_ensp(xref: Any) -> Optional[str]:\n",
        "            \"\"\"Return first ENSP-like token from a cross-reference field.\"\"\"\n",
        "            m = re.search(r'(ENSP\\d+)', str(xref))\n",
        "            return m.group(1) if m else None\n",
        "\n",
        "        working_df['ensembl_protein_id'] = working_df['Cross-reference (Ensembl)'].apply(extract_ensp)\n",
        "        working_df['ensembl_id_pass1'] = working_df['ensembl_protein_id'].map(protein_to_gene_map)\n",
        "        working_df['ensembl_genus_pass1'] = working_df['ensembl_protein_id'].map(protein_to_genus_map)\n",
        "\n",
        "        # Build species+gene_symbol -> ENSG and -> genus maps without set_index()\n",
        "        species_gene_to_id_map: Dict[Tuple[str, str], str] = {}\n",
        "        species_gene_to_genus_map: Dict[Tuple[str, str], str] = {}\n",
        "        for row in df_ensembl_map.itertuples(index=False):\n",
        "            sp = getattr(row, 'species', None)\n",
        "            gs = getattr(row, 'gene_symbol', None)\n",
        "            eg = getattr(row, 'ensembl_gene_id', None)\n",
        "            if sp and gs and eg:\n",
        "                species_gene_to_id_map[(sp, gs)] = eg\n",
        "                species_gene_to_genus_map[(sp, gs)] = str(sp).split('_', 1)[0]\n",
        "\n",
        "        # Derive a species_key in working_df compatible with Ensembl style\n",
        "        working_df['species_key'] = working_df['Organism'].apply(_normalise_species_from_organism)\n",
        "\n",
        "        # Second-pass matching on (species_key, gene_symbol_norm)\n",
        "        unmatched_mask = working_df['ensembl_id_pass1'].isna()\n",
        "        if unmatched_mask.any():\n",
        "            sp_keys = working_df.loc[unmatched_mask, 'species_key'].astype(str)\n",
        "            gene_syms = working_df.loc[unmatched_mask, 'gene_symbol_norm'].astype(str)\n",
        "            keys = list(zip(sp_keys, gene_syms))\n",
        "            working_df.loc[unmatched_mask, 'ensembl_id_pass2'] = [species_gene_to_id_map.get(k) for k in keys]\n",
        "            working_df.loc[unmatched_mask, 'ensembl_genus_pass2'] = [species_gene_to_genus_map.get(k) for k in keys]\n",
        "        else:\n",
        "            working_df['ensembl_id_pass2'] = pd.NA\n",
        "            working_df['ensembl_genus_pass2'] = pd.NA\n",
        "\n",
        "        # Consolidate passes\n",
        "        working_df['ensembl_id'] = working_df['ensembl_id_pass1'].fillna(working_df['ensembl_id_pass2'])\n",
        "        working_df['ensembl_genus'] = working_df['ensembl_genus_pass1'].fillna(working_df['ensembl_genus_pass2'])\n",
        "\n",
        "        # Clean up (be permissive)\n",
        "        working_df.drop(\n",
        "            columns=['ensembl_id_pass1', 'ensembl_genus_pass1', 'ensembl_id_pass2', 'ensembl_genus_pass2'],\n",
        "            inplace=True,\n",
        "            errors='ignore'\n",
        "        )\n",
        "\n",
        "        matched_count = int(working_df['ensembl_id'].notna().sum())\n",
        "        total = int(len(working_df))\n",
        "        percentage = (matched_count / total) * 100 if total > 0 else 0.0\n",
        "        logger.info(f\"âœ… Ensembl ID Discovery complete. Matched {matched_count}/{total} ({percentage:.1f}%) entries.\")\n",
        "    else:\n",
        "        logger.warning(\"No gene family members were discovered. Synteny check will be skipped.\")\n",
        "else:\n",
        "    logger.warning(\"Working dataframe is empty. Skipping Ensembl ID discovery.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-CPDv618q1o"
      },
      "source": [
        "## Cell 23 â€“ Gene Variant & Paralog Handling\n",
        "\n",
        "This cell processes the working dataset to handle gene variants, often found in teleost fish, which are indicated by suffixes like `_L`, `_S`, or `_1`, `_2`. It normalizes these gene symbols to a common base name and adds specific columns (`variant_type`, `species_note`) to track them for downstream analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ay7x3G1rK4Nr"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 23 =====\n",
        "# Gene Variant & Paralog Handling (Generalized)\n",
        "\n",
        "def handle_gene_variants(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Identifies and normalizes gene variants with suffixes.\n",
        "    Handles three patterns: COL1A1a, COL1A1_L, COL1A1_2, COL1A1_ISOX1\n",
        "    \"\"\"\n",
        "    if df.empty or 'gene_symbol_norm' not in df.columns:\n",
        "        return df\n",
        "\n",
        "    logger.info(\"ðŸ§¬ Processing gene variants/paralogs...\")\n",
        "    processed_df = df.copy()\n",
        "\n",
        "    # Regex to find any known variant suffix pattern\n",
        "    variant_mask = processed_df['gene_symbol_norm'].str.contains(r'(?:_[A-Z0-9_]+|[A-Z])$', na=False)\n",
        "\n",
        "    if not variant_mask.any():\n",
        "        logger.info(\"No gene variants with recognized suffixes detected.\")\n",
        "        return processed_df\n",
        "\n",
        "    variant_df = processed_df[variant_mask].copy()\n",
        "\n",
        "    # Extract the suffix as the variant type\n",
        "    variant_df['variant_type'] = variant_df['gene_symbol_norm'].str.extract(r'(?:_([A-Z0-9_]+)|([A-Z]))$').fillna('').sum(axis=1)\n",
        "    # Extract the base gene name\n",
        "    variant_df['base_gene'] = variant_df['gene_symbol_norm'].str.replace(r'(?:_[A-Z0-9_]+|[A-Z])$', '', regex=True)\n",
        "    # Create a descriptive note\n",
        "    variant_df['species_note'] = 'gene_variant_' + variant_df['variant_type']\n",
        "\n",
        "    # Update the gene symbol to the base form for consistent grouping\n",
        "    variant_df['gene_symbol_norm'] = variant_df['base_gene']\n",
        "\n",
        "    # Update the main dataframe\n",
        "    processed_df.update(variant_df)\n",
        "\n",
        "    # Add new columns if they don't exist\n",
        "    if 'variant_type' not in processed_df.columns: processed_df['variant_type'] = pd.NA\n",
        "    if 'species_note' not in processed_df.columns: processed_df['species_note'] = pd.NA\n",
        "\n",
        "    processed_df.loc[variant_mask, 'variant_type'] = variant_df['variant_type']\n",
        "    processed_df.loc[variant_mask, 'species_note'] = variant_df['species_note']\n",
        "    processed_df.loc[variant_mask, 'gene_symbol_norm'] = variant_df['gene_symbol_norm']\n",
        "\n",
        "    num_fixed = variant_mask.sum()\n",
        "    logger.info(f\"âœ… Gene variant processing complete: {num_fixed} mappings updated.\")\n",
        "    return processed_df\n",
        "\n",
        "if 'working_df' in globals() and not working_df.empty:\n",
        "    working_df = handle_gene_variants(working_df)\n",
        "    working_df.to_csv(WORKING_SNAPSHOT, sep='\\t', index=False)\n",
        "    logger.info(\"ðŸ“¸ Updated working snapshot with variant/paralog fixes.\")\n",
        "else:\n",
        "    logger.info(\"âš ï¸ No working dataset to process for variants/paralogs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC88BFmY8u3K"
      },
      "source": [
        "## Cell 24 â€“ Taxonomic Lineage Expansion & Genus Finalization\n",
        "\n",
        "This cell standardizes the taxonomic information for each entry. It parses UniProt lineage strings into standard ranks and creates composite keys for analysis.\n",
        "\n",
        "**Crucially, it now creates the final, authoritative `cluster_genus` column.** It prioritizes the genus name discovered from the Ensembl database (from Cell 22) as it is more standardized. If an Ensembl genus is not available, it falls back to parsing the genus from the UniProt `Organism` string. This ensures the most reliable possible genus is used for all downstream phylogenetic weighting and grouping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlV_QcbrICVE"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 24 =====\n",
        "# Taxonomic lineage expansion & authoritative genus finalization\n",
        "\n",
        "RANK_CODES = [\"King\",\"Phyl\",\"Clas\",\"Orde\",\"Fami\",\"Genu\",\"Spec\"]\n",
        "\n",
        "# ... [parse_taxonomic_lineage and _split_lineage_names functions remain the same] ...\n",
        "def parse_taxonomic_lineage(lineage_ids_str: str) -> Dict[str, Optional[int]]:\n",
        "    out = {k: None for k in RANK_CODES};\n",
        "    if not isinstance(lineage_ids_str, str): return out\n",
        "    ids = [int(x) for x in re.findall(r\"\\d+\", lineage_ids_str)];\n",
        "    if not ids: return out\n",
        "    tail = ids[-7:] if len(ids) >= 7 else [None]*(7-len(ids)) + ids\n",
        "    return {code: taxid for code, taxid in zip(RANK_CODES, tail)}\n",
        "\n",
        "def _split_lineage_names(s: str) -> List[str]:\n",
        "    if not isinstance(s, str) or not s.strip(): return []\n",
        "    return [p.strip() for p in re.split(r\"[;,]\\s*\", s) if p]\n",
        "\n",
        "if 'working_df' in globals() and not working_df.empty:\n",
        "    logger.info(\"Expanding taxonomic lineages and finalizing genus...\")\n",
        "\n",
        "    # Standard lineage expansion from UniProt strings\n",
        "    working_df[\"Lineage_Names\"] = working_df.get(\"Taxonomic lineage\", pd.Series([[]]*len(working_df))).apply(_split_lineage_names)\n",
        "    parsed_ids = working_df.get(\"Taxonomic lineage (Ids)\", pd.Series([\"\"]*len(working_df))).astype(str).apply(parse_taxonomic_lineage)\n",
        "    for code in RANK_CODES:\n",
        "        working_df[f\"{code}_id\"] = parsed_ids.apply(lambda d: d.get(code))\n",
        "\n",
        "    # --- FIX: Create the authoritative 'cluster_genus' column ---\n",
        "    # 1. Parse Genus from UniProt 'Organism' string as a fallback\n",
        "    working_df['uniprot_genus'] = working_df['Organism'].str.split().str[0]\n",
        "\n",
        "    # 2. Coalesce, prioritizing the Ensembl-derived genus\n",
        "    # If 'ensembl_genus' exists and is not null, use it; otherwise, use 'uniprot_genus'.\n",
        "    if 'ensembl_genus' in working_df.columns:\n",
        "        working_df['cluster_genus'] = working_df['ensembl_genus'].fillna(working_df['uniprot_genus'])\n",
        "    else:\n",
        "        logger.warning(\"'ensembl_genus' column not found, falling back to UniProt only.\")\n",
        "        working_df['cluster_genus'] = working_df['uniprot_genus']\n",
        "\n",
        "    # 3. Infer Family and Order for other grouping tasks\n",
        "    working_df['Family'] = working_df['Lineage_Names'].apply(lambda L: next((n for n in reversed(L) if n.endswith(\"idae\")), \"\"))\n",
        "    working_df['Order'] = working_df['Lineage_Names'].apply(lambda L: next((n for n in reversed(L) if n.endswith(\"iformes\")), \"\"))\n",
        "\n",
        "    logger.info(f\"Taxonomy expanded. Authoritative 'cluster_genus' column created.\")\n",
        "else:\n",
        "    logger.info(\"No working rows; skipping taxonomy enrichment.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1b1HTS18zZF"
      },
      "source": [
        "## Cell 25 â€“ Final Input Checks\n",
        "\n",
        "This final pre-processing step ensures that the `working_df` DataFrame contains the essential columns (`Entry`, `Sequence`) and that sequences are not empty, preventing errors in downstream cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMopIRjmLok8"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 25 =====\n",
        "# Final filters and input checks\n",
        "\n",
        "if 'working_df' in globals() and not working_df.empty:\n",
        "    required_cols = ['Entry', 'Sequence']\n",
        "    missing_cols = [c for c in required_cols if c not in working_df.columns]\n",
        "    if missing_cols:\n",
        "        logger.error(f\"FATAL: working_df is missing required columns: {missing_cols}. Halting execution.\")\n",
        "        # Stop execution if critical columns are missing\n",
        "        assert False, \"Missing critical columns in working_df\"\n",
        "    else:\n",
        "        # Filter out rows with empty or NaN sequences\n",
        "        initial_rows = len(working_df)\n",
        "        working_df.dropna(subset=['Sequence'], inplace=True)\n",
        "        working_df = working_df[working_df['Sequence'].str.len() > 0].copy()\n",
        "        final_rows = len(working_df)\n",
        "        logger.info(f\"Post-filter rows: {initial_rows} -> {final_rows} (removed empty sequences).\")\n",
        "else:\n",
        "    logger.warning(\"working_df is empty or not defined. Downstream cells may fail.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6MbOWcs83ST"
      },
      "source": [
        "## Cell 26 â€“ Persist Session Rejected Entries\n",
        "\n",
        "Identifies entries from the initial dataset that were filtered out during pre-processing (e.g., by gene, taxon, or QC). These rejected IDs are saved to a run-specific file and appended to the durable master rejection list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC1tfGvn84ky"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 26 =====\n",
        "# Persist session rejected IDs\n",
        "\n",
        "logger.info(\"Archiving entries rejected during pre-processing...\")\n",
        "\n",
        "if 'full_df' in globals() and 'working_df' in globals():\n",
        "    all_entries = set(full_df['Entry'].dropna())\n",
        "    accepted_entries = set(working_df['Entry'].dropna())\n",
        "    session_rejected_ids = sorted(list(all_entries - accepted_entries))\n",
        "\n",
        "    if session_rejected_ids:\n",
        "        logger.info(f\"{len(session_rejected_ids)} entries were rejected in this session.\")\n",
        "\n",
        "        # Save session-specific rejection list\n",
        "        with open(SESSION_REJECTED_PATH, 'w') as f:\n",
        "            for acc in session_rejected_ids:\n",
        "                f.write(f\"{acc}\\n\")\n",
        "        logger.info(f\"Session rejection list saved to: {SESSION_REJECTED_PATH}\")\n",
        "\n",
        "        # Update the master rejection TSV\n",
        "        new_rejections = pd.DataFrame({\n",
        "            \"Entry\": session_rejected_ids,\n",
        "            \"reason\": \"filtered_by_gene_or_taxon\",\n",
        "            \"run_id\": RUN_ID\n",
        "        })\n",
        "\n",
        "        # Load, append, deduplicate, and save master list\n",
        "        master_rejected = pd.DataFrame(columns=[\"Entry\",\"reason\",\"run_id\"])\n",
        "        if REJECTED_IDS_PATH.exists():\n",
        "            master_rejected = safe_read_tsv(REJECTED_IDS_PATH)\n",
        "\n",
        "        updated_master = pd.concat([master_rejected, new_rejections], ignore_index=True)\n",
        "        updated_master.drop_duplicates(subset=['Entry'], keep='last', inplace=True)\n",
        "        updated_master.to_csv(REJECTED_IDS_PATH, sep='\\t', index=False)\n",
        "\n",
        "        new_count = len(updated_master) - len(master_rejected)\n",
        "        logger.info(f\"Master rejection TSV updated with {new_count} new entries. Total: {len(updated_master)}.\")\n",
        "    else:\n",
        "        logger.info(\"No new entries were rejected in this session.\")\n",
        "else:\n",
        "    logger.warning(\"full_df or working_df not available; cannot compute session rejections.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0MBLjzR87Iy"
      },
      "source": [
        "# **Part 3: Chain Identification & Quality Pre-selection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCg77jPd891Y"
      },
      "source": [
        "## Cell 31 â€“ Detect Main G-X-Y Collagenous Chain\n",
        "\n",
        "Scans each protein sequence for long, contiguous runs of G-X-Y triplets, which define the primary collagenous domain. It identifies the longest such segment and calculates a preliminary quality score based on its length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwiohWohM3w4"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 31 =====\n",
        "# Identify main Gâ€“Xâ€“Y chain(s) (with robust column preservation)\n",
        "\n",
        "def find_main_chain_info(sequence: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Analyzes a single sequence to find the main GXY chain and its properties.\n",
        "    Returns a pandas Series, suitable for use with DataFrame.apply.\n",
        "    \"\"\"\n",
        "    if not isinstance(sequence, str):\n",
        "        return pd.Series({\n",
        "            'main_chain_segments': None,\n",
        "            'quality_score': 0.0,\n",
        "            'quality_flags': 'missing_sequence'\n",
        "        })\n",
        "\n",
        "    segs = []\n",
        "    i = 0\n",
        "    while i <= len(sequence) - 3:\n",
        "        if sequence[i] == 'G':\n",
        "            j = i\n",
        "            while j + 2 < len(sequence) and sequence[j] == 'G': j += 3\n",
        "            if (j - i) % 3 != 0: j -= ((j - i) % 3)\n",
        "            if (j - i) // 3 >= MIN_GXY_TRIPLETS:\n",
        "                segs.append({'start': i + 1, 'end': j})\n",
        "            i = j\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    if not segs:\n",
        "        return pd.Series({\n",
        "            'main_chain_segments': None,\n",
        "            'quality_score': 0.0,\n",
        "            'quality_flags': 'no_gxy_chain_found'\n",
        "        })\n",
        "\n",
        "    main_seg = max(segs, key=lambda x: x['end'] - x['start'])\n",
        "    triplets = (main_seg['end'] - main_seg['start']) // 3\n",
        "    qscore = float(min(100.0, 100.0 * triplets / max(1, MIN_GXY_TRIPLETS)))\n",
        "    has_cys = 'C' in sequence[main_seg['start']-1:main_seg['end']]\n",
        "\n",
        "    return pd.Series({\n",
        "        'main_chain_segments': [main_seg],\n",
        "        'quality_score': qscore,\n",
        "        'quality_flags': \"cys_in_helix\" if has_cys else \"\"\n",
        "    })\n",
        "\n",
        "if 'working_df' in globals() and not working_df.empty:\n",
        "    logger.info(\"Identifying main GXY chains using robust apply method...\")\n",
        "\n",
        "    # --- FIX: Use DataFrame.apply for a robust, column-preserving operation ---\n",
        "    tqdm.pandas(desc=\"Finding GXY Chains\")\n",
        "    chain_info_df = working_df['Sequence'].progress_apply(find_main_chain_info)\n",
        "\n",
        "    # Join the new columns back to the original working_df\n",
        "    chain_df = working_df.join(chain_info_df)\n",
        "\n",
        "    logger.info(f\"Chain identification complete. All original columns preserved.\")\n",
        "else:\n",
        "    chain_df = pd.DataFrame()\n",
        "    logger.warning(\"Working dataframe is empty. Skipping chain identification.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aUD1q4Z9BFv"
      },
      "source": [
        "## Cell 32 â€“ Quality Control and Rejection Persistence\n",
        "\n",
        "Applies quality control filters based on sequence length, G-X-Y content, and the presence of cysteine in the helix. Entries that fail QC are logged and added to the master rejection list with a specific failure reason. This cell now uses an index-based filtering method to ensure all original columns are preserved in the `df_high_quality` output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HETW4169Nm6N"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 32 =====\n",
        "# QC pass/fail and rejection persistence (with robust column preservation)\n",
        "\n",
        "df_high_quality = pd.DataFrame()\n",
        "df_failed_qc = pd.DataFrame()\n",
        "\n",
        "if 'chain_df' in globals() and not chain_df.empty:\n",
        "    reasons_list = []\n",
        "    for _, r in chain_df.iterrows():\n",
        "        reasons = []\n",
        "\n",
        "        # --- FIX: Defensively check for a valid segment list before subscripting ---\n",
        "        segments = r.get('main_chain_segments')\n",
        "\n",
        "        # Check if segments is None, NaN, or an empty list\n",
        "        if pd.isna(segments) or not isinstance(segments, list) or not segments:\n",
        "            reasons.append('no_main_chain')\n",
        "        else:\n",
        "            # It's now safe to access the first element\n",
        "            seg = segments[0]\n",
        "            if (seg['end'] - seg['start']) // 3 < MIN_GXY_TRIPLETS:\n",
        "                reasons.append('low_gxy_content')\n",
        "            if len(str(r.get('Sequence',''))) < MIN_LEN_AA:\n",
        "                reasons.append('short_sequence')\n",
        "            # The quality_flags column is now guaranteed to exist from Cell 31\n",
        "            if 'cys_in_helix' in r.get('quality_flags', ''):\n",
        "                reasons.append('cys_in_helix')\n",
        "\n",
        "        reasons_list.append(';'.join(reasons) if reasons else None)\n",
        "\n",
        "    # Add the failure reasons as a new column to the original dataframe\n",
        "    chain_df['failure_reasons'] = reasons_list\n",
        "\n",
        "    # Use boolean indexing to create the pass/fail dataframes, preserving all columns\n",
        "    pass_mask = chain_df['failure_reasons'].isna()\n",
        "    df_high_quality = chain_df[pass_mask].copy()\n",
        "    df_failed_qc = chain_df[~pass_mask].copy()\n",
        "\n",
        "    # Clean up the reasons column from the passing dataframe\n",
        "    df_high_quality.drop(columns=['failure_reasons'], inplace=True, errors='ignore')\n",
        "\n",
        "    logger.info(f\"QC results: {len(df_high_quality)} passed, {len(df_failed_qc)} failed.\")\n",
        "\n",
        "    if not df_failed_qc.empty:\n",
        "        qc_rejections = df_failed_qc[['Entry','failure_reasons']].rename(columns={'failure_reasons':'reason'})\n",
        "        qc_rejections['run_id'] = RUN_ID\n",
        "\n",
        "        master_rejected = safe_read_tsv(REJECTED_IDS_PATH)\n",
        "        updated_master = pd.concat([master_rejected, qc_rejections], ignore_index=True)\n",
        "        updated_master.drop_duplicates(subset=['Entry'], keep='last', inplace=True)\n",
        "        updated_master.to_csv(str(REJECTED_IDS_PATH), sep='\\t', index=False)\n",
        "\n",
        "        logger.info(f\"{len(qc_rejections)} QC failures merged into master rejection TSV.\")\n",
        "else:\n",
        "    logger.info(\"No chain candidates to perform QC on.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npHMKiLW1dBu"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 32A =====\n",
        "# Diagnostic Check on df_high_quality\n",
        "\n",
        "if 'df_high_quality' in globals() and not df_high_quality.empty:\n",
        "    print(\"--- First 5 rows of df_high_quality ---\")\n",
        "    display(df_high_quality.head())\n",
        "    print(\"\\n--- Columns in df_high_quality ---\")\n",
        "    print(df_high_quality.columns.tolist())\n",
        "else:\n",
        "    print(\"df_high_quality is empty or not defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg88C1ha_svI"
      },
      "source": [
        "## Cell 33 â€“ Ensembl Genus Pre-flight Filter\n",
        "\n",
        "To dramatically improve the efficiency of the exon mapping step, this cell proactively filters the high-quality candidate list based on a robust biological heuristic. It first fetches a complete list of all **genera** present in the Ensembl database. It then removes any candidate sequences from our `df_high_quality` list whose genus (using the authoritative **`cluster_genus`** column) is not in this Ensembl set. This ensures that we only attempt to map exons for species that have a close relative in the target database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7Jx8rDOJcjS"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 33 =====\n",
        "# Ensembl Genus Pre-flight Filter (with Production-Grade Client)\n",
        "\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def _session_with_retries() -> requests.Session:\n",
        "    # ... [function is correct, remains unchanged] ...\n",
        "    sess = requests.Session()\n",
        "    retry = Retry(total=5, backoff_factor=0.5, status_forcelist=(408, 429, 500, 502, 503, 504))\n",
        "    adapter = HTTPAdapter(max_retries=retry)\n",
        "    sess.mount(\"https://\", adapter)\n",
        "    sess.headers.update({\"Accept\": \"application/json\"})\n",
        "    return sess\n",
        "\n",
        "def get_ensembl_genera_set() -> Set[str]:\n",
        "    \"\"\"\n",
        "    Retrieves the species list from Ensembl and returns a set of all unique,\n",
        "    lowercase genus names.\n",
        "    \"\"\"\n",
        "    if 'ensembl_genera_set' in globals() and globals().get('ensembl_genera_set'):\n",
        "        logger.info(\"Using cached Ensembl genera list.\")\n",
        "        return globals()['ensembl_genera_set']\n",
        "    logger.info(\"Fetching complete species list from Ensembl to build genus filter...\")\n",
        "    sess = _session_with_retries()\n",
        "    try:\n",
        "        r = sess.get(f\"{ENSEMBL_BASE}/info/species\", timeout=60)\n",
        "        r.raise_for_status()\n",
        "        species_list = r.json().get(\"species\", [])\n",
        "        genera_set = {name.split('_')[0].lower() for s in species_list if (name := s.get('name')) and '_' in name}\n",
        "        logger.info(f\"Successfully identified {len(genera_set)} unique genera in Ensembl.\")\n",
        "        globals()['ensembl_genera_set'] = genera_set\n",
        "        return genera_set\n",
        "    except (requests.RequestException, KeyError, ValueError) as e:\n",
        "        logger.error(f\"Failed to fetch Ensembl species data: {e}\")\n",
        "        logger.warning(\"Proceeding without genus pre-flight check.\")\n",
        "        globals()['ensembl_genera_set'] = set()\n",
        "        return set()\n",
        "\n",
        "# --- Main Filtering Logic ---\n",
        "ensembl_genera = get_ensembl_genera_set()\n",
        "\n",
        "if \"df_high_quality\" in globals() and not df_high_quality.empty and ensembl_genera:\n",
        "    initial_count = len(df_high_quality)\n",
        "    logger.info(f\"Applying Ensembl genus pre-filter to {initial_count} candidates...\")\n",
        "\n",
        "    # --- FIX: Use the authoritative 'cluster_genus' column for filtering ---\n",
        "    if \"cluster_genus\" in df_high_quality.columns:\n",
        "        # Filter based on the presence of the genus in Ensembl\n",
        "        mask = df_high_quality[\"cluster_genus\"].str.lower().isin(ensembl_genera)\n",
        "        df_high_quality_filtered = df_high_quality[mask].copy()\n",
        "\n",
        "        removed_count = initial_count - len(df_high_quality_filtered)\n",
        "        logger.info(f\"Removed {removed_count} entries for genera not present in Ensembl.\")\n",
        "        logger.info(f\"High-quality candidates remaining for mapping: {len(df_high_quality_filtered)}\")\n",
        "        df_high_quality = df_high_quality_filtered\n",
        "    else:\n",
        "        logger.warning(\"Cannot apply filter: 'cluster_genus' column not found in df_high_quality.\")\n",
        "else:\n",
        "    logger.info(\"Skipping Ensembl genus pre-filter (no data or failed API call).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLxeZTl-U3P0"
      },
      "source": [
        "## Cell 34 â€“ Synteny-Based Pseudogene Filter\n",
        "\n",
        "This cell introduces a critical pre-filtering step to improve the quality of candidates for exon mapping by removing likely pseudogenes and mis-annotated entries based on their genomic location.\n",
        "\n",
        "### Rationale: Leveraging Evolutionary Synteny\n",
        "\n",
        "This filter relies on the **`ensembl_id`** column populated by the proactive discovery in **Cell 22**. It checks if the gene is located on a canonical chromosome or an unreliable scaffold, ensuring only stable genomic models are used for the seed consensus.\n",
        "\n",
        "### Process\n",
        "\n",
        "*   For each high-quality candidate sequence, the gene's `ensembl_id` is used to query the Ensembl REST API for its physical location.\n",
        "*   Sequences located on unplaced scaffolds or patches are filtered out.\n",
        "\n",
        "### Outcome\n",
        "\n",
        "This step produces the final, highly purified \"seed set\" for building the consensus architectural template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-CoYisLI9Xm"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 34 =====\n",
        "# Synteny-Based Sanity Check to Filter Pseudogenes (with Caching)\n",
        "\n",
        "#@markdown #### **Synteny Filter Settings**\n",
        "ENABLE_SYNTENY_FILTER = True #@param {type:\"boolean\"}\n",
        "\n",
        "def check_gene_location(ensembl_id: str) -> Optional[str]:\n",
        "    \"\"\"Fetches the chromosome name for a given Ensembl gene ID.\"\"\"\n",
        "    if not isinstance(ensembl_id, str) or not ensembl_id.startswith(\"ENS\"):\n",
        "        return None\n",
        "    url = f\"{ENSEMBL_BASE}/lookup/id/{ensembl_id}?content-type=application/json\"\n",
        "    try:\n",
        "        # Short timeout because we expect a fast response for valid IDs\n",
        "        response = requests.get(url, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            seq_region = data.get('seq_region_name', '').lower()\n",
        "            if 'scaffold' in seq_region or 'patch' in seq_region:\n",
        "                return \"unplaced_scaffold\"\n",
        "            return data.get('seq_region_name', 'lookup_failed')\n",
        "    except requests.RequestException:\n",
        "        return 'network_error'\n",
        "    return 'unknown_error'\n",
        "\n",
        "if ENABLE_SYNTENY_FILTER and 'df_high_quality' in globals() and not df_high_quality.empty:\n",
        "    if 'ensembl_id' not in df_high_quality.columns:\n",
        "        logger.error(\"FATAL: Prerequisite 'ensembl_id' column not found. Skipping Synteny Filter.\")\n",
        "    else:\n",
        "        logger.info(\"ðŸ§¬ Applying synteny filter with caching...\")\n",
        "\n",
        "        # --- Caching Logic ---\n",
        "        # 1. Load existing cache if it exists\n",
        "        cached_locations = {}\n",
        "        if SYNTENY_CACHE_TSV.exists():\n",
        "            try:\n",
        "                cache_df = pd.read_csv(SYNTENY_CACHE_TSV, sep='\\t')\n",
        "                # Create a fast lookup dictionary from Entry -> chromosome\n",
        "                cached_locations = pd.Series(cache_df.chromosome.values, index=cache_df.Entry).to_dict()\n",
        "                logger.info(f\"Loaded {len(cached_locations)} locations from synteny cache.\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Could not load synteny cache, will rebuild. Error: {e}\")\n",
        "\n",
        "        # 2. Identify entries that need to be checked via API\n",
        "        entries_to_check_df = df_high_quality[\n",
        "            ~df_high_quality['Entry'].isin(cached_locations.keys())\n",
        "        ].dropna(subset=['ensembl_id']).copy()\n",
        "\n",
        "        # 3. Perform the slow API calls ONLY for new entries\n",
        "        if not entries_to_check_df.empty:\n",
        "            logger.info(f\"Found {len(entries_to_check_df)} new entries to check for synteny.\")\n",
        "            tqdm.pandas(desc=\"Checking New Gene Locations\")\n",
        "            new_locations = entries_to_check_df['ensembl_id'].progress_apply(check_gene_location)\n",
        "\n",
        "            # 4. Update the cache file with the new results\n",
        "            new_results_df = pd.DataFrame({\n",
        "                'Entry': entries_to_check_df['Entry'],\n",
        "                'chromosome': new_locations\n",
        "            })\n",
        "            # Use mode 'a' (append) and no header if file already exists\n",
        "            new_results_df.to_csv(\n",
        "                SYNTENY_CACHE_TSV,\n",
        "                sep='\\t',\n",
        "                index=False,\n",
        "                mode='a',\n",
        "                header=not SYNTENY_CACHE_TSV.exists()\n",
        "            )\n",
        "            logger.info(f\"Appended {len(new_results_df)} new results to synteny cache.\")\n",
        "\n",
        "            # Add new results to our in-memory dictionary\n",
        "            cached_locations.update(new_results_df.set_index('Entry')['chromosome'].to_dict())\n",
        "        else:\n",
        "            logger.info(\"No new entries to check; all locations found in cache.\")\n",
        "\n",
        "        # 5. Map all locations (cached + new) to the main dataframe\n",
        "        initial_count = len(df_high_quality)\n",
        "        df_high_quality['chromosome'] = df_high_quality['Entry'].map(cached_locations)\n",
        "\n",
        "        # 6. Filter out entries on scaffolds, with failed lookups, or with no Ensembl ID\n",
        "        valid_mask = df_high_quality['chromosome'].notna() & (~df_high_quality['chromosome'].isin(['unplaced_scaffold', 'lookup_failed', 'network_error', 'unknown_error']))\n",
        "        df_high_quality = df_high_quality[valid_mask].copy()\n",
        "\n",
        "        removed_count = initial_count - len(df_high_quality)\n",
        "        logger.info(f\"Synteny filter complete. Removed {removed_count} entries.\")\n",
        "        logger.info(f\"High-quality candidates remaining for mapping: {len(df_high_quality)}\")\n",
        "else:\n",
        "    logger.info(\"â˜‘ï¸ Skipping synteny filter.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT0NhU5TcYZa"
      },
      "outputs": [],
      "source": [
        "# # ===== TEMPORARY PATCH CELL =====\n",
        "# # Save in-memory synteny results to the new cache file.\n",
        "# # This cell can be deleted after you run it successfully one time.\n",
        "\n",
        "# if 'df_high_quality' in globals() and 'chromosome' in df_high_quality.columns:\n",
        "#     logger.info(\"PATCH: Saving in-memory synteny results to cache file...\")\n",
        "\n",
        "#     # Select the two columns needed for the cache\n",
        "#     synteny_results_to_save = df_high_quality[['Entry', 'chromosome']].copy()\n",
        "\n",
        "#     # Drop any rows where the lookup might have failed or was skipped\n",
        "#     synteny_results_to_save.dropna(subset=['chromosome'], inplace=True)\n",
        "\n",
        "#     # Ensure there are no duplicate entries\n",
        "#     synteny_results_to_save.drop_duplicates(subset=['Entry'], keep='first', inplace=True)\n",
        "\n",
        "#     try:\n",
        "#         # Write the results to the cache file\n",
        "#         synteny_results_to_save.to_csv(SYNTENY_CACHE_TSV, sep='\\t', index=False)\n",
        "#         logger.info(f\"âœ… PATCH SUCCESS: Saved {len(synteny_results_to_save)} results to {SYNTENY_CACHE_TSV.name}\")\n",
        "\n",
        "#         # Optional: Display the first few rows to verify\n",
        "#         print(\"\\n--- Sample of saved cache data ---\")\n",
        "#         display(synteny_results_to_save.head())\n",
        "\n",
        "#     except Exception as e:\n",
        "#         logger.error(f\"PATCH FAILED: Could not write to file. Error: {e}\")\n",
        "#         logger.error(\"Please ensure you have re-run Cell 12 to define the SYNTENY_CACHE_TSV path.\")\n",
        "\n",
        "# else:\n",
        "#     logger.warning(\"PATCH SKIPPED: `df_high_quality` with 'chromosome' column not found in memory.\")\n",
        "#     logger.warning(\"This means the run was stopped before the API calls were made.\")\n",
        "#     logger.warning(\"You will need to let the new caching version of Cell 34 run once to build the cache.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZcfLNOZDhhJ"
      },
      "source": [
        "## Cell 34A â€“ Seed Set Quality Assessment\n",
        "\n",
        "This cell provides a quick diagnostic profile of the final \"seed set\" of high-quality candidates. The goal is to verify that despite the small number of sequences, the set retains high phylogenetic diversity, which is crucial for building a robust consensus template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ7-G6XRM9j9"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 34A =====\n",
        "# Diagnostic Profiling of the Final Seed Set\n",
        "\n",
        "if 'df_high_quality' in globals() and not df_high_quality.empty:\n",
        "    logger.info(\"--- ðŸ“Š Profiling Final Seed Set for Mapping ---\")\n",
        "\n",
        "    total_entries = len(df_high_quality)\n",
        "    unique_species = df_high_quality['Organism'].nunique()\n",
        "    # --- FIX: Use the authoritative 'cluster_genus' column from Cell 24 ---\n",
        "    unique_genera = df_high_quality['cluster_genus'].nunique()\n",
        "\n",
        "    logger.info(f\"Total Entries in Seed Set: {total_entries}\")\n",
        "    logger.info(f\"Unique Species Represented: {unique_species}\")\n",
        "    logger.info(f\"Unique Genera Represented: {unique_genera}\")\n",
        "\n",
        "    if total_entries > 0:\n",
        "        diversity_score = (unique_genera / total_entries) * 100\n",
        "        logger.info(f\"Phylogenetic Diversity Score (Genera/Entries): {diversity_score:.1f}%\")\n",
        "\n",
        "    print(\"\\n--- Top 15 Genera in Seed Set ---\")\n",
        "    # --- FIX: Use the authoritative 'cluster_genus' column for the value counts ---\n",
        "    display(df_high_quality['cluster_genus'].value_counts().head(15))\n",
        "\n",
        "else:\n",
        "    logger.warning(\"Seed set (df_high_quality) is empty. No profile to generate.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkLCqe9v9EMR"
      },
      "source": [
        "# **Part 4: Exon Mapping & Architecture Definition**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "077pfMjA9FsH"
      },
      "source": [
        "## Cell 40 â€“ Exon Mapping Safety & Validation Utilities\n",
        "\n",
        "This cell defines helper functions for robust, incremental exon mapping. It includes utilities for memory monitoring, validating the integrity of cache files, loading already-mapped accessions to resume runs, and atomically merging new results into the cache with backups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTxSb8e9McZv"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 40 =====\n",
        "# Mapping safety helpers (cache validation, resume, atomic writes)\n",
        "\n",
        "def get_memory_usage() -> Dict[str, float]:\n",
        "    \"\"\"Returns current memory usage statistics.\"\"\"\n",
        "    process = psutil.Process(os.getpid())\n",
        "    mem_info = process.memory_info()\n",
        "    return {'rss_mb': mem_info.rss / 1e6, 'percent': process.memory_percent()}\n",
        "\n",
        "def validate_cache_integrity(cache_path: Path, expected_cols: list) -> Tuple[bool, str]:\n",
        "    \"\"\"Validates cache file integrity, returning (is_valid, message).\"\"\"\n",
        "    if not cache_path.exists():\n",
        "        return True, \"Cache file does not exist (will be created).\"\n",
        "    try:\n",
        "        df_sample = pd.read_csv(cache_path, sep='\\t', nrows=5, low_memory=False)\n",
        "        missing = set(expected_cols) - set(df_sample.columns)\n",
        "        if missing:\n",
        "            return False, f\"Missing required columns: {missing}\"\n",
        "        return True, \"Cache validation passed.\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Cache validation error: {e}\"\n",
        "\n",
        "def load_mapped_accessions_from_cache() -> Set[str]:\n",
        "    \"\"\"Reads the master exon cache and returns the set of already-mapped accessions.\"\"\"\n",
        "    if not RAW_EXONS_CACHE.exists():\n",
        "        logger.info(\"No existing exon cache found.\")\n",
        "        return set()\n",
        "\n",
        "    is_valid, msg = validate_cache_integrity(RAW_EXONS_CACHE, ['accession'])\n",
        "    if not is_valid:\n",
        "        logger.error(f\"Exon cache validation failed: {msg}. Moving corrupted file.\")\n",
        "        corrupted_path = RAW_EXONS_CACHE.with_suffix(f'.corrupted_{RUN_TIMESTAMP}')\n",
        "        RAW_EXONS_CACHE.rename(corrupted_path)\n",
        "        logger.info(f\"Corrupted cache backed up to: {corrupted_path}\")\n",
        "        return set()\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(RAW_EXONS_CACHE, sep='\\t', usecols=['accession'], low_memory=False)\n",
        "        accession_set = set(df['accession'].dropna().astype(str))\n",
        "        logger.info(f\"Loaded {len(accession_set)} unique mapped accessions from cache.\")\n",
        "        return accession_set\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error reading exon cache for resume: {e}. Starting fresh.\")\n",
        "        return set()\n",
        "\n",
        "def atomic_merge_into_cache(new_rows_df: pd.DataFrame) -> None:\n",
        "    \"\"\"Atomically merges new rows into the master exon cache with backups.\"\"\"\n",
        "    if new_rows_df is None or new_rows_df.empty:\n",
        "        logger.info(\"No new rows to merge into cache.\")\n",
        "        return\n",
        "\n",
        "    key_cols = ['accession', 'exon_num_in_chain']\n",
        "    if not all(c in new_rows_df.columns for c in key_cols):\n",
        "        logger.error(\"New rows missing key columns for merge. Aborting cache write.\")\n",
        "        return\n",
        "\n",
        "    old_df = pd.DataFrame()\n",
        "    if RAW_EXONS_CACHE.exists():\n",
        "        old_df = safe_read_tsv(RAW_EXONS_CACHE)\n",
        "\n",
        "    combined = pd.concat([old_df, new_rows_df], ignore_index=True)\n",
        "    combined.drop_duplicates(subset=key_cols, keep='last', inplace=True)\n",
        "\n",
        "    # Atomic write procedure\n",
        "    temp_path = RAW_EXONS_CACHE.with_suffix('.tmp')\n",
        "    backup_path = RAW_EXONS_CACHE.with_suffix(f'.bak_{RUN_TIMESTAMP}')\n",
        "    try:\n",
        "        combined.to_csv(temp_path, sep='\\t', index=False)\n",
        "        if RAW_EXONS_CACHE.exists():\n",
        "            RAW_EXONS_CACHE.rename(backup_path)\n",
        "        temp_path.rename(RAW_EXONS_CACHE)\n",
        "        if backup_path.exists():\n",
        "            backup_path.unlink() # Clean up successful backup\n",
        "        logger.info(f\"Cache merge complete: {len(old_df)} + {len(new_rows_df)} -> {len(combined)} rows.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Cache merge failed: {e}\")\n",
        "        # Restore backup if it exists\n",
        "        if backup_path.exists():\n",
        "            backup_path.rename(RAW_EXONS_CACHE)\n",
        "            logger.info(\"Restored cache from backup.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zEPdAED9JYX"
      },
      "source": [
        "## Cell 41 â€“ Enhanced Exon Coordinate Mapper (Ensembl REST API)\n",
        "\n",
        "This class fetches exon coordinates from the Ensembl REST API. It features robust error handling with distinct categories, automatic retries with exponential backoff, detailed logging, and performance monitoring to diagnose API issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vezju_Iz9KTS"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 41 =====\n",
        "# Enhanced Exon Coordinate Mapper with Negative Caching support\n",
        "\n",
        "from collections import defaultdict\n",
        "from enum import Enum\n",
        "\n",
        "class ErrorCategory(Enum):\n",
        "    API_FAILURE = \"api_failure\"; TIMEOUT = \"timeout\"; DATA_FORMAT = \"data_format\"\n",
        "    COORDINATE_ERROR = \"coordinate_error\"; SEQUENCE_MISMATCH = \"sequence_mismatch\"\n",
        "    EMPTY_RESPONSE = \"empty_response\"; PARSING_ERROR = \"parsing_error\"\n",
        "    NETWORK_ERROR = \"network_error\"; UNKNOWN = \"unknown_error\"\n",
        "\n",
        "class EnhancedExonCoordinateMapper:\n",
        "    \"\"\"Fetches exon coordinates with robust error handling and negative caching.\"\"\"\n",
        "    def __init__(self, base_url=ENSEMBL_BASE, max_retries=3, initial_delay=1.0):\n",
        "        self.base_url = base_url; self.max_retries = max_retries; self.initial_delay = initial_delay\n",
        "        self.cache = {}; self.failed = set(); self.stats = defaultdict(int); self.error_details = defaultdict(list)\n",
        "\n",
        "    def log_error(self, cat: ErrorCategory, acc: str, details: str, context: dict = None):\n",
        "        self.stats[cat.value] += 1; self.stats['total_failures'] += 1\n",
        "        logger.debug(f\"Mapping error for {acc} ({cat.value}): {details}\")\n",
        "\n",
        "    def _fetch(self, accession: str) -> Optional[Dict]:\n",
        "        url = f\"{self.base_url}/lookup/id/{accession}?content-type=application/json;expand=1\"\n",
        "        for attempt in range(self.max_retries):\n",
        "            try:\n",
        "                self.stats['api_calls'] += 1\n",
        "                response = requests.get(url, timeout=ENSEMBL_TIMEOUT_SECS)\n",
        "                if response.status_code == 200:\n",
        "                    data = response.json()\n",
        "                    if not data: self.log_error(ErrorCategory.EMPTY_RESPONSE, accession, \"API returned empty JSON.\"); return None\n",
        "                    return data\n",
        "                self.log_error(ErrorCategory.API_FAILURE, accession, f\"HTTP {response.status_code}\", {'attempt': attempt})\n",
        "                time.sleep(self.initial_delay * (2 ** attempt))\n",
        "            except requests.RequestException as e:\n",
        "                self.log_error(ErrorCategory.NETWORK_ERROR, accession, str(e), {'attempt': attempt})\n",
        "                time.sleep(self.initial_delay * (2 ** attempt))\n",
        "        return None\n",
        "\n",
        "    def get_mapped_exons(self, accession: str, main_chain: List[Dict], sequence: str) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Main method to get mapped exons.\n",
        "        Returns a list of exon dicts on success, or a single placeholder dict on failure.\n",
        "        \"\"\"\n",
        "        self.stats['total_attempts'] += 1\n",
        "        if accession in self.cache:\n",
        "            self.stats['cache_hits'] += 1\n",
        "            return self.cache[accession]\n",
        "\n",
        "        # --- NEW: Define a placeholder for failed mappings ---\n",
        "        failure_placeholder = [{\n",
        "            \"accession\": accession, \"exon_num_in_chain\": -999, \"begin_aa\": pd.NA,\n",
        "            \"end_aa\": pd.NA, \"peptide\": \"MAPPING_FAILED\", \"strand\": pd.NA, \"chr\": pd.NA\n",
        "        }]\n",
        "\n",
        "        data = self._fetch(accession)\n",
        "        if data is None:\n",
        "            self.failed.add(accession); self.cache[accession] = failure_placeholder\n",
        "            return failure_placeholder\n",
        "\n",
        "        try:\n",
        "            result = self._process_response(accession, data, main_chain, sequence)\n",
        "            if result:\n",
        "                self.stats['successes'] += 1\n",
        "                self.cache[accession] = result\n",
        "                return result\n",
        "            else:\n",
        "                self.failed.add(accession)\n",
        "                self.cache[accession] = failure_placeholder\n",
        "                return failure_placeholder\n",
        "        except Exception as e:\n",
        "            self.log_error(ErrorCategory.UNKNOWN, accession, f\"Unexpected processing error: {e}\")\n",
        "            self.failed.add(accession); self.cache[accession] = failure_placeholder\n",
        "            return failure_placeholder\n",
        "\n",
        "    def _process_response(self, acc: str, data: Dict, main_chain: List[Dict], seq: str) -> Optional[List[Dict]]:\n",
        "        # This internal logic remains the same.\n",
        "        # It will return None on processing failure, which get_mapped_exons will catch.\n",
        "        try:\n",
        "            gn = data['Transcript'][0]['Exon']\n",
        "            exons = sorted(gn, key=lambda x: x.get('start', 0))\n",
        "        except (KeyError, IndexError):\n",
        "            self.log_error(ErrorCategory.DATA_FORMAT, acc, \"Could not find exon data in response.\")\n",
        "            return None\n",
        "\n",
        "        # ... (rest of the processing logic is unchanged)\n",
        "        first_idx, last_idx = -1, -1\n",
        "        for i, ex in enumerate(exons):\n",
        "            if first_idx == -1: first_idx = i\n",
        "            last_idx = i\n",
        "        if first_idx == -1: return None\n",
        "\n",
        "        mapped, current_pos = [], 1\n",
        "        s_idx, e_idx = max(0, first_idx - 2), min(len(exons) - 1, last_idx + 2)\n",
        "        even_num = -2 * (first_idx - s_idx)\n",
        "        for i in range(s_idx, e_idx + 1):\n",
        "            ex = exons[i]\n",
        "            length = ex['end'] - ex['start'] + 1\n",
        "            begin_aa, end_aa = current_pos, current_pos + (length // 3) - 1\n",
        "            if end_aa > len(seq): continue\n",
        "            mapped.append({\n",
        "                \"accession\": acc, \"exon_num_in_chain\": even_num,\n",
        "                \"begin_aa\": begin_aa, \"end_aa\": end_aa,\n",
        "                \"peptide\": seq[begin_aa-1:end_aa],\n",
        "                \"strand\": ex.get('strand'), \"chr\": data.get('seq_region_name')\n",
        "            })\n",
        "            current_pos = end_aa + 1\n",
        "            even_num += 2\n",
        "        return mapped\n",
        "\n",
        "    def get_stats_summary(self) -> Dict:\n",
        "        summary = dict(self.stats)\n",
        "        summary['cache_size'] = len(self.cache)\n",
        "        summary['failed_count'] = len(self.failed)\n",
        "        return summary\n",
        "\n",
        "enhanced_exon_mapper = EnhancedExonCoordinateMapper()\n",
        "logger.info(\"âœ… Enhanced Exon Coordinate Mapper (with negative caching) loaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHK7g9Fz9NsY"
      },
      "source": [
        "## Cell 42 â€“ Incremental Exon Mapping Runner (Batch-Processed & Resumable)\n",
        "\n",
        "This cell orchestrates the exon mapping process in a robust, resumable manner. It identifies high-quality sequences that are not already in the cache, then iterates through them in manageable **batches**. After each batch is processed, the results are immediately and atomically saved to the master cache. If the run is interrupted, it can be restarted and will automatically resume from the last completed batch, preventing loss of work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cHp2B8m9bDO"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 42 =====\n",
        "# Incremental exon mapping runner (FINAL, MOST ROBUST VERSION)\n",
        "\n",
        "logger.info(\"--- Starting Part 4: Incremental Exon Mapping (Batch Mode) ---\")\n",
        "\n",
        "BATCH_SIZE = 100  #@param {type:\"integer\"}\n",
        "\n",
        "if 'df_high_quality' not in globals() or df_high_quality.empty:\n",
        "    logger.warning(\"df_high_quality is empty. No new entries to map.\")\n",
        "    to_map_accessions = []\n",
        "else:\n",
        "    all_hq_accessions = set(df_high_quality['Entry'].dropna().astype(str).str.strip())\n",
        "    already_mapped = load_mapped_accessions_from_cache()\n",
        "    to_map_accessions = sorted(list(all_hq_accessions - already_mapped))\n",
        "\n",
        "logger.info(f\"High-quality entries total: {len(all_hq_accessions) if 'all_hq_accessions' in locals() else 0}\")\n",
        "logger.info(f\"Already attempted/mapped in cache: {len(already_mapped) if 'already_mapped' in locals() else 0}\")\n",
        "logger.info(f\"Entries to map this run: {len(to_map_accessions)}\")\n",
        "\n",
        "if not to_map_accessions:\n",
        "    logger.info(\"âœ… No new entries to map after filtering. Skipping mapping.\")\n",
        "else:\n",
        "    num_batches = (len(to_map_accessions) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "    logger.info(f\"Processing {len(to_map_accessions)} entries in {num_batches} batches of up to {BATCH_SIZE} each.\")\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        batch_start_time = time.time()\n",
        "        start_index, end_index = i * BATCH_SIZE, (i + 1) * BATCH_SIZE\n",
        "        batch_accessions = to_map_accessions[start_index:end_index]\n",
        "\n",
        "        logger.info(f\"--- Starting Batch {i+1}/{num_batches} ({len(batch_accessions)} entries) ---\")\n",
        "\n",
        "        df_batch = df_high_quality[df_high_quality['Entry'].isin(batch_accessions)]\n",
        "\n",
        "        newly_mapped_rows_batch = []\n",
        "        with tqdm(total=len(df_batch), desc=f\"Mapping Batch {i+1}/{num_batches}\") as pbar:\n",
        "            for _, row in df_batch.iterrows():\n",
        "                # The mapper now ALWAYS returns a list, even for failures\n",
        "                exon_data = enhanced_exon_mapper.get_mapped_exons(row['Entry'], row['main_chain_segments'], row['Sequence'])\n",
        "                newly_mapped_rows_batch.extend(exon_data)\n",
        "                pbar.update(1)\n",
        "\n",
        "        # --- Commit After Each Batch ---\n",
        "        # This block now runs for every batch, recording both successes and failures.\n",
        "        batch_df = pd.DataFrame(newly_mapped_rows_batch)\n",
        "        atomic_merge_into_cache(batch_df)\n",
        "        successful_maps = len(batch_df[batch_df['peptide'] != 'MAPPING_FAILED'])\n",
        "        logger.info(f\"âœ… Batch {i+1} complete. Committed {len(batch_df)} total rows ({successful_maps} successful) to cache.\")\n",
        "\n",
        "        batch_snapshot_path = RUN_DIR / f\"mapped_exons_batch_{i+1}.tsv\"\n",
        "        batch_df.to_csv(batch_snapshot_path, sep='\\t', index=False)\n",
        "\n",
        "        batch_duration = time.time() - batch_start_time\n",
        "        logger.info(f\"Batch {i+1} finished in {batch_duration / 60:.2f} minutes.\")\n",
        "\n",
        "logger.info(\"--- Incremental Exon Mapping Complete ---\")\n",
        "logger.info(f\"Final mapping stats for the run: {json.dumps(enhanced_exon_mapper.get_stats_summary(), indent=2)}\")\n",
        "\n",
        "all_batch_files = sorted(RUN_DIR.glob(\"mapped_exons_batch_*.tsv\"))\n",
        "if all_batch_files:\n",
        "    all_run_mapped_df = pd.concat([pd.read_csv(f, sep='\\t', low_memory=False) for f in all_batch_files], ignore_index=True)\n",
        "    all_run_mapped_df.to_csv(MAPPED_SNAPSHOT, sep='\\t', index=False)\n",
        "    logger.info(f\"Consolidated snapshot for this run saved to {MAPPED_SNAPSHOT.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht8cGJAy9RQg"
      },
      "source": [
        "## Cell 43 â€“ Error Analysis & Recovery Planning\n",
        "\n",
        "Analyzes the detailed failure patterns from the exon mapper to identify systematic issues. It generates a summary report and suggests potential recovery actions, providing insight into the pipeline's performance and potential areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQxhSZsZ9SaQ"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 43 =====\n",
        "# Error analysis and recovery system\n",
        "\n",
        "def analyze_mapping_failures(mapper: EnhancedExonCoordinateMapper) -> Dict:\n",
        "    \"\"\"Analyzes failure patterns and generates a report.\"\"\"\n",
        "    logger.info(\"ðŸ” Analyzing mapping failure patterns...\")\n",
        "    stats = mapper.stats\n",
        "    total_failures = stats.get('total_failures', 0)\n",
        "\n",
        "    analysis = {\n",
        "        'run_id': RUN_ID,\n",
        "        'summary': dict(stats),\n",
        "        'failure_analysis': {},\n",
        "        'recommendations': []\n",
        "    }\n",
        "\n",
        "    if total_failures > 0:\n",
        "        sorted_errors = sorted(\n",
        "            [(k, v) for k, v in stats.items() if k not in ['total_failures', 'total_attempts', 'api_calls', 'successes', 'cache_hits']],\n",
        "            key=lambda item: item[1], reverse=True\n",
        "        )\n",
        "        for category, count in sorted_errors:\n",
        "            percentage = (count / total_failures) * 100\n",
        "            analysis['failure_analysis'][category] = {'count': count, 'percentage': f\"{percentage:.1f}%\"}\n",
        "\n",
        "        top_failure = sorted_errors[0][0] if sorted_errors else None\n",
        "        if top_failure == 'api_failure':\n",
        "            analysis['recommendations'].append(\"High API failure rate suggests network issues or server-side problems. Check Ensembl status.\")\n",
        "        elif top_failure == 'coordinate_error':\n",
        "            analysis['recommendations'].append(\"Coordinate errors are frequent. Review the logic for mapping genomic to protein coordinates in Cell 41.\")\n",
        "\n",
        "    else:\n",
        "        logger.info(\"âœ… No mapping failures detected.\")\n",
        "\n",
        "    try:\n",
        "        with open(ERROR_REPORT_PATH, 'w') as f:\n",
        "            json.dump(analysis, f, indent=2)\n",
        "        logger.info(f\"ðŸ’¾ Error analysis report saved to: {ERROR_REPORT_PATH}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to save error analysis report: {e}\")\n",
        "\n",
        "    return analysis\n",
        "\n",
        "failure_analysis = analyze_mapping_failures(enhanced_exon_mapper)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfVEpfoZ9VWA"
      },
      "source": [
        "## Cell 44 â€“ Load Seed Exon Data from Cache\n",
        "\n",
        "This cell performs a final check on the master exon cache (`raw_exons_cache.tsv`). It then loads the currently mapped data, which will serve as the high-confidence **seed data** for building our initial architectural template. This seed dataset contains exons mapped only from the most reliable, pre-filtered sequences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ll0Dt3OZtCI"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 44 =====\n",
        "# Load seed exon data from cache\n",
        "\n",
        "def _safe_row_count(p: Path) -> int:\n",
        "    \"\"\"Safely estimates row count, returning -1 on failure.\"\"\"\n",
        "    if not p.is_file(): return -1\n",
        "    try:\n",
        "        # Fast line count for large files\n",
        "        with open(p, 'rb') as f:\n",
        "            return sum(1 for _ in f) - 1\n",
        "    except Exception:\n",
        "        return -1\n",
        "\n",
        "logger.info(\"Performing final cache integrity check...\")\n",
        "current_rows = _safe_row_count(RAW_EXONS_CACHE)\n",
        "logger.info(f\"Current exon cache: ~{current_rows} rows.\")\n",
        "\n",
        "backups = sorted(\n",
        "    list(CACHE_DIR.glob(\"raw_exons_cache.bak_*\")),\n",
        "    key=lambda p: p.stat().st_mtime,\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "if backups:\n",
        "    latest_bak = backups[0]\n",
        "    bak_rows = _safe_row_count(latest_bak)\n",
        "    logger.info(f\"Latest backup: {latest_bak.name} (~{bak_rows} rows).\")\n",
        "\n",
        "    if current_rows >= 0 and bak_rows > current_rows:\n",
        "        logger.warning(f\"Backup ({bak_rows} rows) is larger than current cache ({current_rows} rows). Restoring from backup.\")\n",
        "        try:\n",
        "            latest_bak.replace(RAW_EXONS_CACHE)\n",
        "            logger.info(\"âœ… Auto-restore from backup complete.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to restore from backup: {e}\")\n",
        "else:\n",
        "    logger.info(\"No backups found for comparison.\")\n",
        "\n",
        "# --- MODIFICATION: Load the mapped data into a 'seed' DataFrame ---\n",
        "# This data is from the high-confidence run and will be used to build the rescue template.\n",
        "if RAW_EXONS_CACHE.exists():\n",
        "    df_raw_exons_seed = safe_read_tsv(RAW_EXONS_CACHE)\n",
        "    # Filter out any failed mappings that might be in the cache\n",
        "    df_raw_exons_seed = df_raw_exons_seed[df_raw_exons_seed['peptide'] != 'MAPPING_FAILED'].copy()\n",
        "    logger.info(f\"Loaded high-confidence seed exon dataset with {len(df_raw_exons_seed)} rows.\")\n",
        "else:\n",
        "    df_raw_exons_seed = pd.DataFrame()\n",
        "    logger.warning(\"Seed exon cache is not available for building rescue template.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pDabC6DRHKt"
      },
      "source": [
        "## Cell 45 â€“ Phylogenetic Consensus Engine\n",
        "\n",
        "This cell defines the core `PhylogeneticConsensusEngine`. It is initialized once and then used by all subsequent consensus and dating cells. It loads the Metazoan Newick tree and the pre-computed node age cache (from Cell 15) to provide two key services with high performance:\n",
        "1.  **MRCA Dating**: Finds the Most Recent Common Ancestor (MRCA) for any group of genera and returns its age in millions of years.\n",
        "2.  **Phylogenetic Weighting**: Calculates weights for each genus based on its evolutionary distance from the MRCA, ensuring that diverse taxa contribute more to consensus calculations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 45 =====\n",
        "# Phylogenetic Consensus Engine (Genus-only; with summary reporting)\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "from ete3 import Tree\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class PhylogeneticConsensusEngine:\n",
        "    \"\"\"\n",
        "    Handles phylogenetic weighting and dating using a pre-computed cache.\n",
        "\n",
        "    This engine:\n",
        "      1) loads a Newick phylogeny and a CSV of node ages,\n",
        "      2) normalises all leaf labels to extract *only* the Genus,\n",
        "      3) builds a genus â†’ leaf map (single representative per genus for lookups),\n",
        "      4) records a summary of how many leaves were associated with each genus.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    nwk_path : Path\n",
        "        Path to the Newick tree file.\n",
        "    node_ages_csv : Path\n",
        "        Path to a CSV with columns ['node_name', 'age'] giving node ages.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Robust genus-extraction machinery (class-level constants) ---\n",
        "    _GENUS_PAT = re.compile(\n",
        "        r\"\"\"\n",
        "        ^\\s*[\"']?                             # optional leading quote/space\n",
        "        (?P<genus>[A-Z][a-zA-Z\\-]+)           # Genus: capitalised Latin word (allow hyphen)\n",
        "        (?:                                   # optional trailing parts (species, subspecies, qualifiers)\n",
        "            [\\s_./|:-]+\n",
        "            (?:cf\\.|aff\\.|nr\\.|gr\\.|sp\\.|spp\\.|x|Ã—|hybrid)?\n",
        "            .*?\n",
        "        )?\n",
        "        [\"']?\\s*$                             # optional closing quote/space\n",
        "        \"\"\",\n",
        "        re.VERBOSE,\n",
        "    )\n",
        "    _HYBRID_SEP = re.compile(r\"[\\s_]+(?:x|Ã—|hybrid)[\\s_]+\", re.IGNORECASE)\n",
        "    _SEP_NORM = re.compile(r\"[ \\t\\n\\r_.:/|\\\\-]+\")\n",
        "\n",
        "    def __init__(self, nwk_path: Path, node_ages_csv: Path) -> None:\n",
        "        self.tree: Optional[Tree] = None\n",
        "        self.age_lookup: Dict[str, float] = {}\n",
        "        self.genus_to_leaf_map: Dict[str, Tree] = {}\n",
        "        self._genus_leaf_count: Dict[str, int] = {}\n",
        "        self._is_ready: bool = False\n",
        "\n",
        "        if not nwk_path.exists() or not node_ages_csv.exists():\n",
        "            logger.error(\n",
        "                \"Phylogenetic tree (%s) or node age cache (%s) missing. Engine disabled.\",\n",
        "                nwk_path.name,\n",
        "                node_ages_csv.name,\n",
        "            )\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            logger.info(\"Initializing Phylogenetic Engine from tree: %s\", nwk_path.name)\n",
        "            self.tree = Tree(str(nwk_path), format=1)\n",
        "\n",
        "            # First pass: compute genus counts across *all* leaves.\n",
        "            for leaf in self.tree.iter_leaves():\n",
        "                label = str(leaf.name) if leaf.name is not None else \"\"\n",
        "                genus_key = self.extract_genus(label, return_canonical=False)\n",
        "                if genus_key is None:\n",
        "                    # If desired, you can switch this to 'continue' to be more permissive.\n",
        "                    raise ValueError(f\"Could not extract genus from label: {label!r}\")\n",
        "                self._genus_leaf_count[genus_key] = self._genus_leaf_count.get(genus_key, 0) + 1\n",
        "\n",
        "            # Second pass: build representative genus â†’ leaf mapping.\n",
        "            # Policy: keep the *first* leaf encountered for a genus as the representative.\n",
        "            # (We already have counts in _genus_leaf_count for summary purposes.)\n",
        "            for leaf in self.tree.iter_leaves():\n",
        "                label = str(leaf.name) if leaf.name is not None else \"\"\n",
        "                genus_key = self.extract_genus(label, return_canonical=False)\n",
        "                if genus_key is None:\n",
        "                    continue\n",
        "                if genus_key not in self.genus_to_leaf_map:\n",
        "                    self.genus_to_leaf_map[genus_key] = leaf\n",
        "\n",
        "            # Load node ages\n",
        "            df_ages = pd.read_csv(node_ages_csv)\n",
        "            required_cols = {\"node_name\", \"age\"}\n",
        "            if not required_cols.issubset(set(df_ages.columns)):\n",
        "                raise ValueError(\n",
        "                    f\"Node-age CSV must contain columns {sorted(required_cols)}; \"\n",
        "                    f\"found {sorted(df_ages.columns)}\"\n",
        "                )\n",
        "            df_ages[\"node_name\"] = df_ages[\"node_name\"].astype(str)\n",
        "            self.age_lookup = df_ages.set_index(\"node_name\")[\"age\"].to_dict()\n",
        "\n",
        "            self._is_ready = True\n",
        "            logger.info(\n",
        "                \"âœ… Phylogenetic Engine is ready with %d genera (from %d total leaves).\",\n",
        "                len(self.genus_to_leaf_map),\n",
        "                sum(self._genus_leaf_count.values()),\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\"Failed to initialize Phylogenetic Engine: %s\", e, exc_info=True)\n",
        "\n",
        "    # ------------------ Genus extraction (static utility) ------------------\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_genus(label: str, *, return_canonical: bool = True) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Extract the Genus epithet from a taxon label robustly.\n",
        "\n",
        "        The function:\n",
        "          - normalises separators,\n",
        "          - breaks hybrid expressions at the first hybrid marker (Ã—, x, 'hybrid'),\n",
        "          - ignores qualifiers (cf., aff., nr., gr., sp., spp.),\n",
        "          - returns only the Genus part.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        label : str\n",
        "            The original taxon label from the Newick tree or input list.\n",
        "        return_canonical : bool, optional\n",
        "            If True, return the Genus with canonical capitalisation ('Bos').\n",
        "            If False, return a lower-case normalised form ('bos').\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Optional[str]\n",
        "            The extracted Genus, or None if no valid genus token is found.\n",
        "        \"\"\"\n",
        "        if label is None:\n",
        "            return None\n",
        "\n",
        "        # Normalise whitespace and mixed separators.\n",
        "        s = str(label)\n",
        "        s = PhylogeneticConsensusEngine._SEP_NORM.sub(\" \", s).strip()\n",
        "\n",
        "        # If the label appears to encode a hybrid, only consider the left-hand side.\n",
        "        s = PhylogeneticConsensusEngine._HYBRID_SEP.split(s, maxsplit=1)[0]\n",
        "\n",
        "        # Strip qualifiers at the head (e.g., \"cf. Bos taurus\" â†’ \"Bos taurus\").\n",
        "        s = re.sub(r\"^(?:cf\\.|aff\\.|nr\\.|gr\\.)\\s+\", \"\", s, flags=re.IGNORECASE)\n",
        "\n",
        "        # Now match the leading Genus token.\n",
        "        m = PhylogeneticConsensusEngine._GENUS_PAT.match(s)\n",
        "        if not m:\n",
        "            return None\n",
        "\n",
        "        genus = m.group(\"genus\")\n",
        "        if return_canonical:\n",
        "            return genus[0].upper() + genus[1:]\n",
        "        return genus.lower()\n",
        "\n",
        "    # ------------------------- Public API methods --------------------------\n",
        "\n",
        "    def is_ready(self) -> bool:\n",
        "        \"\"\"Return True if the engine initialised without errors.\"\"\"\n",
        "        return self._is_ready\n",
        "\n",
        "    def genus_leaf_counts(self, *, descending: bool = True) -> Dict[str, int]:\n",
        "        \"\"\"\n",
        "        Return a dictionary of {genus(lower-case): leaf_count}.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        descending : bool\n",
        "            If True, sort by count descending.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Dict[str, int]\n",
        "            Mapping of genera to their observed leaf counts in the tree.\n",
        "        \"\"\"\n",
        "        items = sorted(self._genus_leaf_count.items(), key=lambda kv: kv[1], reverse=descending)\n",
        "        return dict(items)\n",
        "\n",
        "    def save_genus_leaf_summary(self, out_csv: Path) -> Path:\n",
        "        \"\"\"\n",
        "        Save the per-genus leaf-count summary to CSV.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        out_csv : Path\n",
        "            Output path for the CSV file.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Path\n",
        "            The path written.\n",
        "        \"\"\"\n",
        "        df = pd.DataFrame(\n",
        "            [{\"genus\": g, \"leaf_count\": c} for g, c in self._genus_leaf_count.items()]\n",
        "        ).sort_values(\"leaf_count\", ascending=False, kind=\"mergesort\")\n",
        "        df.to_csv(out_csv, index=False)\n",
        "        return out_csv\n",
        "\n",
        "    def get_mrca_age(self, genera_list: List[str]) -> Tuple[Optional[str], float]:\n",
        "        \"\"\"\n",
        "        Find the MRCA for a list of genera and return its node name and age.\n",
        "\n",
        "        The lookup is case-insensitive. If no valid nodes are found, returns\n",
        "        (\"No_Valid_Nodes\", 0.0). If the ancestor cannot be determined, returns\n",
        "        (\"Ancestor_Not_Found\", 0.0).\n",
        "        \"\"\"\n",
        "        if not self.is_ready() or not genera_list:\n",
        "            return None, 0.0\n",
        "\n",
        "        leaf_nodes = [\n",
        "            self.genus_to_leaf_map.get(str(g).lower())\n",
        "            for g in genera_list\n",
        "            if g is not None and str(g).strip() != \"\"\n",
        "        ]\n",
        "        valid_nodes = [node for node in leaf_nodes if node is not None]\n",
        "\n",
        "        if not valid_nodes:\n",
        "            return \"No_Valid_Nodes\", 0.0\n",
        "\n",
        "        ancestor = self.tree.get_common_ancestor(valid_nodes)\n",
        "        if ancestor is None:\n",
        "            return \"Ancestor_Not_Found\", 0.0\n",
        "\n",
        "        age = self.age_lookup.get(str(ancestor.name), 0.0)\n",
        "        return str(ancestor.name), float(age)\n",
        "\n",
        "    def weights_from_mrca(self, genera_list: List[str]) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Calculate phylogenetic weights for genera based on distance from the MRCA age.\n",
        "\n",
        "        The weighting here is a simple inverse-distance scheme:\n",
        "            weight(g) âˆ 1 / (1 + max(0, age(g) - age(MRCA)))\n",
        "        and is normalised to sum to 1 over the provided genera that are present in the tree.\n",
        "        \"\"\"\n",
        "        unique_genera = sorted({g for g in genera_list if g})\n",
        "        weights = {g: 0.1 for g in unique_genera}  # default baseline\n",
        "\n",
        "        if not self.is_ready() or not unique_genera:\n",
        "            return weights\n",
        "\n",
        "        _, mrca_age = self.get_mrca_age(unique_genera)\n",
        "\n",
        "        total_inverse_dist = 0.0\n",
        "        temp_weights: Dict[str, float] = {}\n",
        "\n",
        "        for genus in unique_genera:\n",
        "            node = self.genus_to_leaf_map.get(str(genus).lower())\n",
        "            if node:\n",
        "                genus_age = float(self.age_lookup.get(str(node.name), mrca_age))\n",
        "                distance = max(0.0, genus_age - mrca_age)\n",
        "                inverse_dist = 1.0 / (1.0 + distance)\n",
        "                temp_weights[genus] = inverse_dist\n",
        "                total_inverse_dist += inverse_dist\n",
        "\n",
        "        if total_inverse_dist > 0.0:\n",
        "            for genus, inv_dist in temp_weights.items():\n",
        "                weights[genus] = inv_dist / total_inverse_dist\n",
        "\n",
        "        return weights\n",
        "\n",
        "\n",
        "# Initialise the engine for use in subsequent cells\n",
        "phylo_engine = PhylogeneticConsensusEngine(DRIVE_METAZOAN_TREE_PATH, NODE_AGES_CSV_PATH)\n"
      ],
      "metadata": {
        "id": "E9hlMCIYJwVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = phylo_engine.genus_leaf_counts()\n",
        "list(counts.items())[:15]  # preview top 15 genera by leaf count"
      ],
      "metadata": {
        "id": "danuU3fKJ8i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # ===== Cell 45 =====\n",
        "# # Phylogenetic Consensus Engine (Definitive Correction)\n",
        "\n",
        "# from __future__ import annotations\n",
        "\n",
        "# import logging\n",
        "# import re\n",
        "# from pathlib import Path\n",
        "# from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "# import pandas as pd\n",
        "# from ete3 import Tree\n",
        "\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# class PhylogeneticConsensusEngine:\n",
        "#     \"\"\"\n",
        "#     Handles phylogenetic weighting and dating using a pre-computed cache.\n",
        "\n",
        "#     This engine:\n",
        "#       1) loads a Newick phylogeny and a CSV of node ages,\n",
        "#       2) normalises all leaf labels to extract *only* the Genus,\n",
        "#       3) builds a genus â†’ leaf map for downstream MRCA and weighting queries.\n",
        "\n",
        "#     Parameters\n",
        "#     ----------\n",
        "#     nwk_path : Path\n",
        "#         Path to the Newick tree file.\n",
        "#     node_ages_csv : Path\n",
        "#         Path to a CSV with columns ['node_name', 'age'] giving node ages.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # --- Robust genus-extraction machinery (class-level constants) ---\n",
        "#     _GENUS_PAT = re.compile(\n",
        "#         r\"\"\"\n",
        "#         ^\\s*[\"']?                             # optional leading quote/space\n",
        "#         (?P<genus>[A-Z][a-zA-Z\\-]+)           # Genus: capitalised Latin word (allow hyphen)\n",
        "#         (?:                                   # optional trailing parts (species, subspecies, qualifiers)\n",
        "#             [\\s_./|:-]+\n",
        "#             (?:cf\\.|aff\\.|nr\\.|gr\\.|sp\\.|spp\\.|x|Ã—|hybrid)?\n",
        "#             .*?\n",
        "#         )?\n",
        "#         [\"']?\\s*$                             # optional closing quote/space\n",
        "#         \"\"\",\n",
        "#         re.VERBOSE,\n",
        "#     )\n",
        "#     _HYBRID_SEP = re.compile(r\"[\\s_]+(?:x|Ã—|hybrid)[\\s_]+\", re.IGNORECASE)\n",
        "#     _SEP_NORM = re.compile(r\"[ \\t\\n\\r_.:/|\\\\-]+\")\n",
        "\n",
        "#     def __init__(self, nwk_path: Path, node_ages_csv: Path) -> None:\n",
        "#         self.tree: Optional[Tree] = None\n",
        "#         self.age_lookup: Dict[str, float] = {}\n",
        "#         self.genus_to_leaf_map: Dict[str, Tree] = {}\n",
        "#         self._is_ready: bool = False\n",
        "\n",
        "#         if not nwk_path.exists() or not node_ages_csv.exists():\n",
        "#             logger.error(\n",
        "#                 \"Phylogenetic tree (%s) or node age cache (%s) missing. Engine disabled.\",\n",
        "#                 nwk_path.name,\n",
        "#                 node_ages_csv.name,\n",
        "#             )\n",
        "#             return\n",
        "\n",
        "#         try:\n",
        "#             logger.info(\"Initializing Phylogenetic Engine from tree: %s\", nwk_path.name)\n",
        "#             # format=1 is common when leaves have names; adjust if your Newick differs.\n",
        "#             self.tree = Tree(str(nwk_path), format=1)\n",
        "\n",
        "#             # Build genus â†’ leaf mapping; keys are lower-case for stable lookups.\n",
        "#             for leaf in self.tree.iter_leaves():\n",
        "#                 label = str(leaf.name) if leaf.name is not None else \"\"\n",
        "#                 genus_key = self.extract_genus(label, return_canonical=False)\n",
        "#                 if genus_key is None:\n",
        "#                     raise ValueError(f\"Could not extract genus from label: {label!r}\")\n",
        "\n",
        "#                 # Warn on collisions (same genus seen multiple times)\n",
        "#                 if genus_key in self.genus_to_leaf_map and self.genus_to_leaf_map[genus_key] is not leaf:\n",
        "#                     logger.warning(\n",
        "#                         \"Multiple leaves map to the same genus %r. Keeping the first occurrence; label=%r\",\n",
        "#                         genus_key,\n",
        "#                         label,\n",
        "#                     )\n",
        "#                     # Policy: keep first occurrence (deterministic). Alternatively, store a list.\n",
        "\n",
        "#                 else:\n",
        "#                     self.genus_to_leaf_map[genus_key] = leaf\n",
        "\n",
        "#             # Load ages\n",
        "#             df_ages = pd.read_csv(node_ages_csv)\n",
        "#             required_cols = {\"node_name\", \"age\"}\n",
        "#             if not required_cols.issubset(set(df_ages.columns)):\n",
        "#                 raise ValueError(\n",
        "#                     f\"Node-age CSV must contain columns {sorted(required_cols)}; \"\n",
        "#                     f\"found {sorted(df_ages.columns)}\"\n",
        "#                 )\n",
        "#             # Normalise node_name to string for safety\n",
        "#             df_ages[\"node_name\"] = df_ages[\"node_name\"].astype(str)\n",
        "#             self.age_lookup = df_ages.set_index(\"node_name\")[\"age\"].to_dict()\n",
        "\n",
        "#             self._is_ready = True\n",
        "#             logger.info(\"âœ… Phylogenetic Engine is ready with %d genera.\", len(self.genus_to_leaf_map))\n",
        "\n",
        "#         except Exception as e:\n",
        "#             logger.error(\"Failed to initialize Phylogenetic Engine: %s\", e, exc_info=True)\n",
        "\n",
        "#     # ------------------ Genus extraction (static utility) ------------------\n",
        "\n",
        "#     @staticmethod\n",
        "#     def extract_genus(label: str, *, return_canonical: bool = True) -> Optional[str]:\n",
        "#         \"\"\"\n",
        "#         Extract the Genus epithet from a taxon label robustly.\n",
        "\n",
        "#         The function:\n",
        "#           - normalises separators,\n",
        "#           - breaks hybrid expressions at the first hybrid marker (Ã—, x, 'hybrid'),\n",
        "#           - ignores qualifiers (cf., aff., nr., gr., sp., spp.),\n",
        "#           - returns only the Genus part.\n",
        "\n",
        "#         Parameters\n",
        "#         ----------\n",
        "#         label : str\n",
        "#             The original taxon label from the Newick tree or input list.\n",
        "#         return_canonical : bool, optional\n",
        "#             If True, return the Genus with canonical capitalisation ('Bos').\n",
        "#             If False, return a lower-case normalised form ('bos').\n",
        "\n",
        "#         Returns\n",
        "#         -------\n",
        "#         Optional[str]\n",
        "#             The extracted Genus, or None if no valid genus token is found.\n",
        "#         \"\"\"\n",
        "#         if label is None:\n",
        "#             return None\n",
        "\n",
        "#         # Normalise whitespace and mixed separators.\n",
        "#         s = str(label)\n",
        "#         s = PhylogeneticConsensusEngine._SEP_NORM.sub(\" \", s).strip()\n",
        "\n",
        "#         # If the label appears to encode a hybrid, only consider the left-hand side.\n",
        "#         # E.g., \"Bos indicus x Bos taurus\" â†’ keep \"Bos indicus ...\"\n",
        "#         s = PhylogeneticConsensusEngine._HYBRID_SEP.split(s, maxsplit=1)[0]\n",
        "\n",
        "#         # Strip qualifiers at the head (e.g., \"cf. Bos taurus\" â†’ \"Bos taurus\").\n",
        "#         s = re.sub(r\"^(?:cf\\.|aff\\.|nr\\.|gr\\.)\\s+\", \"\", s, flags=re.IGNORECASE)\n",
        "\n",
        "#         # Now match the leading Genus token.\n",
        "#         m = PhylogeneticConsensusEngine._GENUS_PAT.match(s)\n",
        "#         if not m:\n",
        "#             return None\n",
        "\n",
        "#         genus = m.group(\"genus\")\n",
        "#         if return_canonical:\n",
        "#             # Canonical capitalisation: \"Bos\"\n",
        "#             return genus[0].upper() + genus[1:]\n",
        "#         # Normalised lower-case: \"bos\"\n",
        "#         return genus.lower()\n",
        "\n",
        "#     # ------------------------- Public API methods --------------------------\n",
        "\n",
        "#     def is_ready(self) -> bool:\n",
        "#         \"\"\"Return True if the engine initialised without errors.\"\"\"\n",
        "#         return self._is_ready\n",
        "\n",
        "#     def get_mrca_age(self, genera_list: List[str]) -> Tuple[Optional[str], float]:\n",
        "#         \"\"\"\n",
        "#         Find the MRCA for a list of genera and return its node name and age.\n",
        "\n",
        "#         The lookup is case-insensitive. If no valid nodes are found, returns\n",
        "#         (\"No_Valid_Nodes\", 0.0). If the ancestor cannot be determined, returns\n",
        "#         (\"Ancestor_Not_Found\", 0.0).\n",
        "#         \"\"\"\n",
        "#         if not self.is_ready() or not genera_list:\n",
        "#             return None, 0.0\n",
        "\n",
        "#         leaf_nodes = [\n",
        "#             self.genus_to_leaf_map.get(str(g).lower())\n",
        "#             for g in genera_list\n",
        "#             if g is not None and str(g).strip() != \"\"\n",
        "#         ]\n",
        "#         valid_nodes = [node for node in leaf_nodes if node is not None]\n",
        "\n",
        "#         if not valid_nodes:\n",
        "#             return \"No_Valid_Nodes\", 0.0\n",
        "\n",
        "#         ancestor = self.tree.get_common_ancestor(valid_nodes)\n",
        "#         if ancestor is None:\n",
        "#             return \"Ancestor_Not_Found\", 0.0\n",
        "\n",
        "#         age = self.age_lookup.get(str(ancestor.name), 0.0)\n",
        "#         return str(ancestor.name), float(age)\n",
        "\n",
        "#     def weights_from_mrca(self, genera_list: List[str]) -> Dict[str, float]:\n",
        "#         \"\"\"\n",
        "#         Calculate phylogenetic weights for genera based on distance from the MRCA age.\n",
        "\n",
        "#         The weighting here is a simple inverse-distance scheme:\n",
        "#             weight(g) âˆ 1 / (1 + max(0, age(g) - age(MRCA)))\n",
        "#         and is normalised to sum to 1 over the provided genera that are present in the tree.\n",
        "#         \"\"\"\n",
        "#         unique_genera = sorted({g for g in genera_list if g})\n",
        "#         weights = {g: 0.1 for g in unique_genera}  # default baseline\n",
        "\n",
        "#         if not self.is_ready() or not unique_genera:\n",
        "#             return weights\n",
        "\n",
        "#         _, mrca_age = self.get_mrca_age(unique_genera)\n",
        "\n",
        "#         total_inverse_dist = 0.0\n",
        "#         temp_weights: Dict[str, float] = {}\n",
        "\n",
        "#         for genus in unique_genera:\n",
        "#             node = self.genus_to_leaf_map.get(str(genus).lower())\n",
        "#             if node:\n",
        "#                 genus_age = float(self.age_lookup.get(str(node.name), mrca_age))\n",
        "#                 distance = max(0.0, genus_age - mrca_age)\n",
        "#                 inverse_dist = 1.0 / (1.0 + distance)\n",
        "#                 temp_weights[genus] = inverse_dist\n",
        "#                 total_inverse_dist += inverse_dist\n",
        "\n",
        "#         if total_inverse_dist > 0.0:\n",
        "#             for genus, inv_dist in temp_weights.items():\n",
        "#                 weights[genus] = inv_dist / total_inverse_dist\n",
        "\n",
        "#         return weights\n",
        "\n",
        "\n",
        "# # Initialise the engine for use in subsequent cells\n",
        "# phylo_engine = PhylogeneticConsensusEngine(DRIVE_METAZOAN_TREE_PATH, NODE_AGES_CSV_PATH)\n"
      ],
      "metadata": {
        "id": "JD8hlHyEI6bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwT4wZ6sSsZ_"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 45A =====\n",
        "# Pre-flight check for Phylogenetic Engine dependencies\n",
        "\n",
        "logger.info(\"--- ðŸ”Ž Performing pre-flight check for Phylogenetic Engine ---\")\n",
        "\n",
        "# 1. Check for the Newick tree file\n",
        "if not DRIVE_METAZOAN_TREE_PATH.exists():\n",
        "    logger.error(f\"CRITICAL: The Metazoan Newick tree is not found at the expected path.\")\n",
        "    logger.error(f\"Expected path: {DRIVE_METAZOAN_TREE_PATH}\")\n",
        "    logger.error(\"Please verify the path in Cell 12. Halting execution.\")\n",
        "    assert False, \"Missing required Newick tree file.\"\n",
        "else:\n",
        "    logger.info(f\"âœ… Newick tree found: {DRIVE_METAZOAN_TREE_PATH.name}\")\n",
        "\n",
        "# 2. Check for the Node Age Cache file\n",
        "if not NODE_AGES_CSV_PATH.exists():\n",
        "    logger.error(f\"CRITICAL: The pre-computed node age cache is not found.\")\n",
        "    logger.error(f\"Expected path: {NODE_AGES_CSV_PATH}\")\n",
        "    logger.error(\"This is likely because it has not been generated yet.\")\n",
        "    logger.error(\"SOLUTION: Go to Cell 11, set REGENERATE_NODE_AGE_CACHE = True, and re-run from Cell 11.\")\n",
        "    assert False, \"Missing required node age cache. Please regenerate it.\"\n",
        "else:\n",
        "    logger.info(f\"âœ… Node age cache found: {NODE_AGES_CSV_PATH.name}\")\n",
        "\n",
        "# 3. Final check on the engine object itself\n",
        "if not phylo_engine.is_ready():\n",
        "    logger.error(\"CRITICAL: The phylogenetic engine object failed to initialize even though files were found.\")\n",
        "    logger.error(\"This may indicate a problem with the file contents (e.g., corrupted tree).\")\n",
        "    assert False, \"Phylogenetic engine is not ready. Check logs for initialization errors.\"\n",
        "else:\n",
        "    logger.info(\"âœ… Pre-flight check passed. Phylogenetic engine is ready for use.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unAMQyG-ZxBs"
      },
      "source": [
        "# **Part 5: Architecture-Driven Rescue & Correction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWBcIwKXdhRe"
      },
      "source": [
        "## Cell 50 â€“ Generate Seed Consensus & Exon \"Baits\"\n",
        "\n",
        "This cell marks the first pass of our two-pass consensus strategy. It takes the high-confidence \"seed\" exon data and calculates a preliminary consensus architecture using the `phylo_engine`. This robust template is then used to build a library of highly specific regular expression \"baits,\" one for each canonical exon. These baits will be used in the next cell to \"fish\" for exons in the entire working dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 50 =====\n",
        "# Generate Seed Consensus & Build Regex Exon Baits\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "from typing import Iterable, Optional, List\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "logger.info(\"--- Pass 1: Generating high-confidence seed consensus architecture ---\")\n",
        "\n",
        "# ---------- small utilities ----------\n",
        "\n",
        "def _has_cols(df: pd.DataFrame, cols: Iterable[str]) -> bool:\n",
        "    \"\"\"Return True if all columns in `cols` exist in df.\"\"\"\n",
        "    cols = list(cols)\n",
        "    missing = [c for c in cols if c not in df.columns]\n",
        "    return len(missing) == 0\n",
        "\n",
        "def _first_existing_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
        "    \"\"\"Return the first column name that exists in df from candidates, else None.\"\"\"\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _coalesce_columns(df: pd.DataFrame, target: str, sources: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If `target` missing, but any of `sources` exist, create `target` by taking\n",
        "    the first non-null value row-wise across the sources.\n",
        "    \"\"\"\n",
        "    if target in df.columns:\n",
        "        return df\n",
        "    available = [c for c in sources if c in df.columns]\n",
        "    if not available:\n",
        "        return df\n",
        "    df[target] = pd.Series(index=df.index, dtype=object)\n",
        "    if len(available) == 1:\n",
        "        df[target] = df[available[0]]\n",
        "    else:\n",
        "        df[target] = df[available].bfill(axis=1).iloc[:, 0]\n",
        "    return df\n",
        "\n",
        "def _derive_cluster_genus_with_engine(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    If `cluster_genus` is absent or largely null, attempt to derive it from\n",
        "    any available taxon label using the same genus extractor as the engine.\n",
        "    \"\"\"\n",
        "    if \"cluster_genus\" in df.columns and df[\"cluster_genus\"].notna().any():\n",
        "        return df\n",
        "\n",
        "    # Try to find a suitable label column to parse a genus from.\n",
        "    label_col = _first_existing_col(\n",
        "        df,\n",
        "        [\"cluster_label\", \"species\", \"scientific_name\", \"taxon\", \"Entry\", \"entry_name\"]\n",
        "    )\n",
        "    if label_col is None:\n",
        "        return df\n",
        "\n",
        "    # Use the engine's extractor for consistency with the tree vocabulary.\n",
        "    # Fallback: simple first-token on underscore/space if extractor not available.\n",
        "    def _to_genus(label: object) -> Optional[str]:\n",
        "        if label is None or (isinstance(label, float) and np.isnan(label)):\n",
        "            return None\n",
        "        s = str(label)\n",
        "        try:\n",
        "            g = PhylogeneticConsensusEngine.extract_genus(s, return_canonical=False)\n",
        "            if g:\n",
        "                return g\n",
        "        except Exception:\n",
        "            pass\n",
        "        # last resort (very permissive)\n",
        "        token = re.split(r\"[ _]+\", s.strip(), maxsplit=1)[0]\n",
        "        return token.lower() if token else None\n",
        "\n",
        "    df = df.copy()\n",
        "    df[\"cluster_genus\"] = df[label_col].map(_to_genus)\n",
        "    return df\n",
        "\n",
        "def weighted_median(values: np.ndarray, weights: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Compute a weighted median; if inputs are empty or all weights zero, return NaN.\n",
        "    \"\"\"\n",
        "    if values.size == 0 or weights.size == 0:\n",
        "        return float(\"nan\")\n",
        "    wsum = float(np.sum(weights))\n",
        "    if wsum <= 0.0:\n",
        "        return float(\"nan\")\n",
        "    idx = np.argsort(values)\n",
        "    v = values[idx]\n",
        "    w = weights[idx]\n",
        "    c = np.cumsum(w) / wsum\n",
        "    j = min(int(np.searchsorted(c, 0.5)), len(v) - 1)\n",
        "    return float(v[j])\n",
        "\n",
        "\n",
        "# ---------- seed consensus generation ----------\n",
        "\n",
        "if 'df_raw_exons_seed' in globals() and isinstance(df_raw_exons_seed, pd.DataFrame) \\\n",
        "   and not df_raw_exons_seed.empty and phylo_engine.is_ready():\n",
        "\n",
        "    # 1) Build a robust tax_info table from working_df\n",
        "    if 'working_df' not in globals() or not isinstance(working_df, pd.DataFrame) or working_df.empty:\n",
        "        logger.error(\"working_df is not available or empty; cannot annotate seed with taxonomy.\")\n",
        "        consensus_tbl_seed = pd.DataFrame()\n",
        "        assert False, \"Cannot proceed without working_df.\"\n",
        "\n",
        "    # Ensure we have an accession column to merge on.\n",
        "    tax_info_df = working_df.copy()\n",
        "\n",
        "    # If 'Entry' is your accession, keep it; otherwise try to coalesce to 'accession'.\n",
        "    if \"Entry\" in tax_info_df.columns and \"accession\" not in tax_info_df.columns:\n",
        "        tax_info_df = tax_info_df.rename(columns={\"Entry\": \"accession\"})\n",
        "    elif \"accession\" not in tax_info_df.columns:\n",
        "        # Try common alternatives.\n",
        "        acc_col = _first_existing_col(tax_info_df, [\"Accession\", \"uniprot_id\", \"uniprot_accession\"])\n",
        "        if acc_col:\n",
        "            tax_info_df = tax_info_df.rename(columns={acc_col: \"accession\"})\n",
        "        else:\n",
        "            logger.error(\"No accession column found in working_df (expected 'Entry' or 'accession').\")\n",
        "            consensus_tbl_seed = pd.DataFrame()\n",
        "            assert False, \"Cannot proceed without an accession column in working_df.\"\n",
        "\n",
        "    # Bring gene_symbol into a canonical name.\n",
        "    if \"gene_symbol_norm\" in tax_info_df.columns:\n",
        "        tax_info_df = tax_info_df.rename(columns={\"gene_symbol_norm\": \"gene_symbol\"})\n",
        "    elif \"gene_symbol\" not in tax_info_df.columns:\n",
        "        # Try fallback columns; if none exist, leave missing and we'll handle after merge.\n",
        "        gs_fallback = _first_existing_col(tax_info_df, [\"Gene\", \"gene\", \"symbol\", \"GN\"])\n",
        "        if gs_fallback:\n",
        "            tax_info_df = tax_info_df.rename(columns={gs_fallback: \"gene_symbol\"})\n",
        "\n",
        "    # Ensure cluster_genus exists (coalesce common suffix variants)\n",
        "    tax_info_df = _coalesce_columns(\n",
        "        tax_info_df,\n",
        "        target=\"cluster_genus\",\n",
        "        sources=[\"cluster_genus\", \"cluster_genus_x\", \"cluster_genus_y\"]\n",
        "    )\n",
        "    # If still missing or mostly null, derive it from a label via the engine extractor.\n",
        "    tax_info_df = _derive_cluster_genus_with_engine(tax_info_df)\n",
        "\n",
        "    # Keep only the columns we need for the merge\n",
        "    cols_to_keep = [c for c in [\"accession\", \"cluster_genus\", \"gene_symbol\"] if c in tax_info_df.columns]\n",
        "    tax_info_df = tax_info_df[cols_to_keep].drop_duplicates()\n",
        "\n",
        "    # 2) Make sure df_raw_exons_seed has an accession to join on\n",
        "    if \"accession\" not in df_raw_exons_seed.columns:\n",
        "        # Try to rename common fields to 'accession'\n",
        "        ex_acc = _first_existing_col(df_raw_exons_seed, [\"Entry\", \"Accession\", \"uniprot_id\", \"uniprot_accession\"])\n",
        "        if ex_acc:\n",
        "            df_raw_exons_seed = df_raw_exons_seed.rename(columns={ex_acc: \"accession\"})\n",
        "        else:\n",
        "            logger.error(\"df_raw_exons_seed lacks an accession column to merge on.\")\n",
        "            consensus_tbl_seed = pd.DataFrame()\n",
        "            assert False, \"Cannot proceed without a common accession key.\"\n",
        "\n",
        "    # 3) Merge, then coalesce any suffixed columns back\n",
        "    seed_with_tax = df_raw_exons_seed.merge(tax_info_df, on=\"accession\", how=\"left\")\n",
        "\n",
        "    # After merges, Pandas can suffix duplicate names; coalesce them explicitly.\n",
        "    # (Do this even if you do not expect collisions; it prevents intermittent KeyErrors.)\n",
        "    for base in (\"cluster_genus\", \"gene_symbol\"):\n",
        "        if base not in seed_with_tax.columns:\n",
        "            left = f\"{base}_x\"\n",
        "            right = f\"{base}_y\"\n",
        "            if left in seed_with_tax.columns or right in seed_with_tax.columns:\n",
        "                seed_with_tax = _coalesce_columns(seed_with_tax, base, [left, right])\n",
        "\n",
        "    # 4) Enforce presence of required columns safely (avoid KeyError in dropna)\n",
        "    required = [\"cluster_genus\", \"gene_symbol\"]\n",
        "    missing_now = [c for c in required if c not in seed_with_tax.columns]\n",
        "    if missing_now:\n",
        "        logger.error(\"Missing required columns after merge: %s\", missing_now)\n",
        "        consensus_tbl_seed = pd.DataFrame()\n",
        "        assert False, f\"Cannot proceed without columns: {missing_now}\"\n",
        "\n",
        "    # 5) Filter to rows that actually have both fields\n",
        "    seed_with_tax = seed_with_tax.dropna(subset=[\"cluster_genus\", \"gene_symbol\"])\n",
        "\n",
        "    # ---------- weighted consensus ----------\n",
        "\n",
        "    def _aligned_weighted_median(series: pd.Series, row_weights: pd.Series) -> float:\n",
        "        \"\"\"\n",
        "        Compute a weighted median for a numeric series, aligning weights to the series' index.\n",
        "        \"\"\"\n",
        "        snum = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
        "        if snum.empty:\n",
        "            return float(\"nan\")\n",
        "        # Align weights to the numeric series' index\n",
        "        w = row_weights.reindex(snum.index).fillna(0.0).to_numpy(dtype=float)\n",
        "        if np.sum(w) == 0.0:\n",
        "            w = np.ones_like(snum.to_numpy(), dtype=float)\n",
        "        return weighted_median(snum.to_numpy(dtype=float), w)\n",
        "\n",
        "    seed_consensus_rows = []\n",
        "    grouped = seed_with_tax.groupby([\"gene_symbol\", \"exon_num_in_chain\"], sort=False)\n",
        "\n",
        "    for (g, e), df_grp in tqdm(grouped, desc=\"Building Seed Consensus\"):\n",
        "        # Compute phylogenetic weights per genus present in this group\n",
        "        genera = df_grp[\"cluster_genus\"].astype(str).str.lower().unique().tolist()\n",
        "        weights_by_genus = phylo_engine.weights_from_mrca(genera)\n",
        "\n",
        "        # Map each row to its genus weight (default 0.0); will be re-aligned per series below\n",
        "        row_w = df_grp[\"cluster_genus\"].astype(str).str.lower().map(weights_by_genus).fillna(0.0)\n",
        "\n",
        "        cb = _aligned_weighted_median(df_grp[\"begin_aa\"], row_w)\n",
        "        ce = _aligned_weighted_median(df_grp[\"end_aa\"], row_w)\n",
        "\n",
        "        if not np.isnan(cb) and not np.isnan(ce):\n",
        "            seed_consensus_rows.append(\n",
        "                {\"gene_symbol\": g, \"exon_num_in_chain\": e, \"cons_begin\": int(round(cb)), \"cons_end\": int(round(ce))}\n",
        "            )\n",
        "\n",
        "    consensus_tbl_seed = pd.DataFrame(seed_consensus_rows)\n",
        "    logger.info(\"Generated seed consensus for %d canonical exons.\", len(consensus_tbl_seed))\n",
        "\n",
        "else:\n",
        "    consensus_tbl_seed = pd.DataFrame()\n",
        "    logger.error(\"No seed exon data or phylogenetic engine available to build consensus. Halting rescue.\")\n",
        "    if 'phylo_engine' in globals() and not phylo_engine.is_ready():\n",
        "        logger.error(\"Phylogenetic Engine failed to initialise. Check paths in Cell 12 and cache in Cell 15.\")\n",
        "    assert False, \"Cannot proceed without seed consensus.\"\n",
        "\n",
        "# --- Step 2: Build Regex Baits (Logic from original RegExTractor) ---\n",
        "logger.info(\"--- Building Regex 'Baits' from Seed Consensus ---\")\n",
        "\n",
        "@dataclass\n",
        "class ExonBait:\n",
        "    regex: re.Pattern\n",
        "    median_len: int\n",
        "\n",
        "regex_baits_library: dict[tuple[str, int], ExonBait] = {}\n",
        "\n",
        "if not consensus_tbl_seed.empty:\n",
        "    grouped = seed_with_tax.groupby([\"gene_symbol\", \"exon_num_in_chain\"], sort=False)\n",
        "    for (gene, exon_num), group_df in tqdm(grouped, desc=\"Building Regex Baits\"):\n",
        "        peptides = group_df[\"peptide\"].dropna().tolist()\n",
        "        if len(peptides) < 5:\n",
        "            continue\n",
        "\n",
        "        lens = [len(p) for p in peptides if p]\n",
        "        if not lens:\n",
        "            continue\n",
        "        median_len = int(np.median(lens))\n",
        "        if median_len <= 0:\n",
        "            continue\n",
        "\n",
        "        pattern_parts: list[str] = []\n",
        "        for i in range(median_len):\n",
        "            chars_at_pos = [p[i] for p in peptides if len(p) > i]\n",
        "            if not chars_at_pos:\n",
        "                continue\n",
        "\n",
        "            counts = defaultdict(int)\n",
        "            for ch in chars_at_pos:\n",
        "                counts[ch] += 1\n",
        "            total = sum(counts.values())\n",
        "            keep = [c for c, n in counts.items() if (n / total) >= 0.05]\n",
        "\n",
        "            if len(keep) > 1:\n",
        "                # sort for reproducibility; escape only if non-alnum appears\n",
        "                part = \"[\" + \"\".join(sorted(keep)) + \"]\"\n",
        "                pattern_parts.append(part)\n",
        "            elif len(keep) == 1:\n",
        "                pattern_parts.append(re.escape(keep[0]))\n",
        "            else:\n",
        "                pattern_parts.append(\".\")\n",
        "\n",
        "        regex_str = \"\".join(pattern_parts)\n",
        "        try:\n",
        "            compiled = re.compile(regex_str)\n",
        "        except re.error as exc:\n",
        "            logger.error(\"Failed to compile regex for (%s, %s): %s\", gene, exon_num, exc)\n",
        "            continue\n",
        "\n",
        "        regex_baits_library[(gene, exon_num)] = ExonBait(regex=compiled, median_len=median_len)\n",
        "\n",
        "    logger.info(\"Successfully built %d high-confidence exon baits.\", len(regex_baits_library))\n"
      ],
      "metadata": {
        "id": "I1UhhK9gKxZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a7Ropd0dqLM"
      },
      "source": [
        "## Cell 51 â€“ Architecture-Driven Rescue Runner\n",
        "\n",
        "This is the core of the \"fishing\" operation. It iterates through every sequence in the entire `working_df`. For each sequence, it uses the regex bait library to find the best match for *every expected exon* in the consensus architecture defined in the previous cell. If an exon cannot be found, it is marked as `MISSING_EXON`, ensuring that the final output is a complete and uniformly structured dataset for every protein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "935G1MwPdtrd"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 51 =====\n",
        "# Architecture-Driven Rescue Runner (\"Fishing\")\n",
        "\n",
        "logger.info(\"--- Starting Architecture-Driven Rescue ('Fishing') ---\")\n",
        "rescued_rows = []\n",
        "\n",
        "# Get a lookup of expected exons for each gene from the seed consensus\n",
        "expected_exons_by_gene = consensus_tbl_seed.groupby('gene_symbol')['exon_num_in_chain'].apply(list).to_dict()\n",
        "\n",
        "if 'working_df' in globals() and not working_df.empty:\n",
        "    for _, row in tqdm(working_df.iterrows(), total=len(working_df), desc=\"Rescuing Exon Architectures\"):\n",
        "        accession = row['Entry']\n",
        "        gene = row['gene_symbol_norm']\n",
        "        sequence = row['Sequence']\n",
        "\n",
        "        expected_exons = sorted(expected_exons_by_gene.get(gene, []))\n",
        "        if not expected_exons:\n",
        "            continue\n",
        "\n",
        "        last_exon_end = 0\n",
        "        for exon_num in expected_exons:\n",
        "            bait_info = regex_baits_library.get((gene, exon_num))\n",
        "            if not bait_info:\n",
        "                # If no bait, create a missing placeholder\n",
        "                rescued_rows.append({'accession': accession, 'gene_symbol': gene, 'exon_num_in_chain': exon_num, 'peptide': 'NO_BAIT_DEFINED'})\n",
        "                continue\n",
        "\n",
        "            # Search for the pattern, starting from the end of the last found exon\n",
        "            best_match = None\n",
        "            search_region = sequence[last_exon_end:]\n",
        "\n",
        "            for match in bait_info.regex.finditer(search_region):\n",
        "                # A simple scoring: prioritize matches closest to the expected length\n",
        "                length_diff = abs(len(match.group(0)) - bait_info.median_len)\n",
        "                if best_match is None or length_diff < best_match['score']:\n",
        "                    best_match = {'match': match, 'score': length_diff}\n",
        "\n",
        "            if best_match:\n",
        "                match_obj = best_match['match']\n",
        "                start, end = match_obj.span()\n",
        "                # Adjust coordinates to be relative to the full sequence\n",
        "                start += last_exon_end\n",
        "                end += last_exon_end\n",
        "\n",
        "                rescued_rows.append({\n",
        "                    'accession': accession, 'gene_symbol': gene, 'exon_num_in_chain': exon_num,\n",
        "                    'begin_aa': start + 1, # 1-based coordinates\n",
        "                    'end_aa': end,\n",
        "                    'peptide': match_obj.group(0),\n",
        "                    'source': 'rescued'\n",
        "                })\n",
        "                last_exon_end = end\n",
        "            else:\n",
        "                # If no match found, this exon is missing\n",
        "                rescued_rows.append({\n",
        "                    'accession': accession, 'gene_symbol': gene, 'exon_num_in_chain': exon_num,\n",
        "                    'peptide': 'MISSING_EXON', 'source': 'rescued_missing'\n",
        "                })\n",
        "\n",
        "    df_rescued_exons = pd.DataFrame(rescued_rows)\n",
        "    logger.info(f\"âœ… Rescue complete. Reconstructed architectures for {df_rescued_exons['accession'].nunique()} proteins.\")\n",
        "    logger.info(f\"Found {len(df_rescued_exons[df_rescued_exons['source'] == 'rescued'])} exons.\")\n",
        "    logger.info(f\"Identified {len(df_rescued_exons[df_rescued_exons['source'] == 'rescued_missing'])} missing exons.\")\n",
        "else:\n",
        "    df_rescued_exons = pd.DataFrame()\n",
        "    logger.warning(\"Working dataframe is empty. Skipping rescue.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMMJSS4Wef0M"
      },
      "source": [
        "## Cell 52 â€“ Consolidate Data and Finalize Raw Exon Cache\n",
        "\n",
        "This cell finalizes the rescue process. It merges the original high-confidence seed data with the newly rescued exon data, ensuring a complete and consistent dataset. This consolidated data is then atomically written to the master `raw_exons_cache.tsv`, overwriting older, less complete entries. Finally, it reloads this master cache into the main `df_raw_exons` DataFrame, which will be used for all downstream analyses in Part 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLKQZDUBedYh"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 52 =====\n",
        "# Consolidate Rescued Data and Update Master Cache\n",
        "\n",
        "logger.info(\"--- Consolidating seed and rescued exon data ---\")\n",
        "\n",
        "if 'df_rescued_exons' in globals() and not df_rescued_exons.empty:\n",
        "    # Mark the seed data for clarity\n",
        "    df_raw_exons_seed['source'] = 'seed'\n",
        "\n",
        "    # Combine the dataframes\n",
        "    df_complete_exons = pd.concat([df_raw_exons_seed, df_rescued_exons], ignore_index=True)\n",
        "\n",
        "    # The key to consolidation: remove duplicates, preferring the rescued version\n",
        "    # because it is part of a complete, reconstructed architecture.\n",
        "    key_cols = ['accession', 'gene_symbol', 'exon_num_in_chain']\n",
        "    df_complete_exons.sort_values('source', ascending=False, inplace=True) # puts 'rescued' before 'seed'\n",
        "    df_complete_exons.drop_duplicates(subset=key_cols, keep='first', inplace=True)\n",
        "\n",
        "    logger.info(\"Merging rescued data into the master exon cache...\")\n",
        "    atomic_merge_into_cache(df_complete_exons)\n",
        "\n",
        "else:\n",
        "    logger.warning(\"No rescued exons to consolidate. Using only seed data.\")\n",
        "\n",
        "# --- CRITICAL STEP: Reload the global df_raw_exons from the now-complete cache ---\n",
        "logger.info(\"Reloading the master raw exons dataset for final analysis...\")\n",
        "if RAW_EXONS_CACHE.exists():\n",
        "    df_raw_exons = safe_read_tsv(RAW_EXONS_CACHE)\n",
        "    logger.info(f\"âœ… Loaded final, complete raw exons dataset with {len(df_raw_exons)} rows.\")\n",
        "else:\n",
        "    df_raw_exons = pd.DataFrame()\n",
        "    logger.warning(\"Master raw exons cache is not available for Part 5.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UT4LTSN9Y57"
      },
      "source": [
        "# **Part 6: Final Consensus & Evolutionary Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXMjgSsB9dn7"
      },
      "source": [
        "## Cell 61 â€“ Weighted Consensus Calculation\n",
        "\n",
        "This cell calculates the consensus start and end coordinates for each exon across all species. It uses the `PhylogeneticConsensusEngine` for weighting and applies an adaptive tolerance based on the phylogenetic diversity of the group. **Crucially, it first validates and backfills taxonomic data into older cache entries to ensure data consistency.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc3TWWSoWElj"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 61 =====\n",
        "# FAST weighted consensus calculation (with integrated cache upgrade)\n",
        "\n",
        "def weighted_median(values: np.ndarray, weights: np.ndarray) -> float:\n",
        "    \"\"\"Computes the weighted median of an array.\"\"\"\n",
        "    if len(values) == 0: return float(\"nan\")\n",
        "    idx = np.argsort(values)\n",
        "    v, w = values[idx], weights[idx]\n",
        "    c = np.cumsum(w) / np.sum(w)\n",
        "    j = min(np.searchsorted(c, 0.5), len(v)-1)\n",
        "    return float(v[j])\n",
        "\n",
        "def adaptive_tolerance(depth_proxy: float) -> int:\n",
        "    \"\"\"Calculates boundary tolerance based on phylogenetic depth.\"\"\"\n",
        "    if pd.isna(depth_proxy): return 3\n",
        "    x = float(np.clip(depth_proxy, 0.0, 3.0))\n",
        "    return int(round(2.0 + x))\n",
        "\n",
        "consensus_rows, refined_rows = [], []\n",
        "consensus_tbl, consensus_long = pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "if 'df_raw_exons' in globals() and not df_raw_exons.empty:\n",
        "    base_df = df_raw_exons.copy()\n",
        "    base_df = base_df[base_df['peptide'] != 'MAPPING_FAILED'].copy()\n",
        "\n",
        "    # --- ROBUST CACHE UPGRADE AND VALIDATION ---\n",
        "    required_cols = ['cluster_genus', 'Family', 'Order']\n",
        "    missing_cols = [c for c in required_cols if c not in base_df.columns]\n",
        "\n",
        "    if missing_cols:\n",
        "        logger.warning(f\"Cache is missing required columns: {missing_cols}. Backfilling from current working_df...\")\n",
        "        tax_cols_to_add = ['Entry', 'cluster_genus', 'Family', 'Order', 'Clas_id']\n",
        "        available_tax_cols = [c for c in tax_cols_to_add if c in working_df.columns]\n",
        "\n",
        "        if 'Entry' in available_tax_cols:\n",
        "            lineage_lut = working_df[available_tax_cols].drop_duplicates(subset=['Entry'])\n",
        "            lineage_lut = lineage_lut.rename(columns={'Entry': 'accession'})\n",
        "            if 'organism' in base_df.columns: base_df = base_df.drop(columns=['organism'])\n",
        "            base_df = pd.merge(base_df, lineage_lut, on='accession', how='left')\n",
        "            logger.info(\"Cache backfill complete.\")\n",
        "\n",
        "    initial_rows = len(base_df)\n",
        "    base_df.dropna(subset=['cluster_genus'], inplace=True)\n",
        "    final_rows = len(base_df)\n",
        "    if initial_rows > final_rows:\n",
        "        logger.info(f\"Removed {initial_rows - final_rows} orphaned exon rows from cache that are not in the current working set.\")\n",
        "\n",
        "    grouped = base_df.groupby(['gene_symbol', 'exon_num_in_chain'], sort=False)\n",
        "    logger.info(f\"Calculating consensus for {len(grouped)} exon groups...\")\n",
        "\n",
        "    for (g, e), df in tqdm(grouped, desc=\"Calculating Consensus\"):\n",
        "        genera = df['cluster_genus'].tolist()\n",
        "        weights_by_genus = phylo_engine.weights_from_mrca(genera)\n",
        "\n",
        "        ds = [(1.0/max(1e-9, w) - 1.0) for w in weights_by_genus.values()]\n",
        "        mean_depth = float(np.mean(ds)) if ds else float('nan')\n",
        "        tol = adaptive_tolerance(mean_depth)\n",
        "\n",
        "        w = df['cluster_genus'].map(weights_by_genus).fillna(0.0).to_numpy()\n",
        "        if w.sum() == 0: w = np.ones(len(df)) / max(1, len(df))\n",
        "\n",
        "        b = df['begin_aa'].astype(float).to_numpy()\n",
        "        epos = df['end_aa'].astype(float).to_numpy()\n",
        "        cb = int(round(weighted_median(b, w)))\n",
        "        ce = int(round(weighted_median(epos, w)))\n",
        "\n",
        "        consensus_rows.append({\n",
        "            'gene_symbol': g, 'exon_num_in_chain': e,\n",
        "            'cons_begin': cb, 'cons_end': ce,\n",
        "            'tolerance_aa': tol, 'depth_proxy': mean_depth\n",
        "        })\n",
        "\n",
        "        for _, r in df.iterrows():\n",
        "            db, de = int(r['begin_aa']) - cb, int(r['end_aa']) - ce\n",
        "            refined_rows.append({\n",
        "                'accession': r['accession'], 'gene_symbol': g, 'exon_num_in_chain': e,\n",
        "                'begin_aa': int(r['begin_aa']), 'end_aa': int(r['end_aa']),\n",
        "                'peptide': r.get('peptide',''), 'cluster_genus': r.get('cluster_genus',''),\n",
        "                'cons_begin': cb, 'cons_end': ce, 'delta_begin': db, 'delta_end': de,\n",
        "                'boundary_ok': (abs(db) <= tol) and (abs(de) <= tol)\n",
        "            })\n",
        "\n",
        "    consensus_tbl = pd.DataFrame(consensus_rows)\n",
        "    consensus_long = pd.DataFrame(refined_rows)\n",
        "\n",
        "    consensus_tbl.to_csv(CONSENSUS_TABLE_TSV, sep='\\t', index=False)\n",
        "    consensus_long.to_csv(CONSENSUS_LONG_TSV, sep='\\t', index=False)\n",
        "    logger.info(f\"Consensus calculated. Long table: {len(consensus_long)} rows, Consensus table: {len(consensus_tbl)} rows.\")\n",
        "    logger.info(f\"Results saved to {CONSENSUS_LONG_TSV.name} and {CONSENSUS_TABLE_TSV.name}\")\n",
        "else:\n",
        "    logger.warning(\"No raw exon data available; skipping consensus calculation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6hFlOYQ9gqn"
      },
      "source": [
        "## Cell 62 â€“ MRCA Level & Reliability Scoring\n",
        "\n",
        "This cell determines the deepest taxonomic level (e.g., Genus, Family) at which exon boundaries remain consistent. It also computes a 0-100 reliability score for each exon group, combining phylogenetic diversity, boundary consistency, and sequence quality metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxsyvwsoAM11"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 62 =====\n",
        "# MRCA level + reliability scoring\n",
        "\n",
        "def mrca_level_for_group(df: pd.DataFrame, tol: int) -> str:\n",
        "    \"\"\"Determines the deepest taxonomic rank with consistent exon boundaries.\"\"\"\n",
        "    levels = [('cluster_genus', 'Genus'), ('Family', 'Family'), ('Order', 'Order'), ('Clas_id', 'Class')]\n",
        "    for col, label in levels:\n",
        "        if col not in df.columns: continue\n",
        "        is_consistent = True\n",
        "        for _, sub in df.groupby(df[col].astype(str).fillna('NA')):\n",
        "            spread = (sub['end_aa'] - sub['begin_aa']).std()\n",
        "            if pd.notna(spread) and spread > tol:\n",
        "                is_consistent = False; break\n",
        "        if is_consistent: return label\n",
        "    return \"None\"\n",
        "\n",
        "def reliability_score_for_group(df: pd.DataFrame, tol: int) -> float:\n",
        "    \"\"\"Computes a 0-100 reliability score for an exon group.\"\"\"\n",
        "    n_genera = df['cluster_genus'].nunique()\n",
        "    n_orders = df['Order'].nunique() if 'Order' in df.columns else 0\n",
        "    phylo_div = min(1.0, 0.2 * n_genera + 0.1 * n_orders)\n",
        "    ok_frac = float((df['boundary_ok'] == True).mean())\n",
        "    seq_q = df.get('quality_score', pd.Series([50.0])).fillna(50.0).mean() / 100.0\n",
        "    tol_factor = 1.0 - np.clip((tol - 2) / 3.0, 0, 1) * 0.2 # Penalize wide tolerance\n",
        "    score = 100.0 * (0.45 * phylo_div + 0.35 * ok_frac + 0.20 * seq_q) * tol_factor\n",
        "    return float(np.clip(score, 0, 100))\n",
        "\n",
        "mrca_rows = []\n",
        "\n",
        "if not consensus_long.empty and not consensus_tbl.empty:\n",
        "    logger.info(\"Building a comprehensive taxonomic lookup from the master dataset...\")\n",
        "    lineage_cols = ['Entry', 'Clas_id', 'Order', 'Family', 'cluster_genus']\n",
        "\n",
        "    source_df = full_df if 'full_df' in globals() and not full_df.empty else working_df\n",
        "\n",
        "    lineage_lut = source_df[[c for c in lineage_cols if c in source_df.columns]].drop_duplicates(subset=['Entry'])\n",
        "    lineage_lut = lineage_lut.rename(columns={'Entry':'accession'})\n",
        "\n",
        "    q_cols = ['accession', 'exon_num_in_chain', 'quality_score']\n",
        "    q_lut = df_raw_exons[[c for c in q_cols if c in df_raw_exons.columns]].drop_duplicates()\n",
        "\n",
        "    merged = consensus_long.merge(consensus_tbl, on=['gene_symbol','exon_num_in_chain'])\n",
        "    merged = merged.merge(lineage_lut, on='accession', how='left')\n",
        "    if 'quality_score' in q_lut.columns:\n",
        "        merged = merged.merge(q_lut, on=['accession','exon_num_in_chain'], how='left')\n",
        "\n",
        "    for (g, e), sub in tqdm(merged.groupby(['gene_symbol','exon_num_in_chain']), desc=\"Scoring Reliability\"):\n",
        "        if 'cluster_genus' not in sub.columns or sub['cluster_genus'].isna().all():\n",
        "            logger.warning(f\"Skipping scoring for {g} exon {e} due to missing taxonomic data even after master lookup.\")\n",
        "            mrca = \"N/A\"\n",
        "            rel = 0.0\n",
        "        else:\n",
        "            tol = int(sub['tolerance_aa'].iloc[0])\n",
        "            mrca = mrca_level_for_group(sub, tol)\n",
        "            # --- FIX: Pass the 'tol' argument to the function call ---\n",
        "            rel = reliability_score_for_group(sub.dropna(subset=['cluster_genus']), tol)\n",
        "\n",
        "        mrca_rows.append({\n",
        "            'gene_symbol': g, 'exon_num_in_chain': e,\n",
        "            'MRCA_level': mrca, 'reliability_score': round(rel, 1),\n",
        "            'tolerance_aa': sub['tolerance_aa'].iloc[0],\n",
        "            'depth_proxy': sub['depth_proxy'].iloc[0]\n",
        "        })\n",
        "\n",
        "mrca_df = pd.DataFrame(mrca_rows)\n",
        "if not mrca_df.empty:\n",
        "    logger.info(f\"MRCA/reliability computed for {len(mrca_df)} exon groups.\")\n",
        "else:\n",
        "    logger.warning(\"No consensus data to calculate MRCA/reliability scores.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-cell-53-markdown"
      },
      "source": [
        "## Cell 63 â€“ Evolutionary Event Dating\n",
        "\n",
        "This cell leverages the phylogenetic dating engine to analyze the `consensus_long` table. It identifies subgroups of species that share specific structural traits (e.g., a shift in an exon's start boundary) and uses the pre-computed node age cache to estimate *when* that trait likely evolved by finding the age of the Most Recent Common Ancestor (MRCA) of the subgroup. The results provide a quantitative model of how the gene's exon architecture evolved over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new-cell-53-code"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 63 =====\n",
        "# Model Exon Evolution by Dating Structural Changes\n",
        "\n",
        "if 'consensus_long' in globals() and not consensus_long.empty and 'phylo_engine' in globals():\n",
        "    logger.info(\"Modeling exon evolution by dating structural changes...\")\n",
        "\n",
        "    evolutionary_events = []\n",
        "\n",
        "    # Group by each consensus exon (gene + exon number)\n",
        "    grouped = consensus_long.groupby(['gene_symbol', 'exon_num_in_chain'])\n",
        "\n",
        "    for (gene, exon_num), group_df in tqdm(grouped, desc=\"Dating Evolutionary Events\"):\n",
        "\n",
        "        # 1. Find the BASAL MRCA for the existence of this exon in this dataset\n",
        "        all_genera_in_group = group_df['cluster_genus'].unique().tolist()\n",
        "        basal_mrca_name, basal_mrca_age = phylo_engine.get_mrca_age(all_genera_in_group)\n",
        "\n",
        "        # 2. Identify subgroups based on shared structural TRAITS\n",
        "        # Example trait: N-terminal boundary shifts (delta_begin != 0)\n",
        "        boundary_shifts = group_df['delta_begin'].unique()\n",
        "        for shift in boundary_shifts:\n",
        "            if shift == 0: continue # This is the consensus state, not an event\n",
        "\n",
        "            subgroup_df = group_df[group_df['delta_begin'] == shift]\n",
        "            subgroup_genera = subgroup_df['cluster_genus'].unique().tolist()\n",
        "\n",
        "            # 3. Find the MRCA for the subgroup that shares the trait\n",
        "            event_mrca_name, event_mrca_age = phylo_engine.get_mrca_age(subgroup_genera)\n",
        "\n",
        "            if event_mrca_age > 0:\n",
        "                evolutionary_events.append({\n",
        "                    \"gene\": gene,\n",
        "                    \"exon\": exon_num,\n",
        "                    \"event_type\": \"boundary_shift\",\n",
        "                    \"event_detail\": f\"delta_begin = {int(shift)}\",\n",
        "                    \"basal_mrca_age_MYA\": round(basal_mrca_age, 2),\n",
        "                    \"event_mrca_age_MYA\": round(event_mrca_age, 2),\n",
        "                    \"branch_of_origin_MY\": round(basal_mrca_age - event_mrca_age, 2),\n",
        "                    \"event_mrca_name\": event_mrca_name,\n",
        "                    \"num_taxa_with_event\": len(subgroup_df)\n",
        "                })\n",
        "\n",
        "    # 4. Create and save the final event DataFrame\n",
        "    if evolutionary_events:\n",
        "        df_evolution = pd.DataFrame(evolutionary_events)\n",
        "        df_evolution.to_csv(EVOLUTION_EVENTS_TSV, sep='\\t', index=False)\n",
        "        logger.info(f\"âœ… Identified and dated {len(df_evolution)} evolutionary events.\")\n",
        "        logger.info(f\"Saved event model to: {EVOLUTION_EVENTS_TSV.name}\")\n",
        "        display(df_evolution.head())\n",
        "    else:\n",
        "        logger.info(\"No distinct evolutionary events (like boundary shifts) were identified.\")\n",
        "\n",
        "else:\n",
        "    logger.warning(\"Skipping evolutionary event dating: consensus data or phylo engine not available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVJsaFtQ9k4I"
      },
      "source": [
        "## Cell 64 â€“ Final Wide Architecture Generation\n",
        "\n",
        "This cell transforms the long-format consensus data into a wide-format table where each row represents a single protein sequence and columns represent individual exons. This \"exon architecture\" format is useful for comparative analysis and visualization. The process is memory-optimized to handle large datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggOOUEiABCPR"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 64 =====\n",
        "# Final wide architecture with memory management\n",
        "\n",
        "def create_wide_architecture(\n",
        "    long_df: pd.DataFrame,\n",
        "    tbl_df: pd.DataFrame,\n",
        "    mrca_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Builds the final wide-format exon architecture dataframe.\"\"\"\n",
        "    if long_df.empty: return pd.DataFrame()\n",
        "\n",
        "    logger.info(f\"Creating wide architecture from {long_df['accession'].nunique()} accessions.\")\n",
        "\n",
        "    # 1. Pivot the long table to create exon-specific columns for sequence data\n",
        "    pivoted = long_df.pivot_table(\n",
        "        index=['accession', 'gene_symbol'],\n",
        "        columns='exon_num_in_chain',\n",
        "        values=['peptide', 'begin_aa', 'end_aa', 'boundary_ok'],\n",
        "        aggfunc='first'\n",
        "    )\n",
        "    pivoted.columns = [f\"exon_{v}_{c}\" if c != '' else v for v, c in pivoted.columns]\n",
        "    pivoted.reset_index(inplace=True)\n",
        "\n",
        "    # 2. Prepare a separate, wide-format table for consensus and reliability info\n",
        "    annot = tbl_df.merge(mrca_df, on=['gene_symbol', 'exon_num_in_chain'], how='left')\n",
        "\n",
        "    annot['cons_coords'] = annot['cons_begin'].astype(str) + '-' + annot['cons_end'].astype(str)\n",
        "    annot = annot.rename(columns={'reliability_score': 'cons_reliability'})\n",
        "\n",
        "    # Pivot the annotation data to a wide format\n",
        "    consensus_wide = annot.pivot_table(\n",
        "        index='gene_symbol',\n",
        "        columns='exon_num_in_chain',\n",
        "        values=['cons_coords', 'cons_reliability'],\n",
        "        # --- FIX: Explicitly set aggfunc to 'first' to handle string data ---\n",
        "        # The default 'mean' fails on string columns like 'cons_coords'.\n",
        "        aggfunc='first'\n",
        "    )\n",
        "    # Flatten the multi-level column index from the pivot\n",
        "    consensus_wide.columns = [f\"cons_exon_{v}_{c}\" for v, c in consensus_wide.columns]\n",
        "    consensus_wide.reset_index(inplace=True)\n",
        "\n",
        "    # 3. Perform a single, efficient merge to combine the two wide tables\n",
        "    final_wide_df = pd.merge(pivoted, consensus_wide, on='gene_symbol', how='left')\n",
        "\n",
        "    return final_wide_df\n",
        "\n",
        "if 'consensus_long' in globals() and not consensus_long.empty:\n",
        "    wide_df = create_wide_architecture(consensus_long, consensus_tbl, mrca_df)\n",
        "    if not wide_df.empty:\n",
        "        wide_df = wide_df.copy()\n",
        "        wide_df.to_csv(WIDE_ARCH_TSV, sep='\\t', index=False)\n",
        "        logger.info(f\"Final wide architecture saved: {len(wide_df)} rows, {len(wide_df.columns)} columns.\")\n",
        "        logger.info(f\"File saved to: {WIDE_ARCH_TSV}\")\n",
        "    else:\n",
        "        logger.warning(\"Wide DataFrame is empty after processing.\")\n",
        "else:\n",
        "    logger.warning(\"No consensus data available; skipping final wide architecture.\")\n",
        "    wide_df = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4FYYD_T9oZu"
      },
      "source": [
        "# **Part 7: RegExTractor â€“ Gene Classification Engine**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdvpzElBjEur"
      },
      "source": [
        "## Cell 70 â€“ RegExTractor Configuration\n",
        "\n",
        "This cell defines the user-configurable parameters for the RegExTractor engine. In its new role, the engine uses the final, high-quality consensus architectures to build orthology-aware patterns. It then scans a pool of previously rejected or unclassified sequences to predict their gene identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3-KplhSjG4M"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 70 =====\n",
        "# RegExTractor configuration\n",
        "\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# --- User-tunable parameters for RegExTractor ---\n",
        "rex_min_clade_samples = 5           #@param {type:\"integer\"}\n",
        "rex_anchor_triplets_min = 2         #@param {type:\"integer\"}\n",
        "rex_anchor_triplets_max = 4         #@param {type:\"integer\"}\n",
        "rex_anchor_entropy_max = 0.25       #@param {type:\"number\"}\n",
        "rex_freq_threshold_strict = 0.05    #@param {type:\"number\"}\n",
        "rex_freq_threshold_moderate = 0.01  #@param {type:\"number\"}\n",
        "rex_ghead_density_min = 0.80        #@param {type:\"number\"}\n",
        "rex_len_tolerance_strict = 1        #@param {type:\"integer\"}\n",
        "rex_len_tolerance_moderate = 3      #@param {type:\"integer\"}\n",
        "rex_len_tolerance_loose = 6         #@param {type:\"integer\"}\n",
        "rex_chain_min_consecutive = 3       #@param {type:\"integer\"}\n",
        "rex_search_window_pad = 90          #@param {type:\"integer\"}\n",
        "\n",
        "# --- New Path Variables for Classification Output ---\n",
        "# NOTE: These should be added to the path definitions in Cell 12\n",
        "CLASSIFIED_HITS_TSV = RUN_DIR / \"classified_hits.tsv\"\n",
        "CLASSIFIED_CHAINS_TSV = RUN_DIR / \"classified_chains.tsv\"\n",
        "\n",
        "def rex_log(msg: str):\n",
        "    \"\"\"Lightweight logger for the RegExTractor module.\"\"\"\n",
        "    logger.info(f\"[RegExTractor] {msg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jljSIjjNjSZe"
      },
      "source": [
        "## Cell 71 â€“ Data Preparation for Classification\n",
        "\n",
        "This cell prepares the two key inputs for the classification engine:\n",
        "1.  **Training Data:** The final, high-quality, and complete exon data from the `wide_df` (output of Part 6). This data is now orthology-aware, including the `paralog_group` for each sequence.\n",
        "2.  **Target Pool:** The set of truly unclassified sequencesâ€”those from the original `full_df` that were rejected during the initial pre-processing in Part 2."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 71 =====\n",
        "# Input preparation for RegExTractor Classification\n",
        "\n",
        "# 1. Create the TRAINING set from the final, rescued wide_df (from Part 6)\n",
        "training_rows_rex = []\n",
        "\n",
        "if 'wide_df' in globals() and isinstance(wide_df, pd.DataFrame) and not wide_df.empty:\n",
        "    rex_log(f\"[71] Preparing training set from wide_df: rows={len(wide_df)}\")\n",
        "\n",
        "    # We need the paralog group and taxonomy for each entry; merge from working_df\n",
        "    training_info_cols = ['Entry', 'paralog_group', 'Genus', 'Family', 'Order']\n",
        "    if 'working_df' in globals() and isinstance(working_df, pd.DataFrame) and not working_df.empty:\n",
        "        have_cols = [c for c in training_info_cols if c in working_df.columns]\n",
        "        info_lut = working_df[have_cols].rename(columns={'Entry':'accession'})\n",
        "        rex_log(f\"[71] Info LUT columns: {have_cols}\")\n",
        "        wide_for_training = wide_df.merge(info_lut, on='accession', how='left')\n",
        "    else:\n",
        "        rex_log(\"[71] WARNING: working_df missing/empty; proceeding without taxonomy/paralog_group merge.\")\n",
        "        wide_for_training = wide_df.copy()\n",
        "\n",
        "    rex_log(f\"[71] Merged view: rows={len(wide_for_training)}; building long-form exon tableâ€¦\")\n",
        "\n",
        "    # Extract all exon peptides from the wide format into a long format\n",
        "    err, done = 0, 0\n",
        "    for i, (_, row) in enumerate(wide_for_training.iterrows(), 1):\n",
        "        # Find all peptide columns for this row\n",
        "        pep_cols = {int(c.split('_')[-1]): c for c in row.index if str(c).startswith('exon_peptide_')}\n",
        "        try:\n",
        "            for exon_num, pep_col in pep_cols.items():\n",
        "                peptide = row[pep_col]\n",
        "                # Train on all available exons, even those previously missing\n",
        "                if pd.notna(peptide) and peptide not in [\"MISSING_EXON\", \"NO_BAIT_DEFINED\"]:\n",
        "                    rec = {\n",
        "                        \"accession\": row.get(\"accession\"),\n",
        "                        \"gene_symbol\": row.get('gene_symbol'),\n",
        "                        \"paralog_group\": row.get('paralog_group', 'unknown_paralog'),\n",
        "                        \"exon_num_in_chain\": exon_num,\n",
        "                        \"exon_peptide\": peptide,\n",
        "                        \"genus\": row.get('Genus'),\n",
        "                        \"family\": row.get('Family'),\n",
        "                        \"order\": row.get('Order'),\n",
        "                    }\n",
        "                    training_rows_rex.append(rec)\n",
        "            done += 1\n",
        "        except Exception as e:\n",
        "            err += 1\n",
        "            if err <= 5:\n",
        "                rex_log(f\"[71] Row parse error example (#{i}): {e}\")\n",
        "        # Light progress ping every 2k rows\n",
        "        if i % 2000 == 0:\n",
        "            rex_log(f\"[71] Processed {i} recordsâ€¦\")\n",
        "\n",
        "    training_df_rex = pd.DataFrame(training_rows_rex)\n",
        "    rex_log(f\"[71] Created RegExTractor training set with {len(training_df_rex)} exon rows \"\n",
        "            f\"(source records ok={done}, errors={err}).\")\n",
        "\n",
        "else:\n",
        "    rex_log(\"[71] WARNING: wide_df missing/empty; training_df_rex will be empty.\")\n",
        "    training_df_rex = pd.DataFrame(columns=[\n",
        "        \"accession\",\"gene_symbol\",\"paralog_group\",\"exon_num_in_chain\",\"exon_peptide\",\n",
        "        \"genus\",\"family\",\"order\"\n",
        "    ])\n",
        "\n",
        "# 2. Create the TARGET pool of unclassified sequences to scan\n",
        "target_pool_rex = pd.DataFrame()\n",
        "if 'full_df' in globals() and 'working_df' in globals() \\\n",
        "   and isinstance(full_df, pd.DataFrame) and isinstance(working_df, pd.DataFrame) \\\n",
        "   and not full_df.empty and not working_df.empty:\n",
        "\n",
        "    classified_accessions = set(working_df['Entry'].dropna().astype(str).unique())\n",
        "    unclassified_df = full_df[~full_df['Entry'].astype(str).isin(classified_accessions)].copy()\n",
        "\n",
        "    # Minimum taxonomy for targets\n",
        "    unclassified_df['Genus'] = unclassified_df['Organism'].str.split().str[0] if 'Organism' in unclassified_df.columns else pd.NA\n",
        "\n",
        "    target_pool_rex = unclassified_df[['Entry', 'Sequence', 'Genus']].rename(\n",
        "        columns={'Entry':'accession', 'Sequence':'sequence', 'Genus': 'genus'}\n",
        "    )\n",
        "    rex_log(f\"[71] Created classification target pool with {len(target_pool_rex)} unclassified sequences.\")\n",
        "else:\n",
        "    rex_log(\"[71] WARNING: full_df/working_df missing/empty; target_pool_rex will be empty.\")\n"
      ],
      "metadata": {
        "id": "a6rEesbb88p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 72 â€“ Paralog Annotation & Validation for RegEx Training\n",
        "This single cell **replaces 72B/72C/72E/72F** by combining enrichment, inference, and\n",
        "conservative validation of *gene-level* paralogs for `training_df_rex`.\n",
        "\n",
        "**Inputs (assumed present):**\n",
        "- `training_df_rex` (required)\n",
        "- `df_high_quality` (optional; used to enrich missing text/meta via a pragmatic join)\n",
        "\n",
        "**What this cell does (non-destructive):**\n",
        "1) *(Optional)* **Enrich** `training_df_rex` with `gene_symbol_norm`, â€œGene Names (primary)â€,\n",
        "   â€œProtein namesâ€, `stitle`, `description`, `raw_header` from `df_high_quality`\n",
        "   using common ID keys (`seq_id`, `accession`, `uniprot`, etc.).\n",
        "2) **Infer** a *gene-level* paralog label (`COLxAy`) with priority:\n",
        "   `gene_symbol_norm` â†’ â€œGene Names (primary)â€ â†’ parsed **Protein names** / titles\n",
        "   (e.g., â€œCollagen alpha-1(I) chainâ€ â†’ `COL1A1`).\n",
        "   - Adds (but **does not overwrite** existing values):  \n",
        "     `paralog_group`, `paralog_source`, `paralog_confidence`.\n",
        "3) **Validate** with a conservative relabel:\n",
        "   - Adds: `paralog_group_validated`, `paralog_validated_source`,\n",
        "     `paralog_validated_confidence`.\n",
        "   - Conflict audit: `paralog_conflict_flag`, `paralog_conflict_reason`\n",
        "     (e.g., flags likely **XI â†” I** confusions).\n",
        "4) **Diagnostics**: compact counts + top groups.\n",
        "5) **Snapshot** (optional): writes `training_df_rex_paralog_annotated.tsv` to `RUN_DIR` (if set).\n",
        "\n",
        "**Notes**\n",
        "- No functions/variables elsewhere are renamed or removed.\n",
        "- Existing `paralog_group` is **only filled where missing** (never overwritten).\n",
        "- Use `paralog_group_validated` downstream if you want the conservative label.\n"
      ],
      "metadata": {
        "id": "AtmAogtjc7jV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 72 =====\n",
        "# Paralog Annotation & Validation (combined, non-destructive)\n",
        "\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import Optional, List, Tuple\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------- User-tunable controls (#@param) -----------------------\n",
        "ATTEMPT_JOIN_FROM_DF_HQ   = True   #@param {type:\"boolean\"}\n",
        "STRICT_GENE_PATTERN       = True   #@param {type:\"boolean\"}  # require COL\\d+A\\d+\n",
        "ENABLE_DIAGNOSTICS        = True   #@param {type:\"boolean\"}\n",
        "SAVE_SNAPSHOT             = True   #@param {type:\"boolean\"}\n",
        "\n",
        "# -------------------------- Helpers (local to cell) ----------------------------\n",
        "_ROMAN_TO_ARABIC = {\n",
        "    \"I\":1,\"II\":2,\"III\":3,\"IV\":4,\"V\":5,\"VI\":6,\"VII\":7,\"VIII\":8,\"IX\":9,\n",
        "    \"X\":10,\"XI\":11,\"XII\":12,\"XIII\":13,\"XIV\":14,\"XV\":15,\"XVI\":16,\"XVII\":17,\n",
        "    \"XVIII\":18,\"XIX\":19,\"XX\":20,\"XXI\":21,\"XXII\":22,\"XXIII\":23,\"XXIV\":24\n",
        "}\n",
        "\n",
        "_RX_COL_LITERAL      = re.compile(r'(COL\\d+A\\d+)', re.IGNORECASE)\n",
        "_RX_PROTEIN_COLLAGEN = re.compile(\n",
        "    r'\\b(?:pro-)?collagen\\s+alpha[-\\s]?(?P<chain>\\d)\\s*\\(\\s*(?P<roman>[IVXLC]+)\\s*\\)\\s*chain\\b',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "_RX_TYPE_FIRST = re.compile(\n",
        "    r'\\btype\\s+(?P<roman>[IVXLC]+)\\s+collagen\\b.*?\\balpha[-\\s]?(?P<chain>\\d)\\b',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "def _looks_like_col_gene(s: str) -> bool:\n",
        "    return bool(re.fullmatch(r'COL\\d+A\\d+', str(s).strip().upper()))\n",
        "\n",
        "def _coalesce(row: pd.Series, cols: List[str]) -> Optional[str]:\n",
        "    for c in cols:\n",
        "        v = row.get(c)\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            return v.strip()\n",
        "    return None\n",
        "\n",
        "def _parse_gene_from_text(txt: Optional[str]) -> Optional[str]:\n",
        "    \"\"\"Parse from literal COLxAy or 'Collagen alpha-<n>(<ROMAN>) chain' / 'type <ROMAN> collagen ... alpha-<n>'.\"\"\"\n",
        "    if not isinstance(txt, str) or not txt.strip():\n",
        "        return None\n",
        "    s = txt.strip()\n",
        "    m = _RX_COL_LITERAL.search(s)\n",
        "    if m:\n",
        "        return m.group(1).upper()\n",
        "    m = _RX_PROTEIN_COLLAGEN.search(s) or _RX_TYPE_FIRST.search(s)\n",
        "    if not m:\n",
        "        return None\n",
        "    try:\n",
        "        chain = int(m.group(\"chain\"))\n",
        "        roman = m.group(\"roman\").upper()\n",
        "        ctype = _ROMAN_TO_ARABIC.get(roman)\n",
        "        return f\"COL{ctype}A{chain}\" if ctype else None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _pick_join_key(left: List[str], right: List[str]) -> Optional[str]:\n",
        "    for k in [\"seq_id\",\"source_seq_id\",\"record_id\",\"qseqid\",\"sseqid\",\n",
        "              \"accession\",\"uniprot\",\"uniprot_id\",\"uniprot_accession\",\n",
        "              \"subject_id\",\"protein_id\",\"entry\",\"Entry\"]:\n",
        "        if k in left and k in right:\n",
        "            return k\n",
        "    return None\n",
        "\n",
        "def _enrich_from_df_hq(df_train: pd.DataFrame, df_hq: pd.DataFrame) -> pd.DataFrame:\n",
        "    key = _pick_join_key(list(df_train.columns), list(df_hq.columns))\n",
        "    if not key:\n",
        "        return df_train\n",
        "    cols_want = [c for c in [\n",
        "        key, \"gene_symbol_norm\", \"gene_symbol\",\n",
        "        \"Gene Names (primary)\", \"Protein names\",\n",
        "        \"stitle\",\"description\",\"raw_header\",\n",
        "        \"species\",\"species_name\",\n",
        "        \"accession\",\"uniprot\",\"uniprot_id\",\"uniprot_accession\"\n",
        "    ] if c in df_hq.columns]\n",
        "    return df_train.merge(df_hq[cols_want].drop_duplicates(), on=key, how=\"left\")\n",
        "\n",
        "def _infer_gene_paralog(row: pd.Series) -> Tuple[Optional[str], str, float]:\n",
        "    # 1) gene_symbol_norm / gene_symbol\n",
        "    gsn = _coalesce(row, [\"gene_symbol_norm\",\"gene_symbol\"])\n",
        "    if gsn and (not STRICT_GENE_PATTERN or _looks_like_col_gene(gsn)):\n",
        "        return gsn.upper(), \"gene_symbol_norm\", 1.0\n",
        "    # 2) Gene Names (primary)\n",
        "    gnp = _coalesce(row, [\"Gene Names (primary)\",\"Gene names (primary)\",\"gene_names_primary\",\"Gene Names\"])\n",
        "    if gnp:\n",
        "        m = _RX_COL_LITERAL.search(gnp)\n",
        "        if m:\n",
        "            return m.group(1).upper(), \"gene_names_primary\", 0.9\n",
        "    # 3) Protein names / titles\n",
        "    for col in [\"Protein names\",\"Protein name\",\"Protein names (recommended)\",\n",
        "                \"protein_name\",\"protein_names\",\"stitle\",\"description\",\"raw_header\"]:\n",
        "        lab = _parse_gene_from_text(row.get(col))\n",
        "        if lab and (not STRICT_GENE_PATTERN or _looks_like_col_gene(lab)):\n",
        "            return lab, \"protein_or_title.parsed\", 0.75\n",
        "    # 4) fallback to existing (may be empty)\n",
        "    pg = row.get(\"paralog_group\")\n",
        "    if isinstance(pg, str) and pg.strip():\n",
        "        return pg.strip().upper(), \"existing\", 0.5\n",
        "    return None, \"NA\", 0.0\n",
        "\n",
        "def _validated_label(row: pd.Series) -> Tuple[Optional[str], str, float, bool, str]:\n",
        "    \"\"\"\n",
        "    Conservative validated label & conflict audit.\n",
        "    Returns: (label, source, conf, conflict_flag, reason)\n",
        "    \"\"\"\n",
        "    original = row.get(\"paralog_group\")\n",
        "    gsn = _coalesce(row, [\"gene_symbol_norm\",\"gene_symbol\"])\n",
        "\n",
        "    # Prefer GSN if it looks like a collagen gene\n",
        "    if gsn and (not STRICT_GENE_PATTERN or _looks_like_col_gene(gsn)):\n",
        "        label = gsn.upper()\n",
        "        conflict = isinstance(original, str) and original.upper() != label\n",
        "        return label, \"gene_symbol_norm\", 1.0, conflict, (\"GSN!=original\" if conflict else \"\")\n",
        "\n",
        "    # Gene Names (primary)\n",
        "    gnp = _coalesce(row, [\"Gene Names (primary)\",\"Gene names (primary)\",\"gene_names_primary\",\"Gene Names\"])\n",
        "    if gnp:\n",
        "        m = _RX_COL_LITERAL.search(gnp)\n",
        "        if m:\n",
        "            label = m.group(1).upper()\n",
        "            conflict = isinstance(original, str) and original.upper() != label\n",
        "            return label, \"gene_names_primary\", 0.9, conflict, (\"GNP!=original\" if conflict else \"\")\n",
        "\n",
        "    # Parse text\n",
        "    for col in [\"Protein names\",\"Protein name\",\"Protein names (recommended)\",\n",
        "                \"protein_name\",\"protein_names\",\"stitle\",\"description\",\"raw_header\"]:\n",
        "        cand = row.get(col)\n",
        "        lab = _parse_gene_from_text(cand)\n",
        "        if lab and (not STRICT_GENE_PATTERN or _looks_like_col_gene(lab)):\n",
        "            conflict = isinstance(original, str) and original.upper() != lab\n",
        "            reason = \"\"\n",
        "            if conflict and isinstance(cand, str):\n",
        "                # flag likely XI â†” I confusion\n",
        "                if re.search(r'\\btype\\s+I\\b', cand, re.IGNORECASE) and not re.search(r'\\btype\\s+XI\\b', cand, re.IGNORECASE):\n",
        "                    if isinstance(original, str) and original.upper().startswith(\"COL11A\"):\n",
        "                        reason = \"possible XIâ†”I confusion\"\n",
        "            return lab, \"protein_or_title.parsed\", 0.75, conflict, reason\n",
        "\n",
        "    # Fallback to original\n",
        "    lab = original.upper() if isinstance(original, str) else None\n",
        "    return lab, (\"existing\" if lab else \"NA\"), (0.5 if lab else 0.0), False, (\"fallback_to_existing\" if lab else \"unlabeled\")\n",
        "\n",
        "# --------------------------------- Main logic ---------------------------------\n",
        "if \"training_df_rex\" in globals() and isinstance(training_df_rex, pd.DataFrame) and not training_df_rex.empty:\n",
        "    df = training_df_rex.copy()\n",
        "\n",
        "    # (1) Optional enrichment from df_high_quality\n",
        "    if ATTEMPT_JOIN_FROM_DF_HQ and \"df_high_quality\" in globals() \\\n",
        "       and isinstance(df_high_quality, pd.DataFrame) and not df_high_quality.empty:\n",
        "        before_cols = set(df.columns)\n",
        "        df = _enrich_from_df_hq(df, df_high_quality)\n",
        "        if ENABLE_DIAGNOSTICS:\n",
        "            gained = sorted(list(set(df.columns) - before_cols))\n",
        "            print(f\"[Paralog-Enrich] Columns gained from df_high_quality: {gained}\")\n",
        "\n",
        "    # (2) Primary inference â†’ only FILL missing 'paralog_group'\n",
        "    inferred = df.apply(_infer_gene_paralog, axis=1)\n",
        "    df[\"_pg_inferred\"]  = inferred.apply(lambda t: t[0])\n",
        "    df[\"_pg_src\"]       = inferred.apply(lambda t: t[1])\n",
        "    df[\"_pg_conf\"]      = inferred.apply(lambda t: t[2])\n",
        "\n",
        "    if \"paralog_group\" not in df.columns:\n",
        "        df[\"paralog_group\"] = pd.NA\n",
        "    if \"paralog_source\" not in df.columns:\n",
        "        df[\"paralog_source\"] = pd.NA\n",
        "    if \"paralog_confidence\" not in df.columns:\n",
        "        df[\"paralog_confidence\"] = pd.NA\n",
        "\n",
        "    mask_missing_pg = ~df[\"paralog_group\"].astype(str).str.strip().astype(bool)\n",
        "    df.loc[mask_missing_pg, \"paralog_group\"]      = df.loc[mask_missing_pg, \"_pg_inferred\"]\n",
        "    df.loc[mask_missing_pg, \"paralog_source\"]     = df.loc[mask_missing_pg, \"_pg_src\"]\n",
        "    df.loc[mask_missing_pg, \"paralog_confidence\"] = df.loc[mask_missing_pg, \"_pg_conf\"]\n",
        "\n",
        "    # (3) Validated label + conflicts (always computed; non-destructive)\n",
        "    validated = df.apply(_validated_label, axis=1)\n",
        "    df[\"paralog_group_validated\"]       = validated.apply(lambda t: t[0])\n",
        "    df[\"paralog_validated_source\"]      = validated.apply(lambda t: t[1])\n",
        "    df[\"paralog_validated_confidence\"]  = validated.apply(lambda t: t[2])\n",
        "    df[\"paralog_conflict_flag\"]         = validated.apply(lambda t: t[3])\n",
        "    df[\"paralog_conflict_reason\"]       = validated.apply(lambda t: t[4])\n",
        "\n",
        "    # tidy temp columns\n",
        "    df.drop(columns=[\"_pg_inferred\",\"_pg_src\",\"_pg_conf\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    # (4) Diagnostics\n",
        "    if ENABLE_DIAGNOSTICS:\n",
        "        total = len(df)\n",
        "        n_pg   = int(df[\"paralog_group\"].notna().sum())\n",
        "        n_pgv  = int(df[\"paralog_group_validated\"].notna().sum())\n",
        "        n_conf = int(df[\"paralog_conflict_flag\"].sum())\n",
        "        print(f\"[Paralog] rows={total}; filled(paralog_group)={n_pg}; validated={n_pgv}; conflicts={n_conf}\")\n",
        "        with pd.option_context(\"display.max_rows\", 10):\n",
        "            print(\"[Paralog] Top groups:\")\n",
        "            display(df[\"paralog_group\"].value_counts(dropna=True).head(20))\n",
        "            print(\"[Paralog-Validated] Top groups:\")\n",
        "            display(df[\"paralog_group_validated\"].value_counts(dropna=True).head(20))\n",
        "            if n_conf:\n",
        "                print(\"[Paralog] Conflict reasons:\")\n",
        "                display(df[\"paralog_conflict_reason\"].value_counts(dropna=True).head(10))\n",
        "\n",
        "    # (5) Save snapshot\n",
        "    if SAVE_SNAPSHOT:\n",
        "        try:\n",
        "            run_dir = RUN_DIR if \"RUN_DIR\" in globals() else Path(\".\")\n",
        "            outp = Path(run_dir) / \"training_df_rex_paralog_annotated.tsv\"\n",
        "            df.to_csv(outp, sep=\"\\t\", index=False)\n",
        "            if \"logger\" in globals() and logger:\n",
        "                logger.info(f\"Saved paralog-annotated training set â†’ {outp}\")\n",
        "        except Exception as e:\n",
        "            if \"logger\" in globals() and logger:\n",
        "                logger.warning(f\"Could not save paralog snapshot: {e}\")\n",
        "\n",
        "    # Commit back to canonical variable (non-destructive to others)\n",
        "    training_df_rex = df\n",
        "else:\n",
        "    print(\"training_df_rex not found or empty; skipping Cell 71.\")\n"
      ],
      "metadata": {
        "id": "DVb2QBP1c6Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 73 â€“ Orthology-Aware Regex Library Builder (label-agnostic)\n",
        "\n",
        "### Purpose\n",
        "Build an **orthology-aware regex library** of exon motifs per paralog and exon index,\n",
        "without hard-coding a particular label column. This cell consumes the training set\n",
        "from Cell 71 and the validated paralog labels from Cell 72, then emits a library\n",
        "used by the matcher and classifier in Cells 75â€“76.\n",
        "\n",
        "### Inputs & Dependencies\n",
        "- **DataFrames**\n",
        "  - `training_df_rex` (from Cell 71; required).\n",
        "- **Labels**\n",
        "  - Chooses label column via `rex_label_column` (`\"auto\"`, `\"paralog_group_validated\"`,\n",
        "    `\"paralog_group\"`, or `\"custom\"`). Creates a safe view `SOURCE_DF` where the chosen\n",
        "    label is exposed as `paralog_group`.\n",
        "- **Config (from Cell 70)**\n",
        "  - `rex_min_clade_samples`, `rex_freq_cutoff_consensus`, `rex_freq_cutoff_stringent`,\n",
        "    `rex_ghead_min_consensus`, `rex_ghead_min_stringent`, and optional `REX_TAXON_LEVELS`.\n",
        "- **Expected columns in the effective training view (`SOURCE_DF`)**\n",
        "  - Required: `paralog_group`, `exon_num_in_chain`, `exon_peptide`.\n",
        "  - Optional: `gene_symbol` (metadata), clade columns named in `REX_TAXON_LEVELS`\n",
        "    (e.g., `order`, `family`, `genus`).\n",
        "\n",
        "### What it creates\n",
        "- **`SOURCE_DF`** â€” a copy of `training_df_rex` with the chosen label normalized to\n",
        "  the column name `paralog_group` for legacy compatibility.\n",
        "- **`orthology_aware_library`**\n",
        "  - Internal container (`OrthologyAwareLibrary`) with `entries` keyed by\n",
        "    `(paralog_group, exon_num_in_chain, clade_key)`.\n",
        "  - Each value is an `ExonRegexLibraryEntry` holding one or more `RexTier` objects:\n",
        "    - *consensus* tier (uses `rex_freq_cutoff_consensus`, `rex_ghead_min_consensus`)\n",
        "    - *stringent* tier (uses `rex_freq_cutoff_stringent`, `rex_ghead_min_stringent`)\n",
        "\n",
        "### High-level flow\n",
        "1. **Label selection**  \n",
        "   Resolve `LABEL_COL` from user setting â†’ prefer `paralog_group_validated` â†’ fallback.\n",
        "   Build `SOURCE_DF` and expose the chosen label as `paralog_group`.\n",
        "2. **Sanity checks**  \n",
        "   Ensure `SOURCE_DF` has `paralog_group`, `exon_num_in_chain`, `exon_peptide`.\n",
        "   Warn if `unknown_paralog` is present.\n",
        "3. **Tier construction**  \n",
        "   For each `(paralog_group, exon_num_in_chain[, clade])` with at least\n",
        "   `rex_min_clade_samples` peptides:\n",
        "   - Compute simple position-wise frequencies and median length.\n",
        "   - Derive per-position regex tokens by thresholding residue frequencies.\n",
        "   - Build two tiers (*consensus*, *stringent*), compile regex.\n",
        "4. **Library assembly**  \n",
        "   Store tiers in `orthology_aware_library.entries[(pg, exon, clade)]`.\n",
        "   Always attempt a **pan** model; optionally add clade-specific models\n",
        "   if `REX_TAXON_LEVELS` contains columns present in `SOURCE_DF`.\n",
        "\n",
        "### Diagnostics (printed)\n",
        "- Selected label column name, total rows, labeled vs unlabeled counts.\n",
        "- Final count of library entries and number of distinct paralog groups.\n",
        "\n",
        "### Notes & Guarantees\n",
        "- **Non-destructive**: does not rename or drop user data; only creates `SOURCE_DF`\n",
        "  and `orthology_aware_library`.\n",
        "- **Label-agnostic**: switch labels by changing `rex_label_column` or specifying a\n",
        "  `custom_label_name` that exists in `training_df_rex`.\n",
        "- **Downstream**: Cells **75â€“76** (matcher & classification) read from\n",
        "  `orthology_aware_library` and/or `SOURCE_DF`.\n"
      ],
      "metadata": {
        "id": "9PCrUO8RtAVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 73 =====\n",
        "# Orthology-Aware Regex Library Builder (label-agnostic)\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Label selection (no hard-coding)\n",
        "# ------------------------------------------------------------------------------\n",
        "rex_label_column = \"auto\"   #@param [\"auto\", \"paralog_group_validated\", \"paralog_group\", \"custom\"]\n",
        "custom_label_name = \"\"      #@param {type:\"string\"}\n",
        "\n",
        "if \"training_df_rex\" not in globals() or not isinstance(training_df_rex, pd.DataFrame) or training_df_rex.empty:\n",
        "    raise RuntimeError(\"Cell 72: training_df_rex is missing or empty. Build it in Cell 70 and label it in Cell 71.\")\n",
        "\n",
        "LABEL_CANDIDATES = [\n",
        "    \"paralog_group_validated\",\n",
        "    \"paralog_group\",\n",
        "    \"paralog_group_effective\",  # optional alias if you already created it earlier\n",
        "]\n",
        "\n",
        "def _select_label_col(df: pd.DataFrame) -> str:\n",
        "    # explicit choices first\n",
        "    if rex_label_column == \"paralog_group_validated\" and \"paralog_group_validated\" in df.columns:\n",
        "        return \"paralog_group_validated\"\n",
        "    if rex_label_column == \"paralog_group\" and \"paralog_group\" in df.columns:\n",
        "        return \"paralog_group\"\n",
        "    if rex_label_column == \"custom\":\n",
        "        if custom_label_name and custom_label_name in df.columns:\n",
        "            return custom_label_name\n",
        "        raise KeyError(f\"Requested custom label column '{custom_label_name}' not found in training_df_rex.\")\n",
        "\n",
        "    # 'auto' fallback: prefer validated\n",
        "    for c in LABEL_CANDIDATES:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    # last resort: try to guess any string column containing 'paralog' in its name\n",
        "    for c in df.columns:\n",
        "        if isinstance(c, str) and \"paralog\" in c.lower():\n",
        "            return c\n",
        "    raise KeyError(\"No suitable label column found. Expected one of: \"\n",
        "                   \"paralog_group_validated / paralog_group / custom.\")\n",
        "\n",
        "LABEL_COL = _select_label_col(training_df_rex)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Create a safe view for legacy code: always expose chosen label as 'paralog_group'\n",
        "# ------------------------------------------------------------------------------\n",
        "SOURCE_DF = training_df_rex.copy()\n",
        "if LABEL_COL != \"paralog_group\":\n",
        "    SOURCE_DF[\"paralog_group\"] = SOURCE_DF[LABEL_COL]\n",
        "\n",
        "# Light sanity checks\n",
        "_n = len(SOURCE_DF)\n",
        "_n_labeled = SOURCE_DF[\"paralog_group\"].notna().sum()\n",
        "print(f\"[Cell 72] Using label column: {LABEL_COL}\")\n",
        "print(f\"[Cell 72] Rows: {_n}; labeled: {_n_labeled}; unlabeled: {_n - _n_labeled}\")\n",
        "\n",
        "# Optional: avoid training on unknown labels\n",
        "if \"unknown_paralog\" in SOURCE_DF[\"paralog_group\"].astype(str).unique():\n",
        "    unk = int((SOURCE_DF[\"paralog_group\"] == \"unknown_paralog\").sum())\n",
        "    print(f\"[Cell 72] Warning: {unk} rows have 'unknown_paralog'. They may be excluded downstream.\")\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Configuration knobs (safe defaults; override earlier in the notebook if desired)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "# Which taxonomic levels to attempt beyond 'pan' (must be column names in training_df)\n",
        "try:\n",
        "    REX_TAXON_LEVELS  # noqa: F823\n",
        "except NameError:\n",
        "    REX_TAXON_LEVELS: List[str] = [\"pan\"]  # extend e.g. [\"pan\", \"order\", \"family\"]\n",
        "\n",
        "# Minimum number of peptide examples per (paralog, exon, clade) to build a model\n",
        "try:\n",
        "    rex_min_clade_samples  # noqa: F823\n",
        "except NameError:\n",
        "    rex_min_clade_samples: int = 5\n",
        "\n",
        "# Position-wise residue frequency cut-offs for tiers\n",
        "try:\n",
        "    rex_freq_cutoff_consensus  # noqa: F823\n",
        "except NameError:\n",
        "    rex_freq_cutoff_consensus: float = 0.05\n",
        "\n",
        "try:\n",
        "    rex_freq_cutoff_stringent  # noqa: F823\n",
        "except NameError:\n",
        "    rex_freq_cutoff_stringent: float = 0.15\n",
        "\n",
        "# Tier acceptance thresholds (example semantics: minimal â€œg-head densityâ€ or similar)\n",
        "try:\n",
        "    rex_ghead_min_consensus  # noqa: F823\n",
        "except NameError:\n",
        "    rex_ghead_min_consensus: float = 0.85\n",
        "\n",
        "try:\n",
        "    rex_ghead_min_stringent  # noqa: F823\n",
        "except NameError:\n",
        "    rex_ghead_min_stringent: float = 0.92\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Utility functions for robust column handling and simple stats\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "def _has_cols(df: pd.DataFrame, cols: Iterable[str]) -> bool:\n",
        "    \"\"\"Return True if all columns exist in the DataFrame.\"\"\"\n",
        "    cols = list(cols)\n",
        "    return all(c in df.columns for c in cols)\n",
        "\n",
        "def _first_existing_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
        "    \"\"\"Return the first candidate column name that exists in df, else None.\"\"\"\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _coalesce_training_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalise expected column names in training_df:\n",
        "      - peptides in 'exon_peptide' (fall back to 'peptide')\n",
        "      - ensure 'paralog_group' and 'exon_num_in_chain' exist\n",
        "      - keep 'gene_symbol' if available (used for metadata)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Peptide column\n",
        "    if \"exon_peptide\" not in df.columns:\n",
        "        fallback = _first_existing_col(df, [\"peptide\", \"Peptide\", \"peptides\"])\n",
        "        if fallback:\n",
        "            df[\"exon_peptide\"] = df[fallback]\n",
        "\n",
        "    # Sanity checks\n",
        "    required = [\"paralog_group\", \"exon_num_in_chain\", \"exon_peptide\"]\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"training_df is missing required columns: {missing}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Dataclasses expected by downstream matcher/orchestrator\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "@dataclass\n",
        "class RexTier:\n",
        "    \"\"\"\n",
        "    One regex tier describing an exon motif at a given stringency.\n",
        "    Attributes are aligned with the expectations of RegExTractorMatcher/_score_hit.\n",
        "    \"\"\"\n",
        "    tier: str               # e.g., \"consensus\", \"stringent\"\n",
        "    regex: re.Pattern       # compiled Python regex\n",
        "    ghead_min: float        # acceptance threshold used by the matcher\n",
        "\n",
        "@dataclass\n",
        "class ExonRegexLibraryEntry:\n",
        "    \"\"\"Container for all tiers available for one (paralog_group, exon, clade).\"\"\"\n",
        "    gene_symbol: str\n",
        "    exon_num_in_chain: int\n",
        "    clade: str              # e.g., \"pan\", \"Cetartiodactyla\"\n",
        "    tiers: List[RexTier]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Core builder: statistics + tier construction\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class RegExTractorBuilder:\n",
        "    \"\"\"\n",
        "    Builds per-exon regex tiers from peptide examples.\n",
        "\n",
        "    The default implementation provides two tiers:\n",
        "      - 'consensus'  : frequency cut-off = rex_freq_cutoff_consensus; ghead_min = rex_ghead_min_consensus\n",
        "      - 'stringent'  : frequency cut-off = rex_freq_cutoff_stringent; ghead_min = rex_ghead_min_stringent\n",
        "\n",
        "    If you have advanced utilities (Shannon entropy, anchor windows, etc.),\n",
        "    you can refine `build_stats` and/or `build_tiers` here without touching the orchestrator.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def build_stats(peptides: List[str]) -> Dict[str, object]:\n",
        "        \"\"\"\n",
        "        Compute simple per-position character frequencies and lengths.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Dict[str, object]\n",
        "            keys: 'median_len', 'pos_counts' (list[dict[char->count]]), 'pos_totals' (list[int])\n",
        "        \"\"\"\n",
        "        peps = [p for p in peptides if isinstance(p, str) and p]\n",
        "        if not peps:\n",
        "            return {\"median_len\": 0, \"pos_counts\": [], \"pos_totals\": []}\n",
        "\n",
        "        median_len = int(np.median([len(p) for p in peps]))\n",
        "        pos_counts: List[Dict[str, int]] = []\n",
        "        pos_totals: List[int] = []\n",
        "\n",
        "        for i in range(median_len):\n",
        "            counts = defaultdict(int)\n",
        "            total = 0\n",
        "            for p in peps:\n",
        "                if len(p) > i:\n",
        "                    counts[p[i]] += 1\n",
        "                    total += 1\n",
        "            pos_counts.append(dict(counts))\n",
        "            pos_totals.append(total)\n",
        "\n",
        "        return {\"median_len\": median_len, \"pos_counts\": pos_counts, \"pos_totals\": pos_totals}\n",
        "\n",
        "    @staticmethod\n",
        "    def _pattern_from_stats(stats: Dict[str, object], freq_cutoff: float) -> Tuple[str, int]:\n",
        "        \"\"\"\n",
        "        Convert per-position counts to a regex pattern using a frequency threshold.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        stats : dict produced by build_stats\n",
        "        freq_cutoff : float in [0,1], minimal residue frequency to include at a position\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        pattern : str\n",
        "        median_len : int\n",
        "        \"\"\"\n",
        "        median_len: int = int(stats.get(\"median_len\", 0))\n",
        "        pos_counts: List[Dict[str, int]] = stats.get(\"pos_counts\", [])  # type: ignore\n",
        "        pos_totals: List[int] = stats.get(\"pos_totals\", [])              # type: ignore\n",
        "\n",
        "        parts: List[str] = []\n",
        "        for i in range(median_len):\n",
        "            counts = pos_counts[i] if i < len(pos_counts) else {}\n",
        "            total = pos_totals[i] if i < len(pos_totals) else 0\n",
        "            if not counts or total <= 0:\n",
        "                parts.append(\".\")\n",
        "                continue\n",
        "\n",
        "            keep = [aa for aa, n in counts.items() if (n / total) >= freq_cutoff]\n",
        "            if len(keep) == 1:\n",
        "                parts.append(re.escape(keep[0]))\n",
        "            elif len(keep) > 1:\n",
        "                parts.append(\"[\" + \"\".join(sorted(keep)) + \"]\")\n",
        "            else:\n",
        "                parts.append(\".\")\n",
        "        return \"\".join(parts), median_len\n",
        "\n",
        "    def build_tiers(self, peptides: List[str], stats: Dict[str, object]) -> List[RexTier]:\n",
        "        \"\"\"\n",
        "        Construct a set of tiers from peptides and stats. Returns an empty list if unusable.\n",
        "        \"\"\"\n",
        "        if not peptides or stats.get(\"median_len\", 0) <= 0:\n",
        "            return []\n",
        "\n",
        "        tiers: List[RexTier] = []\n",
        "\n",
        "        # Consensus tier\n",
        "        patt_cons, _ = self._pattern_from_stats(stats, freq_cutoff=rex_freq_cutoff_consensus)\n",
        "        try:\n",
        "            tiers.append(RexTier(tier=\"consensus\", regex=re.compile(patt_cons), ghead_min=rex_ghead_min_consensus))\n",
        "        except re.error:\n",
        "            pass  # ignore un-compilable patterns\n",
        "\n",
        "        # Stringent tier\n",
        "        patt_str, _ = self._pattern_from_stats(stats, freq_cutoff=rex_freq_cutoff_stringent)\n",
        "        try:\n",
        "            tiers.append(RexTier(tier=\"stringent\", regex=re.compile(patt_str), ghead_min=rex_ghead_min_stringent))\n",
        "        except re.error:\n",
        "            pass\n",
        "\n",
        "        return [t for t in tiers if isinstance(t.regex, re.Pattern)]\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# Library container keyed by (paralog_group, exon_num_in_chain, clade_key)\n",
        "# ---------------------------------------------------------------------------\n",
        "\n",
        "class OrthologyAwareLibrary:\n",
        "    \"\"\"\n",
        "    Builds and stores an orthology-aware regex library for exon detection.\n",
        "\n",
        "    Key space: (paralog_group, exon_num_in_chain, clade_key)\n",
        "    Each value: ExonRegexLibraryEntry with one or more RexTier objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self) -> None:\n",
        "        self.entries: Dict[Tuple[str, int, str], ExonRegexLibraryEntry] = {}\n",
        "\n",
        "    def build(self, training_df: pd.DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Construct the library from training data.\n",
        "\n",
        "        Expected columns in training_df:\n",
        "          - 'paralog_group'        : str (not 'unknown_paralog')\n",
        "          - 'exon_num_in_chain'    : int\n",
        "          - 'exon_peptide'         : str (sequence)\n",
        "        Optional (but recommended):\n",
        "          - 'gene_symbol'          : str (metadata only)\n",
        "          - additional clade columns named in REX_TAXON_LEVELS (e.g., 'order', 'family')\n",
        "        \"\"\"\n",
        "        builder = RegExTractorBuilder()\n",
        "        df = _coalesce_training_columns(training_df)\n",
        "\n",
        "        built = 0\n",
        "        for paralog_group, pg_df in df.groupby(\"paralog_group\"):\n",
        "            if str(paralog_group) == \"unknown_paralog\":\n",
        "                continue\n",
        "\n",
        "            for exon_num in sorted(pg_df[\"exon_num_in_chain\"].unique()):\n",
        "                edf = pg_df[pg_df[\"exon_num_in_chain\"] == exon_num]\n",
        "\n",
        "                # Always attempt a 'pan' model if enough samples exist\n",
        "                if len(edf) >= rex_min_clade_samples:\n",
        "                    peps = edf[\"exon_peptide\"].dropna().astype(str).tolist()\n",
        "                    stats = builder.build_stats(peps)\n",
        "                    tiers = builder.build_tiers(peps, stats)\n",
        "                    if tiers:\n",
        "                        # Robust gene_symbol fallback: gene_symbol â†’ gene_symbol_norm â†’ \"unknown\"\n",
        "                        gene_symbol = (\n",
        "                            edf[\"gene_symbol\"].dropna().astype(str).iloc[0]\n",
        "                            if \"gene_symbol\" in edf.columns and edf[\"gene_symbol\"].notna().any()\n",
        "                            else (\n",
        "                                edf[\"gene_symbol_norm\"].dropna().astype(str).iloc[0]\n",
        "                                if \"gene_symbol_norm\" in edf.columns and edf[\"gene_symbol_norm\"].notna().any()\n",
        "                                else \"unknown\"\n",
        "                            )\n",
        "                        )\n",
        "                        self.entries[(paralog_group, int(exon_num), \"pan\")] = ExonRegexLibraryEntry(\n",
        "                            gene_symbol=gene_symbol,\n",
        "                            exon_num_in_chain=int(exon_num),\n",
        "                            clade=\"pan\",\n",
        "                            tiers=tiers,\n",
        "                        )\n",
        "                        built += 1\n",
        "\n",
        "                # Then attempt additional clade-specific models if configured and present\n",
        "                for clade_level in REX_TAXON_LEVELS:\n",
        "                    if clade_level in (None, \"pan\"):\n",
        "                        continue\n",
        "                    if clade_level not in edf.columns:\n",
        "                        continue\n",
        "\n",
        "                    for clade_key, sub_df in edf.groupby(clade_level):\n",
        "                        if len(sub_df) < rex_min_clade_samples:\n",
        "                            continue\n",
        "                        peps = sub_df[\"exon_peptide\"].dropna().astype(str).tolist()\n",
        "                        stats = builder.build_stats(peps)\n",
        "                        tiers = builder.build_tiers(peps, stats)\n",
        "                        if not tiers:\n",
        "                            continue\n",
        "                        # Robust gene_symbol fallback for clade entries\n",
        "                        gene_symbol = (\n",
        "                            sub_df[\"gene_symbol\"].dropna().astype(str).iloc[0]\n",
        "                            if \"gene_symbol\" in sub_df.columns and sub_df[\"gene_symbol\"].notna().any()\n",
        "                            else (\n",
        "                                sub_df[\"gene_symbol_norm\"].dropna().astype(str).iloc[0]\n",
        "                                if \"gene_symbol_norm\" in sub_df.columns and sub_df[\"gene_symbol_norm\"].notna().any()\n",
        "                                else \"unknown\"\n",
        "                            )\n",
        "                        )\n",
        "                        self.entries[(paralog_group, int(exon_num), str(clade_key))] = ExonRegexLibraryEntry(\n",
        "                            gene_symbol=gene_symbol,\n",
        "                            exon_num_in_chain=int(exon_num),\n",
        "                            clade=str(clade_key),\n",
        "                            tiers=tiers,\n",
        "                        )\n",
        "                        built += 1\n",
        "\n",
        "        rex_log(f\"Built {built} orthology-aware exon/clade entries \"\n",
        "                f\"across {len(set(k[0] for k in self.entries))} paralog groups.\")\n",
        "\n",
        "# ---- Runner: build the library from SOURCE_DF ----\n",
        "orthology_aware_library = OrthologyAwareLibrary()\n",
        "orthology_aware_library.build(SOURCE_DF)\n",
        "\n",
        "# quick diag\n",
        "n_entries = len(orthology_aware_library.entries)\n",
        "n_groups  = len({k[0] for k in orthology_aware_library.entries})\n",
        "rex_log(f\"[Cell 72] Library entries: {n_entries} across {n_groups} paralog groups.\")\n"
      ],
      "metadata": {
        "id": "iN3E67CcmYIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 74 â€“ Diagnostics: training coverage (â€œwhy 0 entries?â€)\n",
        "\n",
        "### Purpose\n",
        "Explain **why the library in Cell 73** produced few/zero entries by summarizing coverage\n",
        "per `(paralog_group, exon_num_in_chain)` on the **effective label view**.  \n",
        "This cell reads the same view used by the builder (`SOURCE_DF`), so counts reflect your\n",
        "chosen label (e.g., `paralog_group_validated`).\n",
        "\n",
        "### Inputs\n",
        "- `SOURCE_DF` (from Cell 73). If absent, falls back to `training_df_rex`.\n",
        "- `rex_min_clade_samples` (from Cell 70).\n",
        "\n",
        "### What it prints\n",
        "1) Selected label column (from Cell 73) and the top labels.  \n",
        "2) Non-empty peptide count; rows with `unknown_paralog`.  \n",
        "3) Group coverage per `(paralog_group, exon_num_in_chain)` and how many meet\n",
        "   `rex_min_clade_samples`.  \n",
        "4) Optional clade-column coverage (`order`, `family`, `genus`) if present.\n",
        "\n",
        "### Notes\n",
        "- **Read-only**: does not modify any DataFrame.  \n",
        "- Set `DIAG_COMPARE_RAW_VIEW=True` to also show diagnostic stats on the raw\n",
        "  `training_df_rex` for side-by-side comparison.\n",
        "\n"
      ],
      "metadata": {
        "id": "Vv8h1CRir1vD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 74 =====\n",
        "# Diagnostics: training coverage (â€œwhy 0 entries?â€)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#@markdown ### Controls\n",
        "DIAG_COMPARE_RAW_VIEW = False  #@param {type:\"boolean\"}\n",
        "DIAG_SHOW_TOP = 10             #@param {type:\"integer\"}\n",
        "\n",
        "def _summary_training(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Summarise coverage needed for regex building.\n",
        "\n",
        "    Returns a DataFrame with counts per (paralog_group, exon_num_in_chain).\n",
        "    \"\"\"\n",
        "    req = {\"paralog_group\", \"exon_num_in_chain\", \"exon_peptide\"}\n",
        "    cols_ok = req.issubset(df.columns)\n",
        "    print(f\"[Diag] training_df shape: {df.shape}; required columns present: {cols_ok}\")\n",
        "    if not cols_ok:\n",
        "        missing = sorted(req - set(df.columns))\n",
        "        print(f\"[Diag] Missing columns: {missing}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    tmp = df.copy()\n",
        "    tmp[\"exon_peptide\"] = tmp[\"exon_peptide\"].astype(str)\n",
        "    tmp[\"is_empty\"] = tmp[\"exon_peptide\"].eq(\"\") | \\\n",
        "                      tmp[\"exon_peptide\"].isin([\"MISSING_EXON\", \"NO_BAIT_DEFINED\"])\n",
        "\n",
        "    # Top-level counts\n",
        "    print(f\"[Diag] Non-empty peptides: {(~tmp['is_empty']).sum()} / {len(tmp)}\")\n",
        "    unk = (tmp[\"paralog_group\"] == \"unknown_paralog\").sum()\n",
        "    print(f\"[Diag] Rows with 'unknown_paralog': {unk}\")\n",
        "\n",
        "    # Coverage by (paralog, exon)\n",
        "    grp = (\n",
        "        tmp.loc[~tmp[\"is_empty\"] & (tmp[\"paralog_group\"] != \"unknown_paralog\")]\n",
        "          .groupby([\"paralog_group\", \"exon_num_in_chain\"])\n",
        "          .size()\n",
        "          .rename(\"n\")\n",
        "          .reset_index()\n",
        "          .sort_values([\"paralog_group\", \"exon_num_in_chain\"], kind=\"stable\")\n",
        "    )\n",
        "    print(f\"[Diag] Groups with at least 1 sample: {len(grp)}\")\n",
        "    with pd.option_context(\"display.max_rows\", 20):\n",
        "        display(grp.head(20))\n",
        "\n",
        "    # Threshold gate\n",
        "    thr = int(globals().get(\"rex_min_clade_samples\", 5))\n",
        "    passing = grp[grp[\"n\"] >= thr]\n",
        "    print(f\"[Diag] Groups meeting rex_min_clade_samples={thr}: {len(passing)}\")\n",
        "    if len(passing) < 10:\n",
        "        with pd.option_context(\"display.max_rows\", 20):\n",
        "            display(passing)\n",
        "\n",
        "    # Optional clade diagnostics\n",
        "    clade_cols = [c for c in [\"order\", \"family\", \"genus\"] if c in tmp.columns]\n",
        "    if clade_cols:\n",
        "        print(f\"[Diag] Available clade columns: {clade_cols}\")\n",
        "        for c in clade_cols:\n",
        "            cov = tmp[c].notna().mean()\n",
        "            uniq = tmp[c].nunique(dropna=True)\n",
        "            print(f\"[Diag] {c}: non-null coverage={cov:.2%}, unique levels={uniq}\")\n",
        "\n",
        "    return grp\n",
        "\n",
        "# ---- Choose the same view the builder used ----\n",
        "df_effective = SOURCE_DF if 'SOURCE_DF' in globals() else (\n",
        "    training_df_rex if 'training_df_rex' in globals() else None\n",
        ")\n",
        "\n",
        "if isinstance(df_effective, pd.DataFrame) and not df_effective.empty:\n",
        "    if 'LABEL_COL' in globals():\n",
        "        print(f\"[Diag] Using LABEL_COL from builder: {LABEL_COL}\")\n",
        "    # Show top labels seen by the builder (effective view)\n",
        "    with pd.option_context(\"display.max_rows\", DIAG_SHOW_TOP):\n",
        "        top_labels = df_effective[\"paralog_group\"].value_counts(dropna=True).head(DIAG_SHOW_TOP)\n",
        "        print(\"[Diag] Top labels in effective view:\")\n",
        "        display(top_labels)\n",
        "\n",
        "    print(\"\\nâ€” Effective view (used by builder) â€”\")\n",
        "    diag_grp_effective = _summary_training(df_effective)\n",
        "else:\n",
        "    print(\"[Diag] training data is missing or empty (no SOURCE_DF / training_df_rex).\")\n",
        "    diag_grp_effective = pd.DataFrame()\n",
        "\n",
        "# ---- Optional: compare with the raw training_df_rex view ----\n",
        "if DIAG_COMPARE_RAW_VIEW and 'training_df_rex' in globals() \\\n",
        "   and isinstance(training_df_rex, pd.DataFrame) and not training_df_rex.empty:\n",
        "    print(\"\\nâ€” Raw training_df_rex (for comparison only) â€”\")\n",
        "    _summary_training(training_df_rex)\n"
      ],
      "metadata": {
        "id": "bLQ4K58uvFom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cell 75 â€“ RegExTractor Matcher, Hits, and Chain Reconstruction\n",
        "\n",
        "This cell defines the **core matching and chain reconstruction primitives** that the classification engine relies on:\n",
        "\n",
        "1. **`RegExTractorMatcher`**  \n",
        "   - Provides a consistent interface for scoring regex matches against peptide sequences.  \n",
        "   - Implements `_score_hit(matched_str, tier) â†’ (score, gden)` which returns both a lengthâ€“conservationâ€“based score and a proxy for â€œg-head densityâ€ (`gden`).  \n",
        "   - The scoring function uses simple heuristics on the regex pattern itself, making it lightweight and reproducible.\n",
        "\n",
        "2. **`RexHit` dataclass**  \n",
        "   - Encapsulates information about a single regex match (accession, paralog group, exon number, clade, tier, start/end coordinates, peptide, conservation proxy, and score).  \n",
        "   - Designed to be serialisable (`.__dict__`) for logging or DataFrame conversion.\n",
        "\n",
        "3. **`RexChain` dataclass**  \n",
        "   - Represents a reconstructed exon chain for a given paralog group within a sequence.  \n",
        "   - Records the exons found, chain start/end, total score, and the length of the longest consecutive exon run.  \n",
        "   - Holds the mapping of exon numbers to their chosen `RexHit`.\n",
        "\n",
        "4. **Chain reconstruction utilities**  \n",
        "   - `rex_longest_consecutive_run(exons)` computes the longest run of consecutive exons detected.  \n",
        "   - `rex_walk_chain(seed, hits_by_exon, expected_exons)` assembles the best possible chain around a seed hit, extending left and right along the canonical exon order, subject to non-overlap and monotonic coordinate rules.\n",
        "\n",
        "---\n",
        "\n",
        "**Dependencies:**  \n",
        "- Must be executed after **Cell 73** (which builds the orthology-aware regex library).  \n",
        "- Relies on configuration constants defined in **Cell 70** (e.g. `rex_chain_min_consecutive`).  \n",
        "- Produces the functions and dataclasses that are invoked in the **Classification Engine** (Cell 74).\n",
        "\n",
        "This separation ensures that **library building (Cell 72)** and **matching/chain logic (Cell 72B)** remain modular and can be tested or modified independently."
      ],
      "metadata": {
        "id": "NQm-DxKcTjlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 75 =====\n",
        "# RegExTractor Matcher, Hit/Chain dataclasses, and Chain Reconstruction\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "\n",
        "# ----------------------------- Dataclasses ----------------------------- #\n",
        "\n",
        "@dataclass\n",
        "class RexHit:\n",
        "    \"\"\"\n",
        "    A single regex match against a target sequence, annotated with metadata.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    accession : str\n",
        "        Accession of the scanned sequence.\n",
        "    gene_symbol : str\n",
        "        Used in your pipeline to hold the paralog_group label.\n",
        "    exon_num_in_chain : int\n",
        "        Exon ordinal within the paralog architecture.\n",
        "    clade : str\n",
        "        Clade key used for the regex (e.g., 'pan').\n",
        "    tier : str\n",
        "        Tier label for the regex (e.g., 'consensus', 'stringent').\n",
        "    start : int\n",
        "        Match start (0-based, inclusive) on the sequence.\n",
        "    end : int\n",
        "        Match end (0-based, exclusive) on the sequence.\n",
        "    peptide : str\n",
        "        The matched substring.\n",
        "    gden : float\n",
        "        \"G-head density\" (or generalised pattern conservation) used as a gate.\n",
        "    score : float\n",
        "        Overall score used to choose among competing hits.\n",
        "    \"\"\"\n",
        "    accession: str\n",
        "    gene_symbol: str\n",
        "    exon_num_in_chain: int\n",
        "    clade: str\n",
        "    tier: str\n",
        "    start: int\n",
        "    end: int\n",
        "    peptide: str\n",
        "    gden: float\n",
        "    score: float\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RexChain:\n",
        "    \"\"\"\n",
        "    A reconstructed exon chain assembled from per-exon hits.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    accession : str\n",
        "        Accession of the scanned sequence.\n",
        "    paralog_group : str\n",
        "        Paralog group this chain is assigned to.\n",
        "    exons_found : List[int]\n",
        "        Sorted list of exon ordinals present in the chain.\n",
        "    start : int\n",
        "        Start coordinate of the chain span (min of hits).\n",
        "    end : int\n",
        "        End coordinate of the chain span (max of hits).\n",
        "    total_score : float\n",
        "        Sum (or other aggregator) of per-exon hit scores.\n",
        "    consecutive_blocks : int\n",
        "        Length (count) of the longest run of consecutive exons in the chain.\n",
        "    hits : Dict[int, RexHit]\n",
        "        Mapping exon_num_in_chain â†’ chosen hit.\n",
        "    \"\"\"\n",
        "    accession: str\n",
        "    paralog_group: str\n",
        "    exons_found: List[int]\n",
        "    start: int\n",
        "    end: int\n",
        "    total_score: float\n",
        "    consecutive_blocks: int\n",
        "    hits: Dict[int, RexHit]\n",
        "\n",
        "\n",
        "# ----------------------------- Matcher ----------------------------- #\n",
        "\n",
        "class RegExTractorMatcher:\n",
        "    \"\"\"\n",
        "    Lightweight matcher that scores regex hits consistently with your tier design.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    - The matcher exposes `_score_hit(matched_str, tier)` returning `(score, gden)`,\n",
        "      as expected by your Cell 73 orchestrator.\n",
        "    - `gden` is computed from the regex pattern as an approximation of positional\n",
        "      constraint (fraction of non-wildcard positions). The final `score` combines\n",
        "      length and conservation with a simple geometric-mean-like formulation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, library) -> None:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        library : OrthologyAwareLibrary\n",
        "            Not strictly required for scoring (tiers are provided by caller),\n",
        "            but kept for future extensions (e.g., clade-aware rescoring).\n",
        "        \"\"\"\n",
        "        self.library = library\n",
        "\n",
        "    @staticmethod\n",
        "    def _pattern_conservation(pattern: str) -> float:\n",
        "        \"\"\"\n",
        "        Approximate per-position constraint of a regex pattern in [0, 1].\n",
        "\n",
        "        Heuristic:\n",
        "        - Count positions as tokens:\n",
        "            '.'          â†’ unconstrained (0)\n",
        "            '[...]'      â†’ moderately constrained (min(1, len(set)/20))  # cap soft\n",
        "            literal char â†’ constrained (1)\n",
        "        - Escapes '\\\\.' and similar are treated as literals.\n",
        "        - Groups, quantifiers and other advanced constructs are rare in the\n",
        "          generated tiers; if present, they are conservatively treated as 0.\n",
        "\n",
        "        This does not parse the full regex grammar; it is a pragmatic estimator\n",
        "        for motif density that is robust for patterns generated by Cell 72.\n",
        "        \"\"\"\n",
        "        i, n = 0, len(pattern)\n",
        "        tokens = 0\n",
        "        constrained = 0.0\n",
        "\n",
        "        while i < n:\n",
        "            ch = pattern[i]\n",
        "\n",
        "            # Character class\n",
        "            if ch == '[':\n",
        "                j = i + 1\n",
        "                # Find closing bracket (naively, sufficient for our generated patterns)\n",
        "                while j < n and pattern[j] != ']':\n",
        "                    j += 1\n",
        "                content = pattern[i + 1 : j] if j < n else \"\"\n",
        "                # Treat the character set size as a soft constraint (larger set â†’ weaker)\n",
        "                k = len(set(content)) if content else 0\n",
        "                tokens += 1\n",
        "                constrained += min(1.0, max(0.0, 1.0 - (k - 1) / 19.0)) if k > 0 else 0.0\n",
        "                i = j + 1\n",
        "                continue\n",
        "\n",
        "            # Escaped literal (e.g., '\\.')\n",
        "            if ch == '\\\\' and i + 1 < n:\n",
        "                tokens += 1\n",
        "                constrained += 1.0\n",
        "                i += 2\n",
        "                continue\n",
        "\n",
        "            # Unconstrained position\n",
        "            if ch == '.':\n",
        "                tokens += 1\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # Literal single character\n",
        "            if ch.isalpha():\n",
        "                tokens += 1\n",
        "                constrained += 1.0\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # Any other construct: treat as 0 contribution, do not advance tokens\n",
        "            i += 1\n",
        "\n",
        "        return float(constrained) / float(tokens) if tokens > 0 else 0.0\n",
        "\n",
        "    def _score_hit(self, matched: str, tier) -> Tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Score a single regex match.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        matched : str\n",
        "            The matched peptide substring.\n",
        "        tier : object\n",
        "            Must expose: `.regex` (compiled pattern) and `.ghead_min` (float).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (score, gden) : Tuple[float, float]\n",
        "            gden is the conservation proxy in [0, 1].\n",
        "            score is a monotonic function of length and gden.\n",
        "        \"\"\"\n",
        "        # Conservation proxy from the pattern itself\n",
        "        patt = getattr(tier, \"regex\").pattern\n",
        "        gden = self._pattern_conservation(patt)\n",
        "\n",
        "        # Length component\n",
        "        L = len(matched)\n",
        "        # Combine length and conservation; sqrt dampens extreme lengths\n",
        "        score = (L ** 0.5) * (0.5 + 0.5 * gden)\n",
        "\n",
        "        return float(score), float(gden)\n",
        "\n",
        "\n",
        "# ----------------------- Chain reconstruction utils ----------------------- #\n",
        "\n",
        "def rex_longest_consecutive_run(exons: List[int]) -> int:\n",
        "    \"\"\"\n",
        "    Compute the length of the longest run of consecutive integers in `exons`.\n",
        "    \"\"\"\n",
        "    if not exons:\n",
        "        return 0\n",
        "    s = set(int(x) for x in exons)\n",
        "    best = 1\n",
        "    for x in s:\n",
        "        if (x - 1) not in s:\n",
        "            cur = 1\n",
        "            while (x + cur) in s:\n",
        "                cur += 1\n",
        "            if cur > best:\n",
        "                best = cur\n",
        "    return best\n",
        "\n",
        "\n",
        "def rex_walk_chain(\n",
        "    seed: RexHit,\n",
        "    hits_by_exon: Dict[int, RexHit],\n",
        "    expected_exons: List[int],\n",
        "    *,\n",
        "    enforce_order: bool = True,\n",
        ") -> RexChain:\n",
        "    \"\"\"\n",
        "    Greedy chain reconstruction around a seed hit.\n",
        "\n",
        "    The algorithm:\n",
        "      1) Start from the `seed` exon.\n",
        "      2) Attempt to extend left (lower exon numbers) and right (higher exon numbers)\n",
        "         following `expected_exons` order.\n",
        "      3) For each exon, accept the single best available hit (by score) that does not\n",
        "         overlap the current chain span and respects coordinate order if `enforce_order`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    seed : RexHit\n",
        "        The starting hit (usually the highest-scoring hit overall).\n",
        "    hits_by_exon : Dict[int, RexHit]\n",
        "        Best per-exon hits (already selected upstream), indexed by exon number.\n",
        "    expected_exons : List[int]\n",
        "        Paralog's canonical exon architecture (sorted).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    RexChain\n",
        "        The assembled chain.\n",
        "    \"\"\"\n",
        "    # Initialise chain state from the seed\n",
        "    chosen: Dict[int, RexHit] = {int(seed.exon_num_in_chain): seed}\n",
        "    span_start = seed.start\n",
        "    span_end = seed.end\n",
        "\n",
        "    # Determine walking order\n",
        "    if not expected_exons:\n",
        "        expected_exons = sorted(hits_by_exon.keys())\n",
        "\n",
        "    # Helper to try add an exon hit if constraints permit\n",
        "    def _try_add(exon: int) -> bool:\n",
        "        nonlocal span_start, span_end\n",
        "        h = hits_by_exon.get(exon)\n",
        "        if h is None:\n",
        "            return False\n",
        "\n",
        "        if enforce_order:\n",
        "            # Maintain non-overlap and monotonic coordinates relative to seed side\n",
        "            if exon < seed.exon_num_in_chain and not (h.end <= span_start):\n",
        "                return False\n",
        "            if exon > seed.exon_num_in_chain and not (h.start >= span_end):\n",
        "                return False\n",
        "\n",
        "        # Accept\n",
        "        chosen[exon] = h\n",
        "        span_start = min(span_start, h.start)\n",
        "        span_end = max(span_end, h.end)\n",
        "        return True\n",
        "\n",
        "    # Walk left (decreasing exon numbers)\n",
        "    seed_idx = expected_exons.index(seed.exon_num_in_chain) if seed.exon_num_in_chain in expected_exons else None\n",
        "    if seed_idx is not None:\n",
        "        for exon in reversed(expected_exons[:seed_idx]):\n",
        "            _try_add(exon)\n",
        "\n",
        "        # Walk right (increasing exon numbers)\n",
        "        for exon in expected_exons[seed_idx + 1 :]:\n",
        "            _try_add(exon)\n",
        "    else:\n",
        "        # Fallback: attempt all\n",
        "        for exon in sorted(x for x in expected_exons if x < seed.exon_num_in_chain):\n",
        "            _try_add(exon)\n",
        "        for exon in sorted(x for x in expected_exons if x > seed.exon_num_in_chain):\n",
        "            _try_add(exon)\n",
        "\n",
        "    exons_found = sorted(chosen.keys())\n",
        "    consecutive = rex_longest_consecutive_run(exons_found)\n",
        "    total_score = float(sum(chosen[e].score for e in exons_found))\n",
        "\n",
        "    chain = RexChain(\n",
        "        accession=seed.accession,\n",
        "        paralog_group=seed.gene_symbol,  # you store paralog_group in gene_symbol for hits\n",
        "        exons_found=exons_found,\n",
        "        start=span_start,\n",
        "        end=span_end,\n",
        "        total_score=total_score,\n",
        "        consecutive_blocks=consecutive,\n",
        "        hits=chosen,\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "\n",
        "# --------------------------- Runtime safeguards --------------------------- #\n",
        "\n",
        "# Provide a sane default for the orchestrator threshold if it was not defined earlier.\n",
        "try:\n",
        "    rex_chain_min_consecutive  # noqa: F823\n",
        "except NameError:\n",
        "    rex_chain_min_consecutive: int = 3\n"
      ],
      "metadata": {
        "id": "zoX09KJITnMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySnXwTHjjcSt"
      },
      "source": [
        "## Cell 76a â€“ Classification Cache (signature + reuse)\n",
        "\n",
        "This cell avoids re-running the classifier if **inputs havenâ€™t changed**.\n",
        "\n",
        "**What it does**\n",
        "- Computes stable SHA-256 signatures of:\n",
        "  - the effective **training view** (`SOURCE_DF` or `training_df_rex`), limited to\n",
        "    the columns the classifier actually uses (`paralog_group`, `exon_num_in_chain`,\n",
        "    `gene_symbol`, `gene_symbol_norm`).\n",
        "  - the **target pool** (`target_pool_rex`), limited to `accession` and `sequence`.\n",
        "  - the presence/size of the regex **library** if provided.\n",
        "- If signatures match the saved meta file, loads the **cached TSVs** instead of re-running.\n",
        "- If signatures differ or files are missing, runs classification and refreshes the cache.\n",
        "\n",
        "**Outputs**\n",
        "- Reuses your existing files:\n",
        "  - `CLASSIFIED_HITS_TSV`\n",
        "  - `CLASSIFIED_CHAINS_TSV`\n",
        "- Saves meta at `RUN_DIR/classification_cache.meta.json`.\n",
        "\n",
        "**Flags**\n",
        "- `ENABLE_CLASSIFY_CACHE` (default `True`)\n",
        "- `FORCE_RECLASSIFY` (default `False`)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 76a =====\n",
        "# Classification cache wrapper: signature + reuse\n",
        "\n",
        "import hashlib, json\n",
        "from pathlib import Path\n",
        "\n",
        "ENABLE_CLASSIFY_CACHE = bool(globals().get(\"ENABLE_CLASSIFY_CACHE\", True))\n",
        "FORCE_RECLASSIFY = bool(globals().get(\"FORCE_RECLASSIFY\", False))\n",
        "\n",
        "RUN_DIR = globals().get(\"RUN_DIR\", Path(\".\"))\n",
        "RUN_DIR = Path(RUN_DIR)\n",
        "CACHE_META = RUN_DIR / \"classification_cache.meta.json\"\n",
        "\n",
        "def _stable_df_signature(df: pd.DataFrame, cols: list) -> str:\n",
        "    \"\"\"\n",
        "    Return a stable SHA256 over selected columns (order-insensitive).\n",
        "    Sort by all selected cols, stringify, then hash.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        return \"EMPTY\"\n",
        "    use = [c for c in cols if c in df.columns]\n",
        "    if not use:\n",
        "        return \"NOCOLS\"\n",
        "    # Avoid huge memory: hash in chunks\n",
        "    h = hashlib.sha256()\n",
        "    chunk = (\n",
        "        df[use].copy()\n",
        "          .astype(\"string\")\n",
        "          .fillna(\"\")\n",
        "          .sort_values(by=use)\n",
        "          .to_csv(index=False, header=True)\n",
        "          .encode(\"utf-8\")\n",
        "    )\n",
        "    h.update(chunk)\n",
        "    return h.hexdigest()\n",
        "\n",
        "def _library_signature(lib) -> str:\n",
        "    \"\"\"Small signature for library shape (counts of entries/tiers).\"\"\"\n",
        "    try:\n",
        "        n_entries = len(getattr(lib, \"entries\", {}))\n",
        "        # Count total tiers for extra sensitivity\n",
        "        n_tiers = 0\n",
        "        for ent in getattr(lib, \"entries\", {}).values():\n",
        "            n_tiers += len(getattr(ent, \"tiers\", []))\n",
        "        return f\"E{n_entries}_T{n_tiers}\"\n",
        "    except Exception:\n",
        "        return \"NO_LIB\"\n",
        "\n",
        "def _save_meta(meta: dict):\n",
        "    try:\n",
        "        CACHE_META.write_text(json.dumps(meta, indent=2))\n",
        "    except Exception as e:\n",
        "        rex_log(f\"[ClassifyCache] Could not write meta: {e}\")\n",
        "\n",
        "def _load_meta() -> dict:\n",
        "    try:\n",
        "        if CACHE_META.exists():\n",
        "            return json.loads(CACHE_META.read_text())\n",
        "    except Exception:\n",
        "        pass\n",
        "    return {}\n",
        "\n",
        "def _can_reuse_cache(meta_now: dict, meta_prev: dict) -> bool:\n",
        "    keys = [\"sig_training\", \"sig_target\", \"sig_library\",\n",
        "            \"hits_tsv\", \"chains_tsv\"]\n",
        "    if not all(k in meta_prev for k in keys):\n",
        "        return False\n",
        "    # same signatures?\n",
        "    if any(meta_prev.get(k) != meta_now.get(k) for k in [\"sig_training\",\"sig_target\",\"sig_library\"]):\n",
        "        return False\n",
        "    # files exist and sizes match prior record\n",
        "    ht, ct = Path(meta_prev[\"hits_tsv\"]), Path(meta_prev[\"chains_tsv\"])\n",
        "    if not ht.exists() or not ct.exists():\n",
        "        return False\n",
        "    if meta_prev.get(\"hits_bytes\", -1) != ht.stat().st_size:\n",
        "        return False\n",
        "    if meta_prev.get(\"chains_bytes\", -1) != ct.stat().st_size:\n",
        "        return False\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "Anx7vc-iztIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 76b â€“ Execute Classification (with cache)\n",
        "\n",
        "Uses the cache if valid; otherwise runs your existing **Cell 76** logic and writes meta.\n",
        "\n",
        "*No renames; your `run_classification_engine` stays intact.*\n"
      ],
      "metadata": {
        "id": "wKHNkkIT0Cth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 79 â€“ Ensembl ID Coverage Audit (folded from Part 8)\n",
        "\n",
        "This cell **audits** `ensembl_id` coverage produced earlier in Part 7 and (optionally)\n",
        "writes a gap report. It does **not** perform any network/API lookups or refilling.\n",
        "\n",
        "**Inputs**\n",
        "- `df_high_quality` â€” the consolidated, high-quality table produced by Part 7.\n",
        "- `RUN_DIR` (optional) â€” output directory for artifacts.\n",
        "\n",
        "**Outputs**\n",
        "- Console summary of Ensembl ID coverage.\n",
        "- Optional: `RUN_DIR/ensembl_id_gaps.tsv` when coverage < threshold.\n",
        "\n",
        "**Notes**\n",
        "- Idempotent; does not rename or remove any columns.\n",
        "- Ensures required columns exist with stable dtypes.\n"
      ],
      "metadata": {
        "id": "ga4el2KQGzKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 79 =====\n",
        "# Ensembl ID Coverage Audit (no lookups; folded from former Part 8)\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Parameters (safe defaults; editable) ----\n",
        "MIN_COVERAGE = 0.90  #@param {type:\"number\", min:0.0, max:1.0}\n",
        "WRITE_GAP_REPORT = bool(globals().get(\"WRITE_GAP_REPORT\", True))  #@param {type:\"boolean\"}\n",
        "\n",
        "# ---- Resolve output dir ----\n",
        "RUN_DIR = globals().get(\"RUN_DIR\", Path(\".\"))  # defined upstream in your pipeline\n",
        "RUN_DIR = Path(RUN_DIR)\n",
        "\n",
        "def _ensure_id_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Ensure ID columns exist with robust nullable dtypes (idempotent).\n",
        "    Does not overwrite existing values.\n",
        "    \"\"\"\n",
        "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        return df\n",
        "    for col, dtype in ((\"ensembl_id\", \"string\"),\n",
        "                       (\"ensembl_id_source\", \"string\"),\n",
        "                       (\"ensembl_id_conf\", \"Float64\")):\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.Series(pd.NA, index=df.index, dtype=dtype)\n",
        "        else:\n",
        "            try:\n",
        "                if dtype == \"Float64\":\n",
        "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Float64\")\n",
        "                else:\n",
        "                    df[col] = df[col].astype(\"string\")\n",
        "            except Exception:\n",
        "                # best-effort coercion\n",
        "                if dtype == \"Float64\":\n",
        "                    df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"Float64\")\n",
        "                else:\n",
        "                    df[col] = df[col].astype(\"string\")\n",
        "    return df\n",
        "\n",
        "def _coverage_stats(df: pd.DataFrame) -> tuple[int, int, float]:\n",
        "    \"\"\"Return (#have, total, fraction) for ensembl_id coverage.\"\"\"\n",
        "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
        "        return (0, 0, 0.0)\n",
        "    if \"ensembl_id\" not in df.columns:\n",
        "        return (0, len(df), 0.0)\n",
        "    s = df[\"ensembl_id\"].astype(\"string\")\n",
        "    have = int(s.fillna(\"\").str.len().gt(0).sum())\n",
        "    total = len(df)\n",
        "    frac = (have / total) if total else 0.0\n",
        "    return (have, total, frac)\n",
        "\n",
        "# ---- Audit run ----\n",
        "if \"df_high_quality\" in globals() and isinstance(df_high_quality, pd.DataFrame) and not df_high_quality.empty:\n",
        "    df_high_quality = _ensure_id_cols(df_high_quality)\n",
        "\n",
        "    have, total, frac = _coverage_stats(df_high_quality)\n",
        "    print(f\"[ID-Audit] df_high_quality Ensembl coverage: {have}/{total} = {frac:.2%}\")\n",
        "\n",
        "    if WRITE_GAP_REPORT and frac < MIN_COVERAGE:\n",
        "        gap_mask = df_high_quality[\"ensembl_id\"].astype(\"string\").fillna(\"\").str.len().eq(0)\n",
        "        gaps = df_high_quality.loc[gap_mask].copy()\n",
        "        cols = [c for c in [\n",
        "            \"species\",\"Organism\",\"gene_symbol_norm\",\"gene_symbol\",\n",
        "            \"Entry\",\"accession\",\"Protein names\",\"paralog_group\"\n",
        "        ] if c in gaps.columns]\n",
        "        outp = RUN_DIR / \"ensembl_id_gaps.tsv\"\n",
        "        (gaps[cols] if cols else gaps).to_csv(outp, sep=\"\\t\", index=False)\n",
        "        print(f\"[ID-Audit] Coverage below threshold ({MIN_COVERAGE:.0%}). Gap list â†’ {outp}\")\n",
        "else:\n",
        "    print(\"[ID-Audit] df_high_quality not found or empty.\")\n"
      ],
      "metadata": {
        "id": "d1VYelthHQIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_high_quality.head(20)"
      ],
      "metadata": {
        "id": "z7dkk-fvNs-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 9: X/Y Substitution, Recovery & Outliers**"
      ],
      "metadata": {
        "id": "eKepxvJZrIs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90 â€“ Bridge Part 7 â†’ RegExTractor Inputs\n",
        "This cell adapts `df_high_quality` from Part 7 into the structures required by the\n",
        "RegExTractor (Cells 93â€“94). It:\n",
        "- Verifies required columns from Part 7 are present.\n",
        "- Resolves a **single** `paralog_group` (e.g., COL1A1, COL1A2, COL2A1, â€¦).  \n",
        "  Ambiguities like `\"COL1A1 COL1A2\"` are set to `\"unknown_paralog\"` unless already\n",
        "  disambiguated upstream.\n",
        "- Extracts the first triple-helix span from `gxy_spans` into `gxystart`/`gxyend`.\n",
        "- Emits `training_df_rex` (kept for backward compatibility) and `rex_sequences_df`\n",
        "  used by Cells 93â€“94.\n",
        "\n",
        "> Assumptions:\n",
        "> * Part 7 produced `df_high_quality` with UniProt/Ensembl-mapped sequences, species,\n",
        ">   and a list column of G-X-Y spans (here assumed as `gxy_spans`).\n",
        "> * We **do not** rename existing functions or globals used later; we only add safe\n",
        ">   adapters and checks.\n",
        "\n",
        "**Outputs used later**\n",
        "- `rex_sequences_df` : tidy, per-sequence inputs for the recovery walker.\n",
        "- `training_df_rex`  : same rows + a limited superset of columns for training paths.\n",
        "- `PARALOG_ALLOWED_SET` : allowed gene symbols for strict mapping.\n"
      ],
      "metadata": {
        "id": "BzyE3XG2PCbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90 =====\n",
        "# Bridge Part 7 â†’ RegExTractor inputs (non-breaking adapter)\n",
        "\n",
        "import logging\n",
        "from typing import List, Optional\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# --- Configuration (do not rename downstream globals) ---\n",
        "# Allowed fibrillar collagen genes for strict paralog resolution (extend as needed)\n",
        "PARALOG_ALLOWED_SET = {\n",
        "    \"COL1A1\",\"COL1A2\",\"COL2A1\",\"COL3A1\",\"COL5A1\",\"COL5A2\",\"COL5A3\",\n",
        "    \"COL10A1\",\"COL11A1\",\"COL11A2\",\"COL12A1\",\"COL16A1\",\"COL21A1\",\"COL22A1\",\n",
        "    \"COL8A1\",\"COL6A1\",\"COL6A3\",\"COL4A3\",\"COL4A4\",\"COL4A5\"  # keep list in sync with your pipeline\n",
        "}\n",
        "\n",
        "# Column name hints seen in your Part 7 preview\n",
        "CANDIDATE_GENE_COLS = [\"gene_name\", \"Gene Names (primary)\", \"primary_gene\", \"uniprot_gene\"]\n",
        "CANDIDATE_SPECIES_COLS = [\"Organism\", \"species\", \"taxon_name\"]\n",
        "CANDIDATE_ACCESSION_COLS = [\"Entry\", \"accession\", \"uniprot_id\"]\n",
        "CANDIDATE_SEQ_COLS = [\"Sequence\", \"sequence\", \"aa_sequence\"]\n",
        "GXY_SPANS_COL = \"gxy_spans\"  # Part 7 preview showed a list like [{'start': 201, 'end': 1217}]\n",
        "\n",
        "def _first_present(df, candidates: List[str]) -> Optional[str]:\n",
        "    for c in candidates:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _resolve_paralog(val: Optional[str]) -> str:\n",
        "    \"\"\"\n",
        "    Map any gene string to a single canonical paralog_group.\n",
        "    - If multiple symbols are space-separated (e.g., \"COL1A1 COL1A2\"), mark as unknown.\n",
        "    - If not in allowed set (or NaN), mark as unknown.\n",
        "    \"\"\"\n",
        "    if not isinstance(val, str) or not val.strip():\n",
        "        return \"unknown_paralog\"\n",
        "    parts = [p.strip() for p in val.replace(\",\", \" \").split() if p.strip()]\n",
        "    # keep only exact allowed tokens\n",
        "    exact = [p for p in parts if p in PARALOG_ALLOWED_SET]\n",
        "    if len(exact) == 1:\n",
        "        return exact[0]\n",
        "    # ambiguous or none\n",
        "    return \"unknown_paralog\"\n",
        "\n",
        "def _extract_gxy_bounds(spans):\n",
        "    \"\"\"\n",
        "    Given a list like [{'start': 179, 'end': 1192}, ...], return (start, end).\n",
        "    If missing/empty, return (None, None).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(spans, list) and spans:\n",
        "            s = spans[0]\n",
        "            return int(s.get(\"start\")), int(s.get(\"end\"))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "# --- Preconditions ---\n",
        "if 'df_high_quality' not in globals():\n",
        "    raise RuntimeError(\"FATAL: Part 7 output 'df_high_quality' not found.\")\n",
        "\n",
        "dfhq = df_high_quality.copy()\n",
        "\n",
        "acc_col = _first_present(dfhq, CANDIDATE_ACCESSION_COLS)\n",
        "sp_col  = _first_present(dfhq, CANDIDATE_SPECIES_COLS)\n",
        "gn_col  = _first_present(dfhq, CANDIDATE_GENE_COLS)\n",
        "seq_col = _first_present(dfhq, CANDIDATE_SEQ_COLS)\n",
        "\n",
        "missing = [n for n, v in {\n",
        "    \"accession\": acc_col, \"species\": sp_col, \"gene\": gn_col, \"sequence\": seq_col\n",
        "}.items() if v is None]\n",
        "\n",
        "if missing:\n",
        "    raise RuntimeError(f\"FATAL: df_high_quality is missing required columns: {missing}\")\n",
        "\n",
        "if GXY_SPANS_COL not in dfhq.columns:\n",
        "    # tolerate absence; walker can still run but ordering is weaker\n",
        "    logger.warning(f\"[Bridge] '{GXY_SPANS_COL}' missing; gxystart/gxyend will be None.\")\n",
        "\n",
        "# --- Build RegExTractor-ready view ---\n",
        "tmp = dfhq[[acc_col, sp_col, gn_col, seq_col] + ([GXY_SPANS_COL] if GXY_SPANS_COL in dfhq else [])].copy()\n",
        "tmp = tmp.rename(columns={\n",
        "    acc_col: \"accession\",\n",
        "    sp_col:  \"species\",\n",
        "    gn_col:  \"gene_name_raw\",\n",
        "    seq_col: \"sequence\"\n",
        "})\n",
        "\n",
        "# Resolve paralog_group\n",
        "tmp[\"paralog_group\"] = tmp[\"gene_name_raw\"].map(_resolve_paralog)\n",
        "\n",
        "# Extract G-X-Y bounds if present\n",
        "if GXY_SPANS_COL in tmp.columns:\n",
        "    bounds = tmp[GXY_SPANS_COL].apply(_extract_gxy_bounds)\n",
        "    tmp[\"gxystart\"] = bounds.apply(lambda t: t[0])\n",
        "    tmp[\"gxyend\"]   = bounds.apply(lambda t: t[1])\n",
        "else:\n",
        "    tmp[\"gxystart\"] = None\n",
        "    tmp[\"gxyend\"]   = None\n",
        "\n",
        "# Minimal hygiene\n",
        "before = len(tmp)\n",
        "tmp = tmp.dropna(subset=[\"accession\", \"species\"]).drop_duplicates(subset=[\"accession\"])\n",
        "after = len(tmp)\n",
        "\n",
        "logger.info(f\"[Bridge] RegExTractor input built: {after} rows (dropped {before-after} empty/dupes).\")\n",
        "logger.info(f\"[Bridge] Paralog distribution:\\n{tmp['paralog_group'].value_counts(dropna=False).to_string()}\")\n",
        "\n",
        "# Flag ambiguous rows so you can decide to keep/exclude\n",
        "tmp[\"is_ambiguous_paralog\"] = (tmp[\"paralog_group\"] == \"unknown_paralog\")\n",
        "\n",
        "ambig_n = int(tmp[\"is_ambiguous_paralog\"].sum())\n",
        "if ambig_n > 0:\n",
        "    logger.warning(f\"[Bridge] Ambiguous/unknown paralog rows: {ambig_n}. \"\n",
        "                   f\"Examples:\\n{tmp.loc[tmp['is_ambiguous_paralog'], ['accession','gene_name_raw']].head(10).to_string(index=False)}\")\n",
        "\n",
        "# --- Emit both legacy and new globals (non-breaking) ---\n",
        "rex_sequences_df = tmp  # primary input for Cells 93â€“94\n",
        "training_df_rex  = tmp.copy()  # kept for backward compatibility with earlier cells\n",
        "\n",
        "# Optional: exclude ambiguous rows here (safer for recovery). If you prefer to keep them for later disambiguation,\n",
        "# comment the next two lines.\n",
        "rex_sequences_df = rex_sequences_df.loc[~rex_sequences_df[\"is_ambiguous_paralog\"]].copy()\n",
        "training_df_rex  = training_df_rex.loc[~training_df_rex[\"is_ambiguous_paralog\"]].copy()\n",
        "\n",
        "logger.info(f\"[Bridge] Final rex_sequences_df: {len(rex_sequences_df)} rows; \"\n",
        "            f\"unknown_paralog removed: {ambig_n - int(rex_sequences_df['is_ambiguous_paralog'].sum())}\")\n"
      ],
      "metadata": {
        "id": "7tjtXfCtPIo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90b â€“ Compute G-X-Y spans from sequence (triple-helix bounds)\n",
        "This cell scans each sequence for the longest region consistent with collagen\n",
        "Glyâ€“Xâ€“Y periodicity (every 3rd residue is G). It emits `gxy_spans` as a list\n",
        "of dicts (first span used for `gxystart/gxyend`), so later cells can order\n",
        "anchors within the triple helix.\n",
        "\n",
        "Heuristics:\n",
        "- Require a **minimum run length** (default 300 aa) to count as a helix.\n",
        "- Allow a small number of **local mismatches** (frameshift/misreads) via a\n",
        "  tolerance window (sliding reset after too many non-G at 3rd positions).\n",
        "- Never renames existing columns; only **adds** `gxy_spans`, `gxystart`, `gxyend`.\n"
      ],
      "metadata": {
        "id": "ZgLDU41jPrjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90b =====\n",
        "# Compute G-X-Y spans (triple helix) from raw sequences\n",
        "\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "def _find_gxy_spans(seq: str,\n",
        "                    min_span_len: int = 300,\n",
        "                    max_off_period_misses: int = 2) -> List[Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Find contiguous regions where every 3rd residue is 'G' (Gly-X-Y motif).\n",
        "    Tolerates brief local disruptions up to `max_off_period_misses` before\n",
        "    closing the current span.\n",
        "\n",
        "    Returns list of dicts: [{'start': i0, 'end': i1}, ...]  (0-based, end exclusive)\n",
        "    \"\"\"\n",
        "    spans: List[Tuple[int,int]] = []\n",
        "    n = len(seq)\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        # Try to align to a frame so that positions i, i+3, ... are 'G'\n",
        "        best_frame = None\n",
        "        best_len = 0\n",
        "        best_span = None\n",
        "\n",
        "        for frame in (0, 1, 2):\n",
        "            start = i + frame\n",
        "            if start >= n:\n",
        "                continue\n",
        "            j = start\n",
        "            misses = 0\n",
        "            # advance in steps of 1 but enforce 'G' at (j-start)%3==0\n",
        "            while j < n:\n",
        "                if ((j - start) % 3 == 0) and (seq[j] != 'G'):\n",
        "                    misses += 1\n",
        "                    if misses > max_off_period_misses:\n",
        "                        break\n",
        "                j += 1\n",
        "            span_len = j - start\n",
        "            if span_len > best_len:\n",
        "                best_len = span_len\n",
        "                best_frame = frame\n",
        "                best_span = (start, j)\n",
        "\n",
        "        if best_span and (best_span[1] - best_span[0]) >= min_span_len:\n",
        "            spans.append(best_span)\n",
        "            i = best_span[1]  # continue after this span\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # Merge adjacent/near-adjacent spans separated by very short gaps (â‰¤6 aa)\n",
        "    merged: List[Tuple[int,int]] = []\n",
        "    for s in spans:\n",
        "        if not merged:\n",
        "            merged.append(s)\n",
        "        else:\n",
        "            a0, a1 = merged[-1]\n",
        "            b0, b1 = s\n",
        "            if b0 - a1 <= 6:\n",
        "                merged[-1] = (a0, max(a1, b1))\n",
        "            else:\n",
        "                merged.append(s)\n",
        "\n",
        "    return [{\"start\": s, \"end\": e} for (s, e) in merged]\n",
        "\n",
        "if 'rex_sequences_df' not in globals():\n",
        "    raise RuntimeError(\"FATAL: rex_sequences_df not found. Run Cell 90 first.\")\n",
        "\n",
        "rex_sequences_df = rex_sequences_df.copy()\n",
        "rex_sequences_df[\"gxy_spans\"] = rex_sequences_df[\"sequence\"].apply(_find_gxy_spans)\n",
        "\n",
        "# Fill gxystart/gxyend from first span (if present)\n",
        "def _first_bounds(spans):\n",
        "    if isinstance(spans, list) and spans:\n",
        "        s = spans[0]\n",
        "        return int(s[\"start\"]), int(s[\"end\"])\n",
        "    return None, None\n",
        "\n",
        "bounds = rex_sequences_df[\"gxy_spans\"].apply(_first_bounds)\n",
        "rex_sequences_df[\"gxystart\"] = bounds.apply(lambda t: t[0])\n",
        "rex_sequences_df[\"gxyend\"]   = bounds.apply(lambda t: t[1])\n",
        "\n",
        "logger.info(\n",
        "    \"[GXY] gxy_spans computed. With bounds: \"\n",
        "    f\"{int(rex_sequences_df['gxystart'].notna().sum())} / {len(rex_sequences_df)} \"\n",
        "    f\"({100.0*rex_sequences_df['gxystart'].notna().mean():.1f}%).\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "AZTpXrRxPrOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90c â€“ Canonicalise/Rescue `paralog_group`\n",
        "This cell reduces `unknown_paralog` rows by:\n",
        "1) **Canonicalising gene symbols** using an alias map (e.g., synonyms, case/spacing).\n",
        "2) **Parsing protein names** (e.g., â€œCollagen type I alpha 1 chainâ€) to set a\n",
        "   **single** canonical `paralog_group`.\n",
        "3) (**Optional**) A **G-X-Y length heuristic**: if the triple-helix length\n",
        "   matches a narrow window typical of a paralog (e.g., ~1014 aa for COL1A1/1A2,\n",
        "   ~1284 aa for COL2A1, etc.), assign provisionally.\n",
        "\n",
        "All assignments are conservative and never overwrite an existing confident label.\n",
        "We keep your `PARALOG_ALLOWED_SET`; anything outside remains unknown.\n"
      ],
      "metadata": {
        "id": "WG6cV6x6Qik1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90c =====\n",
        "# Canonicalise/Rescue paralog_group from aliases, protein names, and optional length heuristics\n",
        "\n",
        "import re\n",
        "from typing import Optional\n",
        "\n",
        "if 'rex_sequences_df' not in globals():\n",
        "    raise RuntimeError(\"FATAL: rex_sequences_df not found. Run Cell 90 first.\")\n",
        "\n",
        "# Candidate protein-name columns from Part 7 exports\n",
        "CANDIDATE_PNAME_COLS = [\"Protein names\", \"Protein Name\", \"protein_name\", \"Entry name\"]\n",
        "\n",
        "# 1) Alias map for gene symbols â†’ canonical\n",
        "GENE_ALIAS_TO_CANON = {\n",
        "    # exact canonical passthrough\n",
        "    \"COL1A1\":\"COL1A1\",\"COL1A2\":\"COL1A2\",\"COL2A1\":\"COL2A1\",\"COL3A1\":\"COL3A1\",\n",
        "    \"COL5A1\":\"COL5A1\",\"COL5A2\":\"COL5A2\",\"COL5A3\":\"COL5A3\",\n",
        "    \"COL10A1\":\"COL10A1\",\"COL11A1\":\"COL11A1\",\"COL11A2\":\"COL11A2\",\n",
        "    \"COL12A1\":\"COL12A1\",\"COL16A1\":\"COL16A1\",\"COL21A1\":\"COL21A1\",\"COL22A1\":\"COL22A1\",\n",
        "    \"COL8A1\":\"COL8A1\",\"COL6A1\":\"COL6A1\",\"COL6A3\":\"COL6A3\",\"COL4A3\":\"COL4A3\",\n",
        "    \"COL4A4\":\"COL4A4\",\"COL4A5\":\"COL4A5\",\n",
        "    # common noisy forms\n",
        "    \"COL1A1/1A2\":\"unknown_paralog\", \"COL1A1 COL1A2\":\"unknown_paralog\",\n",
        "    \"COL1A1;COL1A2\":\"unknown_paralog\", \"COL1A1, COL1A2\":\"unknown_paralog\",\n",
        "}\n",
        "\n",
        "def _canon_gene(g: Optional[str]) -> str:\n",
        "    if not isinstance(g, str) or not g.strip():\n",
        "        return \"unknown_paralog\"\n",
        "    g = g.strip().upper().replace(\" \", \"\")\n",
        "    if g in GENE_ALIAS_TO_CANON:\n",
        "        return GENE_ALIAS_TO_CANON[g]\n",
        "    # quick acceptance of exact canonical tokens\n",
        "    if g in PARALOG_ALLOWED_SET:\n",
        "        return g\n",
        "    return \"unknown_paralog\"\n",
        "\n",
        "# 2) Parse protein names like \"Collagen type I alpha 1 chain\"\n",
        "_PN_PATTERNS = [\n",
        "    # (regex, canonical)\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+I\\s+ALPHA\\s*1\\b\", re.I),  \"COL1A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+I\\s+ALPHA\\s*2\\b\", re.I),  \"COL1A2\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+II\\s+ALPHA\\s*1\\b\", re.I), \"COL2A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+III\\s+ALPHA\\s*1\\b\", re.I),\"COL3A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+V\\s+ALPHA\\s*1\\b\", re.I),  \"COL5A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+V\\s+ALPHA\\s*2\\b\", re.I),  \"COL5A2\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+V\\s+ALPHA\\s*3\\b\", re.I),  \"COL5A3\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+XI\\s+ALPHA\\s*1\\b\", re.I), \"COL11A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+XI\\s+ALPHA\\s*2\\b\", re.I), \"COL11A2\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+XII\\s+ALPHA\\s*1\\b\", re.I),\"COL12A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+XVI\\s+ALPHA\\s*1\\b\", re.I),\"COL16A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+XXI\\s+ALPHA\\s*1\\b\", re.I),\"COL21A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+XXII\\s+ALPHA\\s*1\\b\", re.I),\"COL22A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+VIII\\s+ALPHA\\s*1\\b\", re.I),\"COL8A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+VI\\s+ALPHA\\s*1\\b\", re.I), \"COL6A1\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+VI\\s+ALPHA\\s*3\\b\", re.I), \"COL6A3\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+IV\\s+ALPHA\\s*3\\b\", re.I), \"COL4A3\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+IV\\s+ALPHA\\s*4\\b\", re.I), \"COL4A4\"),\n",
        "    (re.compile(r\"\\bCOLLAGEN\\s+TYPE\\s+IV\\s+ALPHA\\s*5\\b\", re.I), \"COL4A5\"),\n",
        "]\n",
        "\n",
        "def _pn_to_paralog(pn: Optional[str]) -> str:\n",
        "    if not isinstance(pn, str) or not pn:\n",
        "        return \"unknown_paralog\"\n",
        "    for pat, canon in _PN_PATTERNS:\n",
        "        if pat.search(pn):\n",
        "            return canon\n",
        "    return \"unknown_paralog\"\n",
        "\n",
        "pname_col = next((c for c in CANDIDATE_PNAME_COLS if c in rex_sequences_df.columns), None)\n",
        "\n",
        "# Step A: canonicalise any existing gene label (only fill unknowns)\n",
        "mask_unknown = rex_sequences_df[\"paralog_group\"] == \"unknown_paralog\"\n",
        "rex_sequences_df.loc[mask_unknown, \"paralog_group\"] = rex_sequences_df.loc[mask_unknown, \"gene_name_raw\"].map(_canon_gene)\n",
        "\n",
        "# Step B: rescue via protein names where still unknown\n",
        "if pname_col:\n",
        "    mask_unknown = rex_sequences_df[\"paralog_group\"] == \"unknown_paralog\"\n",
        "    rescued = rex_sequences_df.loc[mask_unknown, pname_col].map(_pn_to_paralog)\n",
        "    # Only accept rescues that are in allowed set\n",
        "    rescued = rescued.where(rescued.isin(PARALOG_ALLOWED_SET), other=\"unknown_paralog\")\n",
        "    rex_sequences_df.loc[mask_unknown, \"paralog_group\"] = rescued\n",
        "\n",
        "# Step C (optional): GXY-length heuristic (very conservative default OFF)\n",
        "USE_GXY_LENGTH_HEURISTIC = False\n",
        "GXY_LENGTH_WINDOWS = {\n",
        "    # helix aa lengths are approximate (end-start); tighten/expand as you wish\n",
        "    \"COL1A1\": (900, 1050),  # ~1014 typical\n",
        "    \"COL1A2\": (900, 1050),\n",
        "    \"COL2A1\": (1200, 1350), # ~1284 typical\n",
        "    \"COL3A1\": (975, 1125),  # ~1029 typical\n",
        "}\n",
        "if USE_GXY_LENGTH_HEURISTIC:\n",
        "    m = rex_sequences_df[\"paralog_group\"] == \"unknown_paralog\"\n",
        "    lengths = (rex_sequences_df[\"gxyend\"] - rex_sequences_df[\"gxystart\"]).where(m, other=None)\n",
        "    for canon, (lo, hi) in GXY_LENGTH_WINDOWS.items():\n",
        "        sel = m & lengths.between(lo, hi, inclusive=\"both\")\n",
        "        rex_sequences_df.loc[sel, \"paralog_group\"] = canon\n",
        "\n",
        "# Mirror back to training_df_rex to keep older cells happy\n",
        "training_df_rex = rex_sequences_df.copy()\n",
        "\n",
        "# Report\n",
        "unk = int((rex_sequences_df[\"paralog_group\"] == \"unknown_paralog\").sum())\n",
        "logger.info(f\"[Paralog Rescue] unknown_paralog remaining: {unk} / {len(rex_sequences_df)} \"\n",
        "            f\"({100.0*unk/len(rex_sequences_df):.1f}%).\")\n",
        "by_para = rex_sequences_df.groupby(\"paralog_group\")[\"accession\"].nunique().sort_values(ascending=False)\n",
        "logger.info(f\"[Paralog Rescue] Unique accessions by paralog_group:\\n{by_para.to_string()}\")\n"
      ],
      "metadata": {
        "id": "cJjuD-o3Qnd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90d â€“ Triple-Helix Bounds Preflight (auto-compute if missing)\n",
        "Ensures `gxystart/gxyend` are populated. If missing, compute `gxy_spans` from\n",
        "the sequence (longest Gâ€“Xâ€“Y run) and fill bounds. Prints coverage.\n"
      ],
      "metadata": {
        "id": "Bt0t0uzRR4b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90d =====\n",
        "# Ensure gxystart/gxyend exist; auto-compute from sequences if needed.\n",
        "\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "def _find_gxy_spans(seq: str,\n",
        "                    min_span_len: int = 300,\n",
        "                    max_off_period_misses: int = 2) -> List[Dict[str, int]]:\n",
        "    spans: List[Tuple[int,int]] = []\n",
        "    n = len(seq)\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        best_span = None\n",
        "        best_len = 0\n",
        "        for frame in (0, 1, 2):\n",
        "            start = i + frame\n",
        "            if start >= n:\n",
        "                continue\n",
        "            j = start\n",
        "            misses = 0\n",
        "            while j < n:\n",
        "                if ((j - start) % 3 == 0) and (seq[j] != 'G'):\n",
        "                    misses += 1\n",
        "                    if misses > max_off_period_misses:\n",
        "                        break\n",
        "                j += 1\n",
        "            span_len = j - start\n",
        "            if span_len > best_len:\n",
        "                best_len = span_len\n",
        "                best_span = (start, j)\n",
        "        if best_span and (best_span[1] - best_span[0]) >= min_span_len:\n",
        "            spans.append(best_span)\n",
        "            i = best_span[1]\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    merged: List[Tuple[int,int]] = []\n",
        "    for s in spans:\n",
        "        if not merged:\n",
        "            merged.append(s)\n",
        "        else:\n",
        "            a0, a1 = merged[-1]\n",
        "            b0, b1 = s\n",
        "            if b0 - a1 <= 6:\n",
        "                merged[-1] = (a0, max(a1, b1))\n",
        "            else:\n",
        "                merged.append(s)\n",
        "    return [{\"start\": s, \"end\": e} for (s, e) in merged]\n",
        "\n",
        "def _first_bounds(spans):\n",
        "    if isinstance(spans, list) and spans:\n",
        "        s = spans[0]\n",
        "        return int(s[\"start\"]), int(s[\"end\"])\n",
        "    return None, None\n",
        "\n",
        "if 'rex_sequences_df' not in globals():\n",
        "    raise RuntimeError(\"FATAL: rex_sequences_df missing. Run Cell 90 first.\")\n",
        "\n",
        "rex_sequences_df = rex_sequences_df.copy()\n",
        "\n",
        "# Compute if missing\n",
        "need_spans = (\"gxy_spans\" not in rex_sequences_df.columns) or \\\n",
        "             (rex_sequences_df[\"gxystart\"].isna().all() if \"gxystart\" in rex_sequences_df else True)\n",
        "\n",
        "if need_spans:\n",
        "    rex_sequences_df[\"gxy_spans\"] = rex_sequences_df[\"sequence\"].apply(_find_gxy_spans)\n",
        "    bounds = rex_sequences_df[\"gxy_spans\"].apply(_first_bounds)\n",
        "    rex_sequences_df[\"gxystart\"] = bounds.apply(lambda t: t[0])\n",
        "    rex_sequences_df[\"gxyend\"]   = bounds.apply(lambda t: t[1])\n",
        "\n",
        "n_total = len(rex_sequences_df)\n",
        "n_bounds = int(rex_sequences_df[\"gxystart\"].notna().sum())\n",
        "logger.info(f\"[GXY] Bounds available: {n_bounds}/{n_total} \"\n",
        "            f\"({100.0 * (n_bounds/max(1,n_total)):.1f}%).\")\n"
      ],
      "metadata": {
        "id": "B75FvF81R7GG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90e â€“ Collagen Coverage Audit (Where did other collagens go?)\n",
        "This audit:\n",
        "1) Extracts raw collagen tokens from `df_high_quality` (`gene_name_raw` + protein\n",
        "   name fallbacks) using a permissive `COL(\\d+)A(\\d+)` detector.\n",
        "2) Compares them to the **final** `paralog_group` in `rex_sequences_df`.\n",
        "3) Reports which collagen families were present upstream but **not admitted**\n",
        "   to the final set (typically because theyâ€™re not in `PARALOG_ALLOWED_SET`\n",
        "   or our protein-name patterns didnâ€™t include them).\n",
        "\n",
        "Nothing is renamed; we only *read* existing globals and log a compact report.\n"
      ],
      "metadata": {
        "id": "Rh1jWrQzThN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90e =====\n",
        "# Collagen coverage audit: df_high_quality (raw) vs rex_sequences_df (final)\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "if 'df_high_quality' not in globals():\n",
        "    raise RuntimeError(\"FATAL: df_high_quality not found (Part 7).\")\n",
        "if 'rex_sequences_df' not in globals():\n",
        "    raise RuntimeError(\"FATAL: rex_sequences_df not found (Cell 90).\")\n",
        "\n",
        "# columns we used earlier\n",
        "gn_col  = next((c for c in [\"gene_name_raw\",\"gene_name\",\"Gene Names (primary)\",\n",
        "                            \"primary_gene\",\"uniprot_gene\"] if c in df_high_quality.columns), None)\n",
        "pname_col_candidates = [\"Protein names\",\"Protein Name\",\"protein_name\",\"Entry name\"]\n",
        "pname_col = next((c for c in pname_col_candidates if c in df_high_quality.columns), None)\n",
        "\n",
        "if gn_col is None:\n",
        "    raise RuntimeError(\"FATAL: Could not find a gene name column in df_high_quality.\")\n",
        "\n",
        "# 1) Extract permissive collagen tokens from df_high_quality\n",
        "def _find_coll_tokens(s: str) -> list:\n",
        "    if not isinstance(s, str):\n",
        "        return []\n",
        "    # match COL # A #  (accept lower/upper, spaces or dashes)\n",
        "    pats = re.findall(r\"COL\\s*([0-9]+)\\s*A\\s*([0-9]+)\", s.replace(\"-\", \" \").upper())\n",
        "    return [f\"COL{a}A{b}\" for a,b in pats]\n",
        "\n",
        "raw_df = df_high_quality[[gn_col] + ([pname_col] if pname_col else [])].copy()\n",
        "raw_df[\"__raw_text__\"] = raw_df[gn_col].astype(str)\n",
        "if pname_col:\n",
        "    raw_df[\"__raw_text__\"] = raw_df[\"__raw_text__\"] + \" ; \" + raw_df[pname_col].astype(str)\n",
        "\n",
        "raw_df[\"raw_coll_tokens\"] = raw_df[\"__raw_text__\"].apply(_find_coll_tokens)\n",
        "\n",
        "# explode and count\n",
        "raw_tokens = (raw_df.explode(\"raw_coll_tokens\")\n",
        "                     .dropna(subset=[\"raw_coll_tokens\"]))\n",
        "raw_counts = raw_tokens[\"raw_coll_tokens\"].value_counts()\n",
        "\n",
        "# 2) Final paralog set from rex_sequences_df\n",
        "final_counts = rex_sequences_df[\"paralog_group\"].value_counts()\n",
        "\n",
        "# 3) Which collagens appeared upstream but not present downstream?\n",
        "raw_set   = set(raw_counts.index.tolist())\n",
        "final_set = set(final_counts.index.tolist())\n",
        "missing_downstream = sorted([c for c in raw_set if c not in final_set])\n",
        "\n",
        "print(\"=== Collagens detected in Part 7 (raw labels/protein names) ===\")\n",
        "print(raw_counts.head(50).to_string())\n",
        "print(\"\\n=== Collagens present in final rex_sequences_df ===\")\n",
        "print(final_counts.to_string())\n",
        "print(\"\\n=== Collagens seen upstream but not admitted downstream ===\")\n",
        "print(\", \".join(missing_downstream) if missing_downstream else \"(none)\")\n",
        "\n",
        "# Optional: show a few example rows for the first 5 \"missing\" families\n",
        "if missing_downstream:\n",
        "    probe = missing_downstream[:5]\n",
        "    ex = raw_tokens[raw_tokens[\"raw_coll_tokens\"].isin(probe)].head(20)\n",
        "    display_cols = [gn_col] + ([pname_col] if pname_col else [])\n",
        "    print(\"\\nExamples from df_high_quality carrying these missing labels:\")\n",
        "    print(ex[display_cols + [\"raw_coll_tokens\"]].to_string(index=False))\n"
      ],
      "metadata": {
        "id": "vDXKdr0XTs9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90f â€“ Anchor Catalog Coverage Audit (Do we even support them?)\n",
        "Even if we detect COL4A2, COL8A2, COL9A1, COL14A1, COL17A1 upstream, recovery\n",
        "wonâ€™t use them unless the **anchor catalog** contains ordered exon anchors for\n",
        "those chains. This cell lists which genes the catalog currently supports.\n"
      ],
      "metadata": {
        "id": "Hlr5WYMaT06_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90f =====\n",
        "# What genes/chains does the anchor catalog actually cover?\n",
        "\n",
        "if 'ANCHOR_CATALOG_DF' not in globals():\n",
        "    print(\"ANCHOR_CATALOG_DF not found. Run Cell 93 first.\")\n",
        "else:\n",
        "    cat = ANCHOR_CATALOG_DF\n",
        "    have_cols = [c for c in [\"gene\",\"chain\",\"exon_index\",\"anchor_regex\",\"loaded\"] if c in cat.columns]\n",
        "    print(f\"Catalog columns: {have_cols}\")\n",
        "    if \"gene\" in cat.columns:\n",
        "        print(\"\\nGenes present in catalog (top 50):\")\n",
        "        print(cat[\"gene\"].value_counts().head(50).to_string())\n",
        "        # useful set for comparison\n",
        "        catalog_genes = set(cat[\"gene\"].unique())\n",
        "        # pull upstream raw tokens again (from 90e) if available\n",
        "        try:\n",
        "            raw_tokens_set = set(raw_tokens[\"raw_coll_tokens\"].unique())\n",
        "            unseen = sorted([g for g in raw_tokens_set if g not in catalog_genes])\n",
        "            print(\"\\nCollagens referenced upstream but NOT in catalog:\")\n",
        "            print(\", \".join(unseen) if unseen else \"(none)\")\n",
        "        except Exception:\n",
        "            pass\n"
      ],
      "metadata": {
        "id": "MpaLU1m7T7pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4KmOKKu3TmPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 90g â€“ Expand rescue to additional collagen families (non-breaking)\n",
        "This cell:\n",
        "1) Extends `PARALOG_ALLOWED_SET` to include your â€œmissingâ€ families.\n",
        "2) Adds permissive protein-name patterns for those families (Type N, Î±M).\n",
        "3) Re-runs the rescue *only for rows still marked unknown* (safe, non-overwriting).\n",
        "\n",
        "**Note**: Recovery still requires anchors. If the anchor catalog has no entries\n",
        "for a gene, stitching will skip it even after rescue succeeds.\n"
      ],
      "metadata": {
        "id": "GHsPZaF3Uv5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 90g =====\n",
        "# Expand allow-list + protein-name patterns; re-run rescue for remaining unknowns\n",
        "\n",
        "import re\n",
        "\n",
        "if 'rex_sequences_df' not in globals():\n",
        "    raise RuntimeError(\"rex_sequences_df missing. Run Cell 90 first.\")\n",
        "\n",
        "# 1) Extend allowed set (explicit list from your audit)\n",
        "EXTENDED_PARALOGS = {\n",
        "    \"COL4A1\",\"COL4A2\",\"COL4A6\",\n",
        "    \"COL6A2\",\"COL6A5\",\"COL6A6\",\n",
        "    \"COL7A1\",\"COL8A2\",\"COL9A1\",\"COL9A2\",\"COL9A3\",\n",
        "    \"COL10A1\",\"COL13A1\",\"COL14A1\",\"COL15A1\",\"COL17A1\",\n",
        "    \"COL18A1\",\"COL19A1\",\"COL20A1\",\"COL23A1\",\"COL24A1\",\n",
        "    \"COL25A1\",\"COL27A1\",\n",
        "}\n",
        "PARALOG_ALLOWED_SET |= EXTENDED_PARALOGS\n",
        "\n",
        "# 2) Add protein-name patterns dynamically for added families\n",
        "# Accept both \"Collagen type XIV alpha 1\" and \"Collagen alpha-1(XIV)\"\n",
        "def _roman(n: int) -> str:\n",
        "    # minimal roman (up to 30-ish)\n",
        "    vals = [(10,'X'),(9,'IX'),(5,'V'),(4,'IV'),(1,'I')]\n",
        "    res, x = \"\", n\n",
        "    for v,s in vals:\n",
        "        while x>=v:\n",
        "            res += s; x -= v\n",
        "    return res\n",
        "\n",
        "_new_patterns = []\n",
        "for gene in sorted(EXTENDED_PARALOGS):\n",
        "    m = re.fullmatch(r\"COL(\\d+)A(\\d+)\", gene)\n",
        "    if not m:\n",
        "        continue\n",
        "    num = int(m.group(1)); a = int(m.group(2))\n",
        "    roman = _roman(num)\n",
        "    # Patterns:\n",
        "    #  A) \"Collagen type N alpha M\"\n",
        "    _new_patterns.append((re.compile(fr\"\\bCOLLAGEN\\s+TYPE\\s+{num}\\s+ALPHA\\s*{a}\\b\", re.I), gene))\n",
        "    _new_patterns.append((re.compile(fr\"\\bCOLLAGEN\\s+TYPE\\s+{roman}\\s+ALPHA\\s*{a}\\b\", re.I), gene))\n",
        "    #  B) \"Collagen alpha-M(N)\" where N can be arabic or roman, allow hyphen/space\n",
        "    _new_patterns.append((re.compile(fr\"\\bCOLLAGEN\\s+ALPHA[-\\s]*{a}\\s*\\(\\s*{num}\\s*\\)\\b\", re.I), gene))\n",
        "    _new_patterns.append((re.compile(fr\"\\bCOLLAGEN\\s+ALPHA[-\\s]*{a}\\s*\\(\\s*{roman}\\s*\\)\\b\", re.I), gene))\n",
        "\n",
        "# Attach to the existing PN patterns list if present; else local fallback\n",
        "if '_PN_PATTERNS' in globals() and isinstance(_PN_PATTERNS, list):\n",
        "    _PN_PATTERNS.extend(_new_patterns)\n",
        "else:\n",
        "    _PN_PATTERNS = _new_patterns  # local fallback if earlier cell not run\n",
        "\n",
        "# 3) Re-run rescue **only for unknowns**\n",
        "pname_col = next((c for c in [\"Protein names\",\"Protein Name\",\"protein_name\",\"Entry name\"]\n",
        "                  if c in rex_sequences_df.columns), None)\n",
        "\n",
        "def _pn_to_paralog(pn: str) -> str:\n",
        "    if not isinstance(pn, str) or not pn:\n",
        "        return \"unknown_paralog\"\n",
        "    for pat, canon in _PN_PATTERNS:\n",
        "        if pat.search(pn):\n",
        "            return canon\n",
        "    return \"unknown_paralog\"\n",
        "\n",
        "mask_unknown = rex_sequences_df[\"paralog_group\"].eq(\"unknown_paralog\")\n",
        "if pname_col:\n",
        "    rescued = rex_sequences_df.loc[mask_unknown, pname_col].map(_pn_to_paralog)\n",
        "    rescued = rescued.where(rescued.isin(PARALOG_ALLOWED_SET), other=\"unknown_paralog\")\n",
        "    rex_sequences_df.loc[mask_unknown, \"paralog_group\"] = rescued\n",
        "\n",
        "# Mirror to training_df_rex for backward compatibility\n",
        "training_df_rex = rex_sequences_df.copy()\n",
        "\n",
        "# Report change\n",
        "unk = int((rex_sequences_df[\"paralog_group\"] == \"unknown_paralog\").sum())\n",
        "logger.info(f\"[Rescue-Extended] unknown_paralog remaining: {unk} / {len(rex_sequences_df)}\")\n",
        "logger.info(f\"[Rescue-Extended] Genes now admitted (top 30):\\n\"\n",
        "            f\"{rex_sequences_df['paralog_group'].value_counts().head(30).to_string()}\")\n"
      ],
      "metadata": {
        "id": "gQvKSc7iU1b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "woXrgPBgU0mw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 91 â€“ XY Substitution Framework (config + utilities)\n",
        "\n",
        "**What this adds**\n",
        "\n",
        "- Extracts **X** and **Y** residues only from Gâ€“Xâ€“Y frames (anchored on **G**).\n",
        "- Builds MRCA-anchored substitution **counts** and per-AA **frequencies**.\n",
        "- Small-n protections and graceful fallbacks if MRCA strings are missing.\n",
        "\n",
        "**Assumptions / Inputs**\n",
        "\n",
        "- `SOURCE_DF` exists and includes at minimum:\n",
        "  - `paralog_group` (gene key), `exon_num_in_chain`, `exon_peptide`, `species`\n",
        "  - Optional: `order` (or similar) for clade; optional `mrca_exon_peptide`.\n",
        "- MRCA per (gene, exon) may be provided in `MRCA_EXON_MAP[(gene, exon)]`.\n",
        "  Otherwise a **majority-vote consensus** is computed from available sequences.\n",
        "\n",
        "**Outputs**\n",
        "\n",
        "- `xy_subst_by_exon_df`: MRCA-anchored 20Ã—20 counts per (gene, exon, pos_class, clade).\n",
        "- `xy_subst_counts_df`: Aggregated counts per (gene, pos_class, clade).\n",
        "- `xy_position_diversity_df`: Per-exon AA diversity by position (for anchor scans).\n",
        "- Utility funcs reused by later cells."
      ],
      "metadata": {
        "id": "71b5BJ7frPoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 91 =====\n",
        "# XY Substitution Framework (config + utilities)\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "\n",
        "# --- Logging shim (reuse your logger/rex_log if present) ---\n",
        "def _log(msg: str):\n",
        "    if 'rex_log' in globals():\n",
        "        try:\n",
        "            rex_log(msg)\n",
        "            return\n",
        "        except Exception:\n",
        "            pass\n",
        "    print(msg)\n",
        "\n",
        "# --- Parameters (do NOT clobber if already defined upstream) ---\n",
        "XY_MIN_GXY_TRIPLETS = int(globals().get(\"XY_MIN_GXY_TRIPLETS\", 5))       # min triplets per exon to include\n",
        "XY_MIN_SPECIES_PER_EXON = int(globals().get(\"XY_MIN_SPECIES_PER_EXON\", 5))\n",
        "XY_ALPHA = float(globals().get(\"XY_ALPHA\", 0.01))                         # sig. threshold\n",
        "XY_EFFECT_CV = float(globals().get(\"XY_EFFECT_CV\", 0.15))                 # CramÃ©r's V small/medium boundary\n",
        "XY_MIN_CELL = int(globals().get(\"XY_MIN_CELL\", 1))                        # smoothing pseudo-counts\n",
        "XY_REGEX_MIN_FREQ = float(globals().get(\"XY_REGEX_MIN_FREQ\", 0.05))       # AA must reach this freq to be included\n",
        "XY_REGEX_IMPORTANCE_FREQ = float(globals().get(\"XY_REGEX_IMPORTANCE_FREQ\", 0.02))  # always include K/R, D/E, P if >= this\n",
        "XY_CLADES_FROM = globals().get(\"XY_CLADES_FROM\", \"order\")                 # column giving clade; fallback to 'pan'\n",
        "XY_AA_ORDER = list(\"ACDEFGHIKLMNPQRSTVWY\")                                # 20 AA (no O/U); 'P' crucial here\n",
        "AA_INDEX = {aa:i for i,aa in enumerate(XY_AA_ORDER)}\n",
        "\n",
        "# --- Safe import for tests (chi2) with fallback G-test ---\n",
        "try:\n",
        "    from scipy.stats import chi2_contingency\n",
        "    _HAVE_SCIPY = True\n",
        "except Exception:\n",
        "    _HAVE_SCIPY = False\n",
        "\n",
        "def _g_test(obs: np.ndarray) -> Tuple[float, float]:\n",
        "    \"\"\"Likelihood-ratio (G) test for independence across rows.\n",
        "    Returns (stat, p). Assumes obs shape (r, c).\"\"\"\n",
        "    r, c = obs.shape\n",
        "    row_sums = obs.sum(axis=1, keepdims=True)\n",
        "    col_sums = obs.sum(axis=0, keepdims=True)\n",
        "    total = obs.sum()\n",
        "    expected = row_sums @ col_sums / max(total, 1)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        valid = (obs > 0) & (expected > 0)\n",
        "        G = 2.0 * np.sum(obs[valid] * np.log(obs[valid] / expected[valid]))\n",
        "    df = (r - 1) * (c - 1)\n",
        "    # simple chi2 approx for p-value\n",
        "    from math import exp\n",
        "    # Fallback: upper-tail via incomplete gamma approx (rough)\n",
        "    # If scipy missing, we approximate p crudely (conservative):\n",
        "    # p â‰ˆ exp(-G/2) * sum_{k=0}^{df-1} (G/2)^k / k!\n",
        "    # For df up to ~19 this is ok-ish; large df users likely have SciPy installed.\n",
        "    def _pois_sf(k, lam):\n",
        "        # P(N>=k) for Poisson(lam)\n",
        "        s = 0.0\n",
        "        for i in range(k):\n",
        "            s += (lam**i)/math.factorial(i)\n",
        "        return 1.0 - math.exp(-lam)*s\n",
        "    p = _pois_sf(df, G/2.0)  # crude\n",
        "    return float(G), float(p)\n",
        "\n",
        "def _chi2_independence(obs: np.ndarray) -> Tuple[float, float]:\n",
        "    \"\"\"Return (stat, p) using SciPy chi2 if available, else G-test fallback.\"\"\"\n",
        "    if _HAVE_SCIPY:\n",
        "        stat, p, _, _ = chi2_contingency(obs, correction=False)\n",
        "        return float(stat), float(p)\n",
        "    return _g_test(obs)\n",
        "\n",
        "def _cramers_v(obs: np.ndarray) -> float:\n",
        "    \"\"\"CramÃ©r's V effect size for contingency table.\"\"\"\n",
        "    stat, _ = _chi2_independence(obs)\n",
        "    n = obs.sum()\n",
        "    if n <= 0:\n",
        "        return 0.0\n",
        "    r, c = obs.shape\n",
        "    return math.sqrt(stat / (n * (min(r, c) - 1 + 1e-12)))\n",
        "\n",
        "# --- Core helpers ---\n",
        "_GXY_TRIPLET = re.compile(r\"G..\")\n",
        "\n",
        "def extract_xy_triplets(peptide: str) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"Return lists of Xs and Ys from all Gâ€“Xâ€“Y frames anchored at 'G'.\"\"\"\n",
        "    xs, ys = [], []\n",
        "    if not peptide:\n",
        "        return xs, ys\n",
        "    s = peptide\n",
        "    for i in range(len(s) - 2):\n",
        "        if s[i] == \"G\":\n",
        "            x, y = s[i+1], s[i+2]\n",
        "            if x.isalpha() and y.isalpha():\n",
        "                xs.append(x)\n",
        "                ys.append(y)\n",
        "    return xs, ys\n",
        "\n",
        "def consensus_from_xy_lists(xs_list: List[str], ys_list: List[str]) -> Tuple[str, str]:\n",
        "    \"\"\"Heuristic MRCA-like consensus: majority residue for each X and Y position\n",
        "    when multiple aligned exons (same length) are available.\"\"\"\n",
        "    # If not aligned lengths, fallback to flat majority\n",
        "    if not xs_list or not ys_list:\n",
        "        return \"\", \"\"\n",
        "    # In collagen, GXY triplets are in register; we form a single \"modal\" token.\n",
        "    x_counts = Counter(xs_list)\n",
        "    y_counts = Counter(ys_list)\n",
        "    return (max(x_counts, key=x_counts.get), max(y_counts, key=y_counts.get))\n",
        "\n",
        "def xy_subst_counts_for_exon(exon_peptides: List[str],\n",
        "                             mrca_peptide: Optional[str] = None\n",
        "                             ) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Build 20x20 substitution matrices for X and Y vs MRCA.\n",
        "    Keys: {'X','Y'} -> np.ndarray[20,20] where rows = ancestral AA, cols = observed AA.\"\"\"\n",
        "    mats = {\"X\": np.zeros((20, 20), dtype=int),\n",
        "            \"Y\": np.zeros((20, 20), dtype=int)}\n",
        "    # If MRCA is provided as full peptide, we derive its X/Y lists from it.\n",
        "    mrca_xs, mrca_ys = extract_xy_triplets(mrca_peptide) if mrca_peptide else ([], [])\n",
        "    for pep in exon_peptides:\n",
        "        xs, ys = extract_xy_triplets(pep)\n",
        "        if len(xs) < XY_MIN_GXY_TRIPLETS or len(ys) < XY_MIN_GXY_TRIPLETS:\n",
        "            continue\n",
        "        # If MRCA not given or length mismatch, fallback to flat MRCA by modal residue\n",
        "        if not mrca_xs or len(mrca_xs) != len(xs):\n",
        "            # per-exon flat MRCA token (single AA baseline)\n",
        "            mx, my = consensus_from_xy_lists(xs, ys)\n",
        "            for x in xs:\n",
        "                if x in AA_INDEX and mx in AA_INDEX:\n",
        "                    mats[\"X\"][AA_INDEX[mx], AA_INDEX[x]] += 1\n",
        "            for y in ys:\n",
        "                if y in AA_INDEX and my in AA_INDEX:\n",
        "                    mats[\"Y\"][AA_INDEX[my], AA_INDEX[y]] += 1\n",
        "        else:\n",
        "            k = min(len(xs), len(mrca_xs))\n",
        "            for i in range(k):\n",
        "                a, o = mrca_xs[i], xs[i]\n",
        "                if a in AA_INDEX and o in AA_INDEX:\n",
        "                    mats[\"X\"][AA_INDEX[a], AA_INDEX[o]] += 1\n",
        "            k = min(len(ys), len(mrca_ys))\n",
        "            for i in range(k):\n",
        "                a, o = mrca_ys[i], ys[i]\n",
        "                if a in AA_INDEX and o in AA_INDEX:\n",
        "                    mats[\"Y\"][AA_INDEX[a], AA_INDEX[o]] += 1\n",
        "    return mats\n",
        "\n",
        "def _safe_get_clade(row: pd.Series) -> str:\n",
        "    return str(row.get(XY_CLADES_FROM, \"pan\")) if XY_CLADES_FROM in row else \"pan\""
      ],
      "metadata": {
        "id": "5Dj6nRGfrY6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 92 â€“ Build MRCA-anchored X/Y substitution matrices\n",
        "\n",
        "**What this does**\n",
        "\n",
        "- Groups `SOURCE_DF` by `(paralog_group, exon_num_in_chain, clade)` and\n",
        "  builds **20Ã—20** substitution matrices for **X** and **Y** positions.\n",
        "- Aggregates to per-gene/per-clade matrices (`xy_subst_counts_df`).\n",
        "- Emits per-position **AA diversity** for plotting anchors.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "- Uses `MRCA_EXON_MAP[(gene, exon)]` if present; falls back to modal MRCA.\n",
        "- Skips under-powered cells by `XY_MIN_SPECIES_PER_EXON` and `XY_MIN_GXY_TRIPLETS`."
      ],
      "metadata": {
        "id": "JHl4UsbLrqk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 92 =====\n",
        "# Build MRCA-anchored X/Y substitution matrices (per exon â†’ aggregated)\n",
        "\n",
        "xy_subst_by_exon_records = []\n",
        "xy_position_diversity_records = []\n",
        "\n",
        "if 'SOURCE_DF' in globals() and isinstance(SOURCE_DF, pd.DataFrame) and not SOURCE_DF.empty:\n",
        "    df = SOURCE_DF.copy()\n",
        "    if XY_CLADES_FROM not in df.columns:\n",
        "        df[XY_CLADES_FROM] = \"pan\"\n",
        "    df['exon_num_in_chain'] = df['exon_num_in_chain'].astype(int, errors='ignore')\n",
        "\n",
        "    # Prefer provided MRCA map if available\n",
        "    MRCA_EXON_MAP = globals().get(\"MRCA_EXON_MAP\", {})\n",
        "\n",
        "    for (gene, exon, clade), g in df.groupby(['paralog_group', 'exon_num_in_chain', XY_CLADES_FROM]):\n",
        "        peps = [str(x) for x in g['exon_peptide'].dropna().astype(str).tolist()]\n",
        "        if len(peps) < XY_MIN_SPECIES_PER_EXON:\n",
        "            continue\n",
        "\n",
        "        # Optional MRCA peptide\n",
        "        mrca = MRCA_EXON_MAP.get((gene, int(exon)))\n",
        "        mats = xy_subst_counts_for_exon(peps, mrca)\n",
        "\n",
        "        # Save 20x20 per-exon matrices\n",
        "        for pos_class in ('X', 'Y'):\n",
        "            mat = mats[pos_class]\n",
        "            if mat.sum() == 0:\n",
        "                continue\n",
        "            xy_subst_by_exon_records.append({\n",
        "                \"gene\": gene, \"exon\": int(exon), \"clade\": clade,\n",
        "                \"pos_class\": pos_class, \"matrix\": mat\n",
        "            })\n",
        "\n",
        "        # Position-wise AA diversity (counts of unique AA at each XY position)\n",
        "        # Use the longest in-register set to avoid ragged artifacts\n",
        "        xs_all, ys_all = [], []\n",
        "        for p in peps:\n",
        "            xs, ys = extract_xy_triplets(p)\n",
        "            xs_all.append(xs); ys_all.append(ys)\n",
        "        max_len = max((len(x) for x in xs_all), default=0)\n",
        "        for i in range(max_len):\n",
        "            obs_x = set(x[i] for x in xs_all if len(x) > i)\n",
        "            obs_y = set(y[i] for y in ys_all if len(y) > i)\n",
        "            if obs_x:\n",
        "                xy_position_diversity_records.append(\n",
        "                    {\"gene\":gene, \"exon\":int(exon), \"clade\":clade, \"pos_class\":\"X\",\n",
        "                     \"position_ix\": i, \"aa_diversity\": len(obs_x)}\n",
        "                )\n",
        "            if obs_y:\n",
        "                xy_position_diversity_records.append(\n",
        "                    {\"gene\":gene, \"exon\":int(exon), \"clade\":clade, \"pos_class\":\"Y\",\n",
        "                     \"position_ix\": i, \"aa_diversity\": len(obs_y)}\n",
        "                )\n",
        "\n",
        "# Materialize DataFrames\n",
        "xy_subst_by_exon_df = pd.DataFrame(xy_subst_by_exon_records)\n",
        "xy_position_diversity_df = pd.DataFrame(xy_position_diversity_records)\n",
        "\n",
        "# Aggregate per-gene/clade/pos_class into 20x20 matrices (sum over exons)\n",
        "xy_subst_matrices: Dict[Tuple[str,str,str], np.ndarray] = {}\n",
        "for _, r in xy_subst_by_exon_df.iterrows():\n",
        "    key = (r[\"gene\"], r[\"pos_class\"], r[\"clade\"])\n",
        "    if key not in xy_subst_matrices:\n",
        "        xy_subst_matrices[key] = np.zeros((20,20), dtype=int)\n",
        "    xy_subst_matrices[key] += r[\"matrix\"]\n",
        "\n",
        "# Tidy count table (long) for downstream stats/plots\n",
        "xy_subst_rows = []\n",
        "for (gene, pos_class, clade), M in xy_subst_matrices.items():\n",
        "    for ai, a in enumerate(XY_AA_ORDER):\n",
        "        for oi, o in enumerate(XY_AA_ORDER):\n",
        "            c = int(M[ai, oi])\n",
        "            if c > 0:\n",
        "                xy_subst_rows.append(\n",
        "                    {\"gene\":gene, \"pos_class\":pos_class, \"clade\":clade,\n",
        "                     \"anc\":a, \"obs\":o, \"count\":c}\n",
        "                )\n",
        "xy_subst_counts_df = pd.DataFrame(xy_subst_rows)\n",
        "\n",
        "_log(f\"[XY] Built matrices: {len(xy_subst_matrices)} keys; \"\n",
        "     f\"{len(xy_subst_by_exon_df)} exon-level records; \"\n",
        "     f\"{len(xy_position_diversity_df)} diversity rows.\")"
      ],
      "metadata": {
        "id": "wJAzQ_M9rziC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xy_subst_counts_df.head(20)\n"
      ],
      "metadata": {
        "id": "mN8WkSkWMCS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 93 â€“ Merge decisions (between genes; between X vs Y) & Regex tokens\n",
        "\n",
        "**Goal**  \n",
        "Combine matrices **unless** there is strong evidence of heterogeneity.\n",
        "\n",
        "**Tests**\n",
        "\n",
        "- Independence tests across **genes** (per clade, per pos_class); and across **X vs Y**.\n",
        "- Criteria to **keep separate**:\n",
        "  - p < `XY_ALPHA` **and** CramÃ©râ€™s V â‰¥ `XY_EFFECT_CV`.\n",
        "- Otherwise we **merge** (sum counts).\n",
        "\n",
        "**Regex token spec (for anchors/recovery)**\n",
        "\n",
        "- From merged (or per-gene) frequencies, build position-class tokens:\n",
        "  - Include AA with freq â‰¥ `XY_REGEX_MIN_FREQ`.\n",
        "  - Always include **K/R**, **D/E**, **P** if they reach `XY_REGEX_IMPORTANCE_FREQ`.\n",
        "  - Guarantee at least one AA (fallback to top-N).\n",
        "- Output:\n",
        "  - `xy_merge_decisions_df` (audit)\n",
        "  - `xy_subst_merged_spec`: {(scope, pos_class, clade) â†’ {\"matrix\":..., \"token\":\"[...]\"}}"
      ],
      "metadata": {
        "id": "p2_jLG3Qr3VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 93 =====\n",
        "# Merge decisions & regex token spec\n",
        "\n",
        "def _stack_by(group_keys, pos_class, clade) -> np.ndarray:\n",
        "    \"\"\"Stack matrices by group (e.g., by gene) to test heterogeneity.\"\"\"\n",
        "    mats = []\n",
        "    labels = []\n",
        "    for key, M in xy_subst_matrices.items():\n",
        "        g, pc, cl = key\n",
        "        if pc != pos_class or cl != clade:\n",
        "            continue\n",
        "        if isinstance(group_keys, list) and g not in group_keys:\n",
        "            continue\n",
        "        mats.append(M.sum(axis=0))  # collapse ancestral rows: focus on obs mix\n",
        "        labels.append(g)\n",
        "    if not mats:\n",
        "        return np.zeros((0, 20), dtype=int), labels\n",
        "    return np.stack(mats, axis=0), labels\n",
        "\n",
        "def _freqs_from_matrix(M: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Return observed AA frequency vector (20,) from 20x20 matrix.\"\"\"\n",
        "    v = M.sum(axis=0).astype(float)\n",
        "    s = v.sum()\n",
        "    return (v / s) if s > 0 else np.zeros_like(v)\n",
        "\n",
        "def _token_from_freqs(freq: np.ndarray) -> str:\n",
        "    \"\"\"Build degenerate class token '[...]' from freq with constraints.\"\"\"\n",
        "    include = set()\n",
        "    # primary filter\n",
        "    for aa, p in zip(XY_AA_ORDER, freq):\n",
        "        if p >= XY_REGEX_MIN_FREQ:\n",
        "            include.add(aa)\n",
        "    # privileged residues (K/R; D/E; P)\n",
        "    privileged = {\"K\",\"R\",\"D\",\"E\",\"P\"}\n",
        "    for aa in privileged:\n",
        "        idx = AA_INDEX[aa]\n",
        "        if freq[idx] >= XY_REGEX_IMPORTANCE_FREQ:\n",
        "            include.add(aa)\n",
        "    # ensure at least one AA\n",
        "    if not include:\n",
        "        # take top 3 by freq\n",
        "        top_idx = np.argsort(freq)[::-1][:3]\n",
        "        include = {XY_AA_ORDER[i] for i in top_idx}\n",
        "    return \"[\" + \"\".join(sorted(include)) + \"]\"\n",
        "\n",
        "merge_rows = []\n",
        "xy_subst_merged_spec = {}   # keys: ('merged' or gene, pos_class, clade) -> dict\n",
        "\n",
        "# 1) Decide per clade whether to merge across genes (for each pos_class)\n",
        "clades = sorted(set(k[2] for k in xy_subst_matrices.keys())) or [\"pan\"]\n",
        "genes = sorted(set(k[0] for k in xy_subst_matrices.keys()))\n",
        "\n",
        "for clade in clades:\n",
        "    for pos_class in (\"X\",\"Y\"):\n",
        "        table, labels = _stack_by(genes, pos_class, clade)\n",
        "        # Underpowered: if fewer than 2 groups or small totals, auto-merge\n",
        "        if table.shape[0] < 2 or table.sum() < 100:\n",
        "            decision = \"merge\"\n",
        "            stat, p, cv = 0.0, 1.0, 0.0\n",
        "        else:\n",
        "            stat, p = _chi2_independence(table)\n",
        "            cv = _cramers_v(table)\n",
        "            decision = \"separate\" if (p < XY_ALPHA and cv >= XY_EFFECT_CV) else \"merge\"\n",
        "\n",
        "        merge_rows.append({\"scope\":\"genes\", \"clade\":clade, \"pos_class\":pos_class,\n",
        "                           \"stat\":stat, \"p\":p, \"cramers_v\":cv, \"decision\":decision})\n",
        "\n",
        "        if decision == \"merge\":\n",
        "            # build merged matrix (sum all genes)\n",
        "            M = np.zeros((20,20), dtype=int)\n",
        "            for g in genes:\n",
        "                M += xy_subst_matrices.get((g, pos_class, clade), 0)\n",
        "            freq = _freqs_from_matrix(M)\n",
        "            token = _token_from_freqs(freq)\n",
        "            xy_subst_merged_spec[(\"merged\", pos_class, clade)] = {\"matrix\": M, \"freq\": freq, \"token\": token}\n",
        "        else:\n",
        "            # keep per-gene specs\n",
        "            for g in genes:\n",
        "                M = xy_subst_matrices.get((g, pos_class, clade))\n",
        "                if M is None or M.sum() == 0:\n",
        "                    continue\n",
        "                freq = _freqs_from_matrix(M)\n",
        "                token = _token_from_freqs(freq)\n",
        "                xy_subst_merged_spec[(g, pos_class, clade)] = {\"matrix\": M, \"freq\": freq, \"token\": token}\n",
        "\n",
        "# 2) Decide whether X and Y can be combined (per clade, and per-scope)\n",
        "xy_merge_decisions_rows = []\n",
        "for clade in clades:\n",
        "    scopes = set(s for (s,_,c) in xy_subst_merged_spec.keys() if c == clade)\n",
        "    for scope in scopes:\n",
        "        Mx = xy_subst_merged_spec.get((scope, \"X\", clade), {}).get(\"matrix\")\n",
        "        My = xy_subst_merged_spec.get((scope, \"Y\", clade), {}).get(\"matrix\")\n",
        "        if Mx is None or My is None:\n",
        "            continue\n",
        "        fx, fy = _freqs_from_matrix(Mx), _freqs_from_matrix(My)\n",
        "        table = np.stack([fx, fy], axis=0)\n",
        "        # transform to pseudo-counts for test stability\n",
        "        counts = (table * max(Mx.sum()+My.sum(), 1)).astype(int)\n",
        "        stat, p = _chi2_independence(counts)\n",
        "        cv = _cramers_v(counts)\n",
        "        xy_merge_decisions_rows.append({\n",
        "            \"scope\": scope, \"clade\": clade, \"compare\": \"X_vs_Y\",\n",
        "            \"stat\": stat, \"p\": p, \"cramers_v\": cv,\n",
        "            \"decision\": \"separate\" if (p < XY_ALPHA and cv >= XY_EFFECT_CV) else \"merge\"\n",
        "        })\n",
        "\n",
        "xy_merge_decisions_df = pd.DataFrame(merge_rows + xy_merge_decisions_rows)\n",
        "\n",
        "# Convenience: build unified tokens preferring merges when permitted\n",
        "xy_unified_tokens = {}\n",
        "for (scope, pos_class, clade), obj in xy_subst_merged_spec.items():\n",
        "    # Check whether we can unify X&Y for this scope/clade\n",
        "    row = xy_merge_decisions_df[\n",
        "        (xy_merge_decisions_df['scope']==scope) &\n",
        "        (xy_merge_decisions_df['clade']==clade) &\n",
        "        (xy_merge_decisions_df['compare']==\"X_vs_Y\")\n",
        "    ]\n",
        "    separate_xy = (not row.empty) and (row.iloc[0]['decision'] == 'separate')\n",
        "    if separate_xy:\n",
        "        xy_unified_tokens[(scope, pos_class, clade)] = obj[\"token\"]\n",
        "    else:\n",
        "        # Merge X and Y tokens by union\n",
        "        tok_x = xy_subst_merged_spec.get((scope, \"X\", clade), {}).get(\"token\", \"\")\n",
        "        tok_y = xy_subst_merged_spec.get((scope, \"Y\", clade), {}).get(\"token\", \"\")\n",
        "        merged = \"[\" + \"\".join(sorted(set(tok_x.strip(\"[]\")) | set(tok_y.strip(\"[]\")))) + \"]\"\n",
        "        xy_unified_tokens[(scope, \"XY\", clade)] = merged\n",
        "\n",
        "_log(\"[XY] Merge decisions ready; tokens constructed.\")"
      ],
      "metadata": {
        "id": "o9LkxRRpsJpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 93a â€“ Anchor Catalog Sanity Check (must be `loaded=True`)\n",
        "Confirms that Cell 93 has a **loaded** catalog with exon ordering context (not\n",
        "a tiny, in-memory fallback). Prints shape and gene coverage.\n"
      ],
      "metadata": {
        "id": "RJgZmyaLSDCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 93a =====\n",
        "# Inspect anchor catalog globals set by Cell 93.\n",
        "\n",
        "def _catshape(df):\n",
        "    try:\n",
        "        return df.shape\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "if 'ANCHOR_CATALOG_DF' not in globals():\n",
        "    logger.warning(\"[Anchors] ANCHOR_CATALOG_DF not found (Cell 93 may not have run).\")\n",
        "else:\n",
        "    shp = _catshape(ANCHOR_CATALOG_DF)\n",
        "    logger.info(f\"[Anchors] Catalog present: shape={shp}\")\n",
        "    cols = [c for c in (\"gene\",\"chain\",\"exon_index\",\"anchor_regex\",\"loaded\") if c in ANCHOR_CATALOG_DF.columns]\n",
        "    logger.info(f\"[Anchors] Columns detected: {cols}\")\n",
        "    if \"loaded\" in ANCHOR_CATALOG_DF.columns:\n",
        "        logger.info(f\"[Anchors] loaded flag values: {ANCHOR_CATALOG_DF['loaded'].unique()[:5]}\")\n",
        "    if \"gene\" in ANCHOR_CATALOG_DF.columns:\n",
        "        logger.info(f\"[Anchors] Genes in catalog (top 10):\\n\"\n",
        "                    f\"{ANCHOR_CATALOG_DF['gene'].value_counts().head(10).to_string()}\")\n",
        "\n",
        "# Optional: require minimal width for recovery\n",
        "if ('ANCHOR_CATALOG_DF' in globals()) and (ANCHOR_CATALOG_DF.shape[0] < 1000):\n",
        "    logger.warning(\"[Anchors] Catalog seems small; recovery may underperform (hits but no chains).\")\n"
      ],
      "metadata": {
        "id": "K75X5OCFSJO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 93a â€“ Anchor Catalog (robust load/build + safe fallbacks)\n",
        "\n",
        "Build or load `anchor_catalog_df` for Recovery.\n",
        "\n",
        "**Strategy**\n",
        "1) If a non-empty `anchor_catalog_df` already exists â†’ keep it.\n",
        "2) Else try to **load** `RUN_DIR/anchor_catalog_df.tsv`.\n",
        "3) Else **build** from `SOURCE_DF` (entropyâ†“, lengthâ†‘; optional gden if available).\n",
        "4) If strict gates yield nothing, **relax** them and still pick top-K per gene.\n",
        "\n",
        "**Outputs**\n",
        "- `anchor_catalog_df` with columns: `gene, exon, median_len, entropy, gden, anchor_score`.\n",
        "- Saved to `RUN_DIR/anchor_catalog_df.tsv` for reuse.\n",
        "\n",
        "**Notes**\n",
        "- Uses `xy_position_diversity_df` as an optional proxy for `gden` when matcher signal isnâ€™t available.\n",
        "- Respects your globals: `REC_MIN_ANCHOR_LEN`, `REC_MIN_ANCHOR_GDEN`, `REC_K_TOP_ANCHORS`.\n"
      ],
      "metadata": {
        "id": "MDdm_DH92mTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 93a =====\n",
        "# Anchor Catalog (robust): load from disk or build with safe fallbacks\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Params / defaults (non-destructive) ----\n",
        "REC_MIN_ANCHOR_LEN   = int(globals().get(\"REC_MIN_ANCHOR_LEN\", 18))\n",
        "REC_MIN_ANCHOR_GDEN  = float(globals().get(\"REC_MIN_ANCHOR_GDEN\", 0.20))\n",
        "REC_K_TOP_ANCHORS    = int(globals().get(\"REC_K_TOP_ANCHORS\", 6))\n",
        "RUN_DIR = Path(globals().get(\"RUN_DIR\", \".\"))\n",
        "\n",
        "def _char_entropy(strings):\n",
        "    if not strings:\n",
        "        return 0.0\n",
        "    s = \"\".join(strings)\n",
        "    if not s:\n",
        "        return 0.0\n",
        "    from collections import Counter\n",
        "    cnt = Counter(s)\n",
        "    total = sum(cnt.values())\n",
        "    ps = [c/total for c in cnt.values()]\n",
        "    return -sum(p*math.log(p+1e-12) for p in ps)\n",
        "\n",
        "def _optional_gden_proxy(gene, exon) -> float:\n",
        "    \"\"\"\n",
        "    Prefer matcher/library conservation if available; else derive a proxy from\n",
        "    XY position diversity (lower diversity => higher gden).\n",
        "    \"\"\"\n",
        "    # 1) Try library/matcher conservation estimate\n",
        "    try:\n",
        "        if 'RegExTractorMatcher' in globals() and 'orthology_aware_library' in globals():\n",
        "            # reuse internal heuristic if exposed (best-effort)\n",
        "            lib = orthology_aware_library\n",
        "            entry = lib.entries.get((gene, int(exon), \"pan\")) if getattr(lib, \"entries\", None) else None\n",
        "            if entry and hasattr(RegExTractorMatcher, \"_pattern_conservation\"):\n",
        "                # Use highest-tier pattern for stability\n",
        "                patt = entry.tiers[0].pattern if entry.tiers else None\n",
        "                if patt is not None:\n",
        "                    return float(RegExTractorMatcher._pattern_conservation(patt))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Fallback: use XY diversity (needs xy_position_diversity_df)\n",
        "    if 'xy_position_diversity_df' in globals() and isinstance(xy_position_diversity_df, pd.DataFrame):\n",
        "        sub = xy_position_diversity_df\n",
        "        mask = (sub[\"gene\"]==gene) & (sub[\"exon\"].astype(int)==int(exon))\n",
        "        if mask.any():\n",
        "            # Normalize mean diversity to [0,1] and invert\n",
        "            vals = sub.loc[mask, \"aa_diversity\"].astype(float)\n",
        "            if not vals.empty:\n",
        "                m = float(vals.mean())\n",
        "                # typical AA diversity range ~1..6; scale conservatively\n",
        "                gden = 1.0 - min(m/6.0, 1.0)\n",
        "                return gden\n",
        "    # 3) Last resort\n",
        "    return 0.0\n",
        "\n",
        "def _build_anchor_catalog_from_source() -> pd.DataFrame:\n",
        "    rows = []\n",
        "    if 'SOURCE_DF' not in globals() or not isinstance(SOURCE_DF, pd.DataFrame) or SOURCE_DF.empty:\n",
        "        return pd.DataFrame(columns=[\"gene\",\"exon\",\"median_len\",\"entropy\",\"gden\",\"anchor_score\"])\n",
        "    df = SOURCE_DF.copy()\n",
        "    if \"exon_num_in_chain\" not in df.columns or \"exon_peptide\" not in df.columns or \"paralog_group\" not in df.columns:\n",
        "        return pd.DataFrame(columns=[\"gene\",\"exon\",\"median_len\",\"entropy\",\"gden\",\"anchor_score\"])\n",
        "\n",
        "    df[\"exon_num_in_chain\"] = df[\"exon_num_in_chain\"].astype(int, errors=\"ignore\")\n",
        "\n",
        "    for (gene, exon), g in df.groupby([\"paralog_group\",\"exon_num_in_chain\"]):\n",
        "        peps = g[\"exon_peptide\"].dropna().astype(str).tolist()\n",
        "        if not peps:\n",
        "            continue\n",
        "        med_len = float(np.median([len(p) for p in peps]))\n",
        "        ent = _char_entropy(peps)\n",
        "        gden = _optional_gden_proxy(str(gene), int(exon))\n",
        "        rows.append({\"gene\":str(gene), \"exon\":int(exon),\n",
        "                     \"median_len\":med_len, \"entropy\":ent, \"gden\":gden})\n",
        "    stat_df = pd.DataFrame(rows)\n",
        "    if stat_df.empty:\n",
        "        return pd.DataFrame(columns=[\"gene\",\"exon\",\"median_len\",\"entropy\",\"gden\",\"anchor_score\"])\n",
        "\n",
        "    # rank-based normalisation (robust)\n",
        "    stat_df[\"len_z\"] = stat_df[\"median_len\"].rank(pct=True)\n",
        "    stat_df[\"ent_z\"] = 1.0 - stat_df[\"entropy\"].rank(pct=True)  # lower entropy better\n",
        "    # If gden is all zeros, fill with median to avoid nuking score\n",
        "    med_g = stat_df[\"gden\"].replace({np.nan:0.0})\n",
        "    fill_g = med_g.median() if not med_g.empty else 0.0\n",
        "    stat_df[\"gden_f\"] = stat_df[\"gden\"].replace({np.nan:fill_g})\n",
        "    stat_df[\"gden_z\"] = stat_df[\"gden_f\"].rank(pct=True)\n",
        "\n",
        "    stat_df[\"anchor_score\"] = 0.4*stat_df[\"gden_z\"] + 0.4*stat_df[\"ent_z\"] + 0.2*stat_df[\"len_z\"]\n",
        "\n",
        "    # Strict gates first\n",
        "    stat_df[\"ok_len\"] = stat_df[\"median_len\"] >= REC_MIN_ANCHOR_LEN\n",
        "    stat_df[\"ok_gden\"] = stat_df[\"gden_f\"] >= REC_MIN_ANCHOR_GDEN\n",
        "\n",
        "    strict = (\n",
        "        stat_df[stat_df[\"ok_len\"] & stat_df[\"ok_gden\"]]\n",
        "        .sort_values([\"gene\",\"anchor_score\"], ascending=[True, False])\n",
        "        .groupby(\"gene\", as_index=False)\n",
        "        .head(REC_K_TOP_ANCHORS)\n",
        "    )\n",
        "\n",
        "    if not strict.empty:\n",
        "        out = strict[[\"gene\",\"exon\",\"median_len\",\"entropy\",\"gden\",\"anchor_score\"]].reset_index(drop=True)\n",
        "        return out\n",
        "\n",
        "    # Fallback 1: drop gden gate\n",
        "    relaxed = (\n",
        "        stat_df[stat_df[\"ok_len\"]]\n",
        "        .sort_values([\"gene\",\"anchor_score\"], ascending=[True, False])\n",
        "        .groupby(\"gene\", as_index=False)\n",
        "        .head(REC_K_TOP_ANCHORS)\n",
        "    )\n",
        "    if not relaxed.empty:\n",
        "        return relaxed[[\"gene\",\"exon\",\"median_len\",\"entropy\",\"gden\",\"anchor_score\"]].reset_index(drop=True)\n",
        "\n",
        "    # Fallback 2: ignore gates, still pick top-K\n",
        "    topk = (\n",
        "        stat_df.sort_values([\"gene\",\"anchor_score\"], ascending=[True, False])\n",
        "        .groupby(\"gene\", as_index=False)\n",
        "        .head(max(REC_K_TOP_ANCHORS, 3))\n",
        "    )\n",
        "    return topk[[\"gene\",\"exon\",\"median_len\",\"entropy\",\"gden\",\"anchor_score\"]].reset_index(drop=True)\n",
        "\n",
        "# ---- Load or build ----\n",
        "\n",
        "loaded = False\n",
        "if \"anchor_catalog_df\" in globals() and isinstance(anchor_catalog_df, pd.DataFrame) and not anchor_catalog_df.empty:\n",
        "    loaded = True\n",
        "else:\n",
        "    # Try disk\n",
        "    tsv = RUN_DIR / \"anchor_catalog_df.tsv\"\n",
        "    if tsv.exists():\n",
        "        try:\n",
        "            anchor_catalog_df = pd.read_csv(tsv, sep=\"\\t\")\n",
        "            if not anchor_catalog_df.empty:\n",
        "                loaded = True\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "if not loaded:\n",
        "    anchor_catalog_df = _build_anchor_catalog_from_source()\n",
        "\n",
        "# Persist for reuse\n",
        "try:\n",
        "    if isinstance(anchor_catalog_df, pd.DataFrame) and not anchor_catalog_df.empty:\n",
        "        RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        anchor_catalog_df.to_csv(RUN_DIR/\"anchor_catalog_df.tsv\", sep=\"\\t\", index=False)\n",
        "except Exception as e:\n",
        "    if 'rex_log' in globals():\n",
        "        try: rex_log(f\"Anchor catalog save failed: {e}\")\n",
        "        except Exception: pass\n",
        "\n",
        "# Diagnostics\n",
        "n_genes = anchor_catalog_df[\"gene\"].nunique() if not anchor_catalog_df.empty else 0\n",
        "n_rows  = len(anchor_catalog_df) if isinstance(anchor_catalog_df, pd.DataFrame) else 0\n",
        "msg = f\"[RegExTractor] Anchor catalog ready: {n_rows} rows across {n_genes} genes (loaded={loaded}).\"\n",
        "print(msg)\n",
        "if 'rex_log' in globals():\n",
        "    try: rex_log(msg)\n",
        "    except Exception: pass\n"
      ],
      "metadata": {
        "id": "iInclv8o2uwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 94 â€“ Recovery Engine (anchor-guided, species-wise)\n",
        "\n",
        "Uses your existing matcher/library; unchanged names/signatures.  \n",
        "Adds:\n",
        "- Robust gene vote tie-breaks\n",
        "- Naming rule: if Gâ€“Xâ€“Y fraction â‰¥ `REC_GXY_MIN_FRACTION` **and** length within\n",
        "  expected â†’ assign bare gene (e.g., **COL1A1**), else **COL1A1\\_0.xx**.\n",
        "- Optional â€œunified X/Y tokenâ€ fallback tiers for pan-clade anchors.\n",
        "\n",
        "**Inputs:** `anchor_catalog_df` from Cell 93 (your upstream); `orthology_aware_library`, `RegExTractorMatcher`, `SOURCE_DF`.  \n",
        "**Outputs:** `recovery_hits_df`, `recovered_chains_df`."
      ],
      "metadata": {
        "id": "ogUNkbJNsB34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 94 =====\n",
        "# Recovery engine (keeps your function/class names; adds guardrails)\n",
        "\n",
        "from collections import Counter\n",
        "from typing import Tuple\n",
        "\n",
        "# Defaults if not present\n",
        "REC_GXY_MIN_FRACTION = float(globals().get(\"REC_GXY_MIN_FRACTION\", 0.85))\n",
        "REC_MIN_ANCHOR_LEN = int(globals().get(\"REC_MIN_ANCHOR_LEN\", 18))\n",
        "REC_MIN_ANCHOR_GDEN = float(globals().get(\"REC_MIN_ANCHOR_GDEN\", 0.2))\n",
        "REC_K_TOP_ANCHORS = int(globals().get(\"REC_K_TOP_ANCHORS\", 6))\n",
        "rex_chain_min_consecutive = int(globals().get(\"rex_chain_min_consecutive\", 3))\n",
        "\n",
        "def _is_gxy_like(seq: str) -> float:\n",
        "    if not seq:\n",
        "        return 0.0\n",
        "    n = 0\n",
        "    for i in range(len(seq)-2):\n",
        "        if seq[i] == \"G\":\n",
        "            n += 1\n",
        "    return n / max(len(seq)-2, 1)\n",
        "\n",
        "def _name_prediction(paralog_group: str, gxy_fraction: float,\n",
        "                     expected_len: Optional[int] = None,\n",
        "                     observed_len: Optional[int] = None) -> str:\n",
        "    \"\"\"Assign final gene name; if ambiguous, append _0.xx.\"\"\"\n",
        "    gene = paralog_group\n",
        "    ok_len = True\n",
        "    if expected_len and observed_len:\n",
        "        ok_len = (0.85 * expected_len) <= observed_len <= (1.15 * expected_len)\n",
        "    if gxy_fraction >= REC_GXY_MIN_FRACTION and ok_len:\n",
        "        return gene\n",
        "    xx = f\"{gxy_fraction:.2f}\".lstrip(\"0\")\n",
        "    return f\"{gene}_{xx}\"\n",
        "\n",
        "def run_recovery_engine(target_pool: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    if 'orthology_aware_library' not in globals():\n",
        "        _log(\"Recovery: orthology_aware_library not found; run Cell 73 first.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "    if 'anchor_catalog_df' not in globals() or anchor_catalog_df.empty:\n",
        "        _log(\"Recovery: anchor catalog unavailable; run Cell 93.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "    if 'SOURCE_DF' not in globals() or SOURCE_DF.empty:\n",
        "        _log(\"Recovery: SOURCE_DF not found.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    lib = orthology_aware_library\n",
        "    matcher = RegExTractorMatcher(lib)\n",
        "\n",
        "    anchors = {(r[\"gene\"], int(r[\"exon\"])) for _, r in anchor_catalog_df.iterrows()}\n",
        "    if not anchors:\n",
        "        _log(\"Recovery: no anchors available after filters.\")\n",
        "        return pd.DataFrame(), pd.DataFrame()\n",
        "\n",
        "    # expected exon architectures per gene (training)\n",
        "    arch = SOURCE_DF.groupby('paralog_group')['exon_num_in_chain'] \\\n",
        "                    .apply(lambda x: sorted(set(map(int, x)))) \\\n",
        "                    .to_dict()\n",
        "\n",
        "    rec_hits, rec_chains = [], []\n",
        "\n",
        "    for _, row in target_pool.iterrows():\n",
        "        acc = row.get(\"accession\") or row.get(\"Entry\") or row.get(\"accession_id\")\n",
        "        seq = str(row.get(\"sequence\", \"\")) or str(row.get(\"Sequence\", \"\"))\n",
        "        if not acc or not seq:\n",
        "            continue\n",
        "\n",
        "        # 1) Anchor matching\n",
        "        best_anchor_hit = {}\n",
        "        for (gene, exon) in anchors:\n",
        "            entry = lib.entries.get((gene, exon, \"pan\"))\n",
        "            if not entry:\n",
        "                continue\n",
        "            for tier in entry.tiers:\n",
        "                for m in tier.regex.finditer(seq):\n",
        "                    score, gden = matcher._score_hit(m.group(0), tier)\n",
        "                    h = RexHit(acc, gene, exon, \"pan\", tier.tier,\n",
        "                               m.start(), m.end(), m.group(0), gden, score)\n",
        "                    cur = best_anchor_hit.get((gene, exon))\n",
        "                    if (cur is None) or (h.score > cur.score):\n",
        "                        best_anchor_hit[(gene, exon)] = h\n",
        "\n",
        "        if not best_anchor_hit:\n",
        "            # Optional: fallback pass using unified XY tokens as very permissive anchors\n",
        "            # (only if you want a last-ditch scan)\n",
        "            # Skipped by default to avoid FP inflation.\n",
        "            continue\n",
        "\n",
        "        # 2) Gene identification by anchor votes (tie-break: total score)\n",
        "        votes = Counter(g for (g, _), _ in best_anchor_hit.items())\n",
        "        # break ties with accumulated score\n",
        "        candidates = []\n",
        "        for g, v in votes.items():\n",
        "            s = sum(h.score for (gg, _e), h in best_anchor_hit.items() if gg == g)\n",
        "            candidates.append((g, v, s))\n",
        "        candidates.sort(key=lambda t: (t[1], t[2]), reverse=True)\n",
        "        top_gene, top_votes, _ = candidates[0]\n",
        "\n",
        "        # 3) Extend chain for chosen gene\n",
        "        gene_hits = [h for (g, e), h in best_anchor_hit.items() if g == top_gene]\n",
        "        # best per exon\n",
        "        best_per_exon = {}\n",
        "        for h in gene_hits:\n",
        "            cur = best_per_exon.get(h.exon_num_in_chain)\n",
        "            if (cur is None) or (h.score > cur.score):\n",
        "                best_per_exon[h.exon_num_in_chain] = h\n",
        "        if not best_per_exon:\n",
        "            continue\n",
        "\n",
        "        seed = max(best_per_exon.values(), key=lambda h: h.score)\n",
        "        chain = rex_walk_chain(seed, best_per_exon, arch.get(top_gene, []))\n",
        "\n",
        "        # 4) Quality gates & naming\n",
        "        span_pep = seq[chain.start:chain.end]\n",
        "        gxy_frac = _is_gxy_like(span_pep)\n",
        "        ok_blocks = getattr(chain, \"consecutive_blocks\", 1) >= rex_chain_min_consecutive\n",
        "        if ok_blocks and gxy_frac >= REC_GXY_MIN_FRACTION:\n",
        "            # expected length (heuristic from training)\n",
        "            exons_for_gene = arch.get(top_gene, [])\n",
        "            expected_len = 3 * len(exons_for_gene) * 9  # very rough; adjust if you track exon lengths\n",
        "            name = _name_prediction(top_gene, gxy_frac, expected_len, len(span_pep))\n",
        "            chain_dict = chain.__dict__.copy()\n",
        "            chain_dict.update({\n",
        "                \"predicted_gene\": name,\n",
        "                \"gxy_fraction\": gxy_frac,\n",
        "                \"accession\": acc\n",
        "            })\n",
        "            rec_chains.append(chain_dict)\n",
        "\n",
        "        # record hits (for auditing)\n",
        "        rec_hits.extend(h.__dict__ for h in best_per_exon.values())\n",
        "\n",
        "    return pd.DataFrame(rec_hits), pd.DataFrame(rec_chains)\n",
        "\n",
        "# Run (user may pre-filter target_pool_rex to 5 trial species)\n",
        "recovery_hits_df, recovered_chains_df = run_recovery_engine(\n",
        "    target_pool_rex if 'target_pool_rex' in globals() else pd.DataFrame()\n",
        ")\n",
        "_log(f\"Recovery: {len(recovery_hits_df)} anchor hits; \"\n",
        "     f\"{len(recovered_chains_df)} recovered chains.\")"
      ],
      "metadata": {
        "id": "gYVk-3PwsBPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anchor_catalog_df.head()"
      ],
      "metadata": {
        "id": "xIPRHM5w8T13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recovered_chains_df.head()"
      ],
      "metadata": {
        "id": "6pBCshSN67a1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 95 â€“ Outlier Exons (per clade)\n",
        "\n",
        "Flags exons that **deviate** more than expected given clade variability.\n",
        "\n",
        "**Metrics**\n",
        "\n",
        "- Entropy vs baseline (gene-wide) with z-score\n",
        "- KL divergence to baseline mix\n",
        "- Missingness given coverage\n",
        "\n",
        "**Outputs**: `outlier_exons_df` with `[gene, exon, clade, metric, value, z, status, n_obs]`"
      ],
      "metadata": {
        "id": "lZo2nUOqtDCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 95 =====\n",
        "# Outlier exons by clade (improved; entropy + KL + missingness)\n",
        "\n",
        "def _entropy_of_string_list(strings: List[str]) -> float:\n",
        "    # character-level entropy across concatenated peptides\n",
        "    if not strings:\n",
        "        return 0.0\n",
        "    counts = Counter(\"\".join(strings))\n",
        "    total = sum(counts.values())\n",
        "    ps = [c/total for c in counts.values()]\n",
        "    return -sum(p*math.log(p+1e-12) for p in ps)\n",
        "\n",
        "def _kl(p: np.ndarray, q: np.ndarray) -> float:\n",
        "    # KL(p||q) safe\n",
        "    p = p.astype(float); q = q.astype(float)\n",
        "    p = p / max(p.sum(), 1.0); q = q / max(q.sum(), 1.0)\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        m = (p > 0) & (q > 0)\n",
        "        return float(np.sum(p[m] * np.log((p[m] + 1e-12) / (q[m] + 1e-12))))\n",
        "\n",
        "rows = []\n",
        "if 'SOURCE_DF' in globals() and not SOURCE_DF.empty:\n",
        "    df = SOURCE_DF.copy()\n",
        "    if XY_CLADES_FROM not in df.columns:\n",
        "        df[XY_CLADES_FROM] = \"pan\"\n",
        "    df['exon_num_in_chain'] = df['exon_num_in_chain'].astype(int, errors='ignore')\n",
        "\n",
        "    for (gene, exon), g in df.groupby([\"paralog_group\", \"exon_num_in_chain\"]):\n",
        "        peps_all = g[\"exon_peptide\"].dropna().astype(str).tolist()\n",
        "        base_ent = _entropy_of_string_list(peps_all)\n",
        "        base_freq = np.zeros(20)\n",
        "        xs_all, ys_all = [], []\n",
        "        for p in peps_all:\n",
        "            xs, ys = extract_xy_triplets(p)\n",
        "            for z in xs+ys:\n",
        "                if z in AA_INDEX:\n",
        "                    base_freq[AA_INDEX[z]] += 1\n",
        "\n",
        "        for clade, gc in g.groupby(XY_CLADES_FROM):\n",
        "            peps = gc[\"exon_peptide\"].dropna().astype(str).tolist()\n",
        "            ent = _entropy_of_string_list(peps)\n",
        "            freq = np.zeros(20)\n",
        "            for p in peps:\n",
        "                xs, ys = extract_xy_triplets(p)\n",
        "                for z in xs+ys:\n",
        "                    if z in AA_INDEX:\n",
        "                        freq[AA_INDEX[z]] += 1\n",
        "            kl = _kl(freq, base_freq) if base_freq.sum() > 0 else 0.0\n",
        "            n = len(peps)\n",
        "            # crude z proxy using 2-point std (defensive)\n",
        "            z = (ent - base_ent) / (np.std([base_ent, ent]) + 1e-6)\n",
        "            status = \"ok\"\n",
        "            if n >= max(XY_MIN_SPECIES_PER_EXON, 8) and (z > 3.0 or kl > 0.5):\n",
        "                status = \"high_variability\"\n",
        "            elif n == 0:\n",
        "                status = \"missing\"\n",
        "\n",
        "            rows.append({\"gene\":gene, \"exon\":int(exon), \"clade\":clade,\n",
        "                         \"entropy\":ent, \"baseline_entropy\":base_ent, \"z\":z,\n",
        "                         \"kl\":kl, \"status\":status, \"n_obs\":n})\n",
        "\n",
        "outlier_exons_df = pd.DataFrame(rows)\n",
        "_log(f\"[Outliers] flagged {outlier_exons_df.query('status != \\\"ok\\\"').shape[0]} cases.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZBhw3d_rBcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 96 â€“ Reports & Artifacts\n",
        "\n",
        "- Save:\n",
        "  - `xy_subst_counts_df.tsv`, `xy_subst_matrices.npz`,\n",
        "  - `xy_merge_decisions_df.tsv`, `xy_unified_tokens.tsv`,\n",
        "  - `anchor_catalog_df.tsv`, `recovery_hits_df.tsv`, `recovered_chains_df.tsv`,\n",
        "  - `outlier_exons_df.tsv`\n",
        "- Plot (optional stubs):\n",
        "  - Heatmaps of X/Y substitution (merged vs per-gene)\n",
        "  - Per-exon AA diversity lines to visualise anchor candidates\n",
        "python"
      ],
      "metadata": {
        "id": "6PDY8EBYS7og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Cell 96 =====\n",
        "# Persist artifacts + plotting stubs (matplotlib)\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "RUN_DIR = globals().get(\"RUN_DIR\", Path(\".\"))\n",
        "RUN_DIR = Path(RUN_DIR)\n",
        "\n",
        "def _save_df(df, name):\n",
        "    try:\n",
        "        if df is not None and isinstance(df, pd.DataFrame) and not df.empty:\n",
        "            df.to_csv(RUN_DIR / f\"{name}.tsv\", sep=\"\\t\", index=False)\n",
        "            _log(f\"[Save] {name}.tsv\")\n",
        "    except Exception as e:\n",
        "        _log(f\"[Save] failed {name}: {e}\")\n",
        "\n",
        "try:\n",
        "    # tables\n",
        "    _save_df(xy_subst_counts_df, \"xy_subst_counts_df\")\n",
        "    _save_df(xy_subst_by_exon_df, \"xy_subst_by_exon_df\")\n",
        "    _save_df(xy_merge_decisions_df, \"xy_merge_decisions_df\")\n",
        "    _save_df(xy_position_diversity_df, \"xy_position_diversity_df\")\n",
        "    _save_df(anchor_catalog_df if 'anchor_catalog_df' in globals() else pd.DataFrame(), \"anchor_catalog_df\")\n",
        "    _save_df(recovery_hits_df if 'recovery_hits_df' in globals() else pd.DataFrame(), \"recovery_hits_df\")\n",
        "    _save_df(recovered_chains_df if 'recovered_chains_df' in globals() else pd.DataFrame(), \"recovered_chains_df\")\n",
        "    _save_df(outlier_exons_df if 'outlier_exons_df' in globals() else pd.DataFrame(), \"outlier_exons_df\")\n",
        "\n",
        "    # tokens as a small table\n",
        "    if 'xy_unified_tokens' in globals() and xy_unified_tokens:\n",
        "        tok_rows = [{\"scope\":k[0], \"pos_class\":k[1], \"clade\":k[2], \"token\":v}\n",
        "                    for k, v in xy_unified_tokens.items()]\n",
        "        pd.DataFrame(tok_rows).to_csv(RUN_DIR / \"xy_unified_tokens.tsv\", sep=\"\\t\", index=False)\n",
        "\n",
        "    # matrices bundle\n",
        "    if 'xy_subst_matrices' in globals() and xy_subst_matrices:\n",
        "        np.savez_compressed(RUN_DIR / \"xy_subst_matrices.npz\",\n",
        "                            **{f\"{g}_{pc}_{cl}\": M\n",
        "                               for (g,pc,cl), M in xy_subst_matrices.items()})\n",
        "\n",
        "    _log(\"Recovery artifacts saved.\")\n",
        "except Exception as e:\n",
        "    _log(f\"Could not save recovery artifacts: {e}\")\n",
        "\n",
        "# --- Plotting stubs (optional) ---\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    # Example: diversity plot for a single gene/exon\n",
        "    # (Commented to avoid accidental runtime in batch)\n",
        "    # gex = xy_position_diversity_df.query(\"gene == @some_gene and exon == @some_exon\")\n",
        "    # for pos_class, gg in gex.groupby(\"pos_class\"):\n",
        "    #     plt.figure()\n",
        "    #     plt.plot(gg[\"position_ix\"], gg[\"aa_diversity\"])\n",
        "    #     plt.title(f\"{some_gene} exon {some_exon} {pos_class}-diversity\")\n",
        "    #     plt.xlabel(\"Position in GXY frame\"); plt.ylabel(\"# distinct AA\")\n",
        "    #     plt.show()\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "90ra9UE3y7ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyj2bkBs97JT"
      },
      "source": [
        "# **Part 7: Shannon Entropy Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aar9Az8N9-x7"
      },
      "source": [
        "## Cell 71 â€“ Per-Exon Entropy Calculation and Visualization\n",
        "\n",
        "Calculates the Shannon entropy for each amino acid position within each exon across all sequences of a given gene. It saves the results as a TSV file and generates plots showing the median exon length and entropy for each gene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFUTSYuqEWI1"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 71 =====\n",
        "# Per-exon entropy and plotting\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "entropy_rows = []\n",
        "if 'wide_df' in globals() and not wide_df.empty:\n",
        "    for g, sub in wide_df.groupby('gene_symbol'):\n",
        "        exon_pep_cols = sorted(\n",
        "            [c for c in sub.columns if c.startswith('exon_peptide_')],\n",
        "            key=lambda c: int(re.search(r'_(-?\\d+)$', c).group(1))\n",
        "        )\n",
        "\n",
        "        for col in exon_pep_cols:\n",
        "            peps = sub[col].dropna().tolist()\n",
        "            if not peps: continue\n",
        "\n",
        "            max_len = max((len(p) for p in peps), default=0)\n",
        "            padded = [p.ljust(max_len, '-') for p in peps]\n",
        "            ents = [rex_shannon_entropy([p[i] for p in padded if i < len(p) and p[i] != '-']) for i in range(max_len)]\n",
        "\n",
        "            entropy_rows.append({\n",
        "                'gene_symbol': g,\n",
        "                'exon_col': col,\n",
        "                'median_length': np.median([len(p) for p in peps]),\n",
        "                'entropy': np.median(ents) if ents else 0\n",
        "            })\n",
        "\n",
        "    entropy_df = pd.DataFrame(entropy_rows)\n",
        "    if not entropy_df.empty:\n",
        "        entropy_df.to_csv(ENTROPY_TSV, sep='\\t', index=False)\n",
        "        logger.info(f\"Entropy stats saved to {ENTROPY_TSV.name}\")\n",
        "\n",
        "        # Plotting\n",
        "        for g, sub_df in entropy_df.groupby('gene_symbol'):\n",
        "            sub_df = sub_df.copy()\n",
        "            sub_df['exon_idx'] = sub_df['exon_col'].str.extract(r'_(-?\\d+)$').astype(int)\n",
        "            sub_df = sub_df.sort_values('exon_idx')\n",
        "\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            plt.bar(sub_df['exon_idx'].astype(str), sub_df['median_length'], yerr=sub_df['entropy'], capsize=4)\n",
        "            plt.title(f\"Median Exon Length and Entropy for {g}\")\n",
        "            plt.xlabel(\"Exon Number\")\n",
        "            plt.ylabel(\"Amino Acid Length\")\n",
        "            plt.xticks(rotation=90)\n",
        "            plt.tight_layout()\n",
        "            plot_path = RUN_DIR / f\"entropy_plot_{g}_{RUN_ID}.png\"\n",
        "            plt.savefig(plot_path)\n",
        "            plt.close()\n",
        "            logger.info(f\"Entropy plot for {g} saved to {plot_path.name}\")\n",
        "else:\n",
        "    logger.warning(\"wide_df not available; skipping entropy analysis.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqDO22CY-B6G"
      },
      "source": [
        "# **Part 8: Reproducibility & Manifest**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUkLt73x-DfP"
      },
      "source": [
        "## Cell 99 â€“ Final Manifest Generation\n",
        "\n",
        "This cell concludes the run by generating a final, comprehensive JSON manifest. It records key counts from each major step and computes SHA256 hashes for all primary output files, ensuring a reproducible record of the pipeline's execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2sZW-NQEauT"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 99 =====\n",
        "# Final manifest writer\n",
        "\n",
        "logger.info(\"Generating final run manifest...\")\n",
        "\n",
        "# Gather final counts from key dataframes\n",
        "final_counts = {\n",
        "    \"working_rows\": len(working_df) if 'working_df' in globals() else 0,\n",
        "    \"high_quality_rows\": len(df_high_quality) if 'df_high_quality' in globals() else 0,\n",
        "    \"total_raw_exons\": len(df_raw_exons) if 'df_raw_exons' in globals() else 0,\n",
        "    \"consensus_exons\": len(consensus_tbl) if 'consensus_tbl' in globals() else 0,\n",
        "    \"wide_architectures\": len(wide_df) if 'wide_df' in globals() else 0,\n",
        "    \"rex_rescued_chains\": len(rex_chains_df) if 'rex_chains_df' in globals() else 0,\n",
        "}\n",
        "\n",
        "# List of primary output files for this run\n",
        "output_files_to_hash = [\n",
        "    WORKING_SNAPSHOT, CONSENSUS_LONG_TSV, CONSENSUS_TABLE_TSV,\n",
        "    WIDE_ARCH_TSV, ENTROPY_TSV, ERROR_REPORT_PATH,\n",
        "    EVOLUTION_EVENTS_TSV, RESCUE_HITS_TSV, RESCUE_CHAINS_TSV\n",
        "]\n",
        "output_files_to_hash = [p for p in output_files_to_hash if p.exists()]\n",
        "\n",
        "# Update the manifest with final stats and file hashes\n",
        "write_run_manifest(extra={\"final_counts\": final_counts}, final_files=output_files_to_hash)\n",
        "\n",
        "logger.info(\"âœ… Collagen Exon Mapper pipeline finished successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBbF4-8akGHs"
      },
      "source": [
        "## Cell 100 â€“ Final Run Summary Report\n",
        "\n",
        "This cell provides a high-level summary of the key metrics and outputs from the entire pipeline run. It consolidates the most important counts from each major stage, offering a quick and clear overview of what was accomplished."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn34CBiAkJSu"
      },
      "outputs": [],
      "source": [
        "# ===== Cell 100 =====\n",
        "# Final Run Summary Report\n",
        "\n",
        "logger.info(\"=\"*50)\n",
        "logger.info(\" PIPELINE RUN SUMMARY\")\n",
        "logger.info(\"=\"*50)\n",
        "\n",
        "# --- Part 2: Data Loading & Pre-processing ---\n",
        "total_initial_seqs = len(full_df) if 'full_df' in globals() else 0\n",
        "working_set_seqs = len(working_df) if 'working_df' in globals() else 0\n",
        "logger.info(f\"[Part 2] Initial Data Loading:\")\n",
        "logger.info(f\"  - Total sequences in master dataset: {total_initial_seqs}\")\n",
        "logger.info(f\"  - Sequences in this run's working set: {working_set_seqs}\")\n",
        "\n",
        "# --- Part 3 & 4: Seed Mapping ---\n",
        "hq_candidates = len(df_high_quality) if 'df_high_quality' in globals() else 0\n",
        "seed_exons_mapped = len(df_raw_exons_seed) if 'df_raw_exons_seed' in globals() else 0\n",
        "logger.info(f\"\\n[Part 3 & 4] High-Confidence Seed Mapping:\")\n",
        "logger.info(f\"  - High-quality candidates after all filters: {hq_candidates}\")\n",
        "logger.info(f\"  - Total exons mapped from seed sequences: {seed_exons_mapped}\")\n",
        "\n",
        "# --- Part 5: Architecture-Driven Rescue ---\n",
        "rescued_proteins = df_rescued_exons['accession'].nunique() if 'df_rescued_exons' in globals() else 0\n",
        "rescued_exons_found = len(df_rescued_exons[df_rescued_exons['peptide'] != 'MISSING_EXON']) if 'df_rescued_exons' in globals() else 0\n",
        "rescued_exons_missing = len(df_rescued_exons[df_rescued_exons['peptide'] == 'MISSING_EXON']) if 'df_rescued_exons' in globals() else 0\n",
        "logger.info(f\"\\n[Part 5] Architecture-Driven Rescue:\")\n",
        "logger.info(f\"  - Full architectures reconstructed for: {rescued_proteins} proteins\")\n",
        "logger.info(f\"  - Total exons found via 'fishing': {rescued_exons_found}\")\n",
        "logger.info(f\"  - Total missing exons identified (padded): {rescued_exons_missing}\")\n",
        "\n",
        "# --- Part 6: Final Consensus & Analysis ---\n",
        "final_consensus_exons = len(consensus_tbl) if 'consensus_tbl' in globals() else 0\n",
        "dated_events = len(df_evolution) if 'df_evolution' in globals() else 0\n",
        "final_architectures = len(wide_df) if 'wide_df' in globals() else 0\n",
        "logger.info(f\"\\n[Part 6] Final Consensus & Evolutionary Analysis:\")\n",
        "logger.info(f\"  - Canonical exons in final consensus: {final_consensus_exons}\")\n",
        "logger.info(f\"  - Evolutionary events dated: {dated_events}\")\n",
        "logger.info(f\"  - Final wide-format architectures generated: {final_architectures}\")\n",
        "\n",
        "# --- Part 7: RegExTractor Gene Classification ---\n",
        "unclassified_pool_size = len(target_pool_rex) if 'target_pool_rex' in globals() else 0\n",
        "newly_classified_chains = len(classified_chains_df) if 'classified_chains_df' in globals() and not classified_chains_df.empty else 0\n",
        "logger.info(f\"\\n[Part 7] Gene Classification:\")\n",
        "logger.info(f\"  - Sequences in the unclassified target pool: {unclassified_pool_size}\")\n",
        "logger.info(f\"  - New sequences classified with high confidence: {newly_classified_chains}\")\n",
        "\n",
        "logger.info(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}